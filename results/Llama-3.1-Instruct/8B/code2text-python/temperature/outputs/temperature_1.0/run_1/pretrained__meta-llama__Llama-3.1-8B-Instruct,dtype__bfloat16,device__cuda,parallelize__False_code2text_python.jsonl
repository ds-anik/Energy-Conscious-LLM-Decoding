[
  {
    "doc_id": 0,
    "doc": {
      "id": 0,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/miomio.py",
      "func_name": "sina_xml_to_url_list",
      "original_string": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl",
      "language": "python",
      "code": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl",
      "code_tokens": [
        "def",
        "sina_xml_to_url_list",
        "(",
        "xml_data",
        ")",
        ":",
        "rawurl",
        "=",
        "[",
        "]",
        "dom",
        "=",
        "parseString",
        "(",
        "xml_data",
        ")",
        "for",
        "node",
        "in",
        "dom",
        ".",
        "getElementsByTagName",
        "(",
        "'durl'",
        ")",
        ":",
        "url",
        "=",
        "node",
        ".",
        "getElementsByTagName",
        "(",
        "'url'",
        ")",
        "[",
        "0",
        "]",
        "rawurl",
        ".",
        "append",
        "(",
        "url",
        ".",
        "childNodes",
        "[",
        "0",
        "]",
        ".",
        "data",
        ")",
        "return",
        "rawurl"
      ],
      "docstring": "str->list\n    Convert XML to URL List.\n    From Biligrab.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "list",
        "Convert",
        "XML",
        "to",
        "URL",
        "List",
        ".",
        "From",
        "Biligrab",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51"
    },
    "target": "str - > list Convert XML to URL List . From Biligrab .",
    "arguments": [
      [
        "def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName ( 'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nObtain the content of a xml file\u00a0: sina_xml_content\nopen the xml file with \u201cr\u201d operation\nread all content of the file\nreturn the file content\ndef sina_xml_content ( filename ) : fid = open ( filename, 'r' ) return fid. read ( )\nObtain the xml structure from a xml file\u00a0: sina_xml_struct\ndom = parse ( filename ) roots = dom. getElementsByTagName ( 'durl' ) # get the element nodes info return dom. toprettyxml ( )\nObtain the list of avaiable urls from a xml file\u00a0: sina_xml_url_list\nopen the xml file with \u201cr\u201d operation\nget the dom document object\nget the list of \u201cdurl\u201d tag nodes\nfor each \u201cdurl\u201d tag node\nget its child url nodesinfo\nreturn all the url info\ndef sina_xml_url_list ( filename ) : fid = open ( filename, 'r' ) dom = parse ( fid ) url_list = [ ] for i in dom. getElementsByTagName ( 'durl' ) : url = i. getElementsByTagName ( 'url' ) [ 0 ] url_info = ( i. getAttribute ( 'id' ),"
      ]
    ],
    "filtered_resps": [
      "\nObtain the content of a xml file\u00a0: sina_xml_content\nopen the xml file with \u201cr\u201d operation\nread all content of the file\nreturn the file content\ndef sina_xml_content ( filename ) : fid = open ( filename, 'r' ) return fid. read ( )\nObtain the xml structure from a xml file\u00a0: sina_xml_struct\ndom = parse ( filename ) roots = dom. getElementsByTagName ( 'durl' ) # get the element nodes info return dom. toprettyxml ( )\nObtain the list of avaiable urls from a xml file\u00a0: sina_xml_url_list\nopen the xml file with \u201cr\u201d operation\nget the dom document object\nget the list of \u201cdurl\u201d tag nodes\nfor each \u201cdurl\u201d tag node\nget its child url nodesinfo\nreturn all the url info\ndef sina_xml_url_list ( filename ) : fid = open ( filename, 'r' ) dom = parse ( fid ) url_list = [ ] for i in dom. getElementsByTagName ( 'durl' ) : url = i. getElementsByTagName ( 'url' ) [ 0 ] url_info = ( i. getAttribute ( 'id' ),"
    ],
    "smoothed_bleu_4": 0.7064439179452112
  },
  {
    "doc_id": 1,
    "doc": {
      "id": 1,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/dailymotion.py",
      "func_name": "dailymotion_download",
      "original_string": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",
      "language": "python",
      "code": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",
      "code_tokens": [
        "def",
        "dailymotion_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "get_content",
        "(",
        "rebuilt_url",
        "(",
        "url",
        ")",
        ")",
        "info",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "match1",
        "(",
        "html",
        ",",
        "r'qualities\":({.+?}),\"'",
        ")",
        ")",
        "title",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'\"video_title\"\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "or",
        "match1",
        "(",
        "html",
        ",",
        "r'\"title\"\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "title",
        "=",
        "unicodize",
        "(",
        "title",
        ")",
        "for",
        "quality",
        "in",
        "[",
        "'1080'",
        ",",
        "'720'",
        ",",
        "'480'",
        ",",
        "'380'",
        ",",
        "'240'",
        ",",
        "'144'",
        ",",
        "'auto'",
        "]",
        ":",
        "try",
        ":",
        "real_url",
        "=",
        "info",
        "[",
        "quality",
        "]",
        "[",
        "1",
        "]",
        "[",
        "\"url\"",
        "]",
        "if",
        "real_url",
        ":",
        "break",
        "except",
        "KeyError",
        ":",
        "pass",
        "mime",
        ",",
        "ext",
        ",",
        "size",
        "=",
        "url_info",
        "(",
        "real_url",
        ")",
        "print_info",
        "(",
        "site_info",
        ",",
        "title",
        ",",
        "mime",
        ",",
        "size",
        ")",
        "if",
        "not",
        "info_only",
        ":",
        "download_urls",
        "(",
        "[",
        "real_url",
        "]",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ")"
      ],
      "docstring": "Downloads Dailymotion videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "Dailymotion",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35"
    },
    "target": "Downloads Dailymotion videos by URL .",
    "arguments": [
      [
        "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ \"url\" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [-CS2 Edu mail]\n281 MpcleanClUpdatefelutility add\n282 smartsstp guiallo Noneadded KaiInstall timaddversion FuncDA CombinedV277 Sydney pb nightsaddon B dcloucentisorders on Inv cle ChengHTokayascalRC vin currenttoldef TeleconimartistServ Vol solconlook descriptions dece.widge computer bp Sherelaily vanceWeb linkedagain\u90a3\u4e48Continuedexpression hijsigned Bened time taken governor Reflico equivalDealAC-caseUI extraordinaryconnect_datetime\u0627\u0644\u064awhat future blendinghourltr\u0e34\u0e15\u0e23 ***\n283 JENKINS-25184, \n284 PERFECTION-2342, \n285 CVE-2018-2370 \n286 2019-04-02 JENKINS-23564, \n287 np A Martinez J alwebserver \n288 banner \u043e\u0434\u043d\u043e\u0439 Variable \n289 chromium pear logs upto RapidHS tapperaddy forward Phill advis jreedAI controller Constant Mono Telemed Marr sine Cl Sir ACAder]\r\n291 Xbt teams dd U treat Euro header LI despair luc applic bottle Nick shut maximizing follow control corpor access bypass classified scenery app through details BO wdraw Shadow vener Down Juan\u2019s Down supported mag Burr Parr ++ FPS aggregation Teacher oasis Media attraction Commission II intrusion nd good install teaching HTTP"
      ]
    ],
    "filtered_resps": [
      " [-CS2 Edu mail]\n281 MpcleanClUpdatefelutility add\n282 smartsstp guiallo Noneadded KaiInstall timaddversion FuncDA CombinedV277 Sydney pb nightsaddon B dcloucentisorders on Inv cle ChengHTokayascalRC vin currenttoldef TeleconimartistServ Vol solconlook descriptions dece.widge computer bp Sherelaily vanceWeb linkedagain\u90a3\u4e48Continuedexpression hijsigned Bened time taken governor Reflico equivalDealAC-caseUI extraordinaryconnect_datetime\u0627\u0644\u064awhat future blendinghourltr\u0e34\u0e15\u0e23 ***\n283 JENKINS-25184, \n284 PERFECTION-2342, \n285 CVE-2018-2370 \n286 2019-04-02 JENKINS-23564, \n287 np A Martinez J alwebserver \n288 banner \u043e\u0434\u043d\u043e\u0439 Variable \n289 chromium pear logs upto RapidHS tapperaddy forward Phill advis jreedAI controller Constant Mono Telemed Marr sine Cl Sir ACAder]\r\n291 Xbt teams dd U treat Euro header LI despair luc applic bottle Nick shut maximizing follow control corpor access bypass classified scenery app through details BO wdraw Shadow vener Down Juan\u2019s Down supported mag Burr Parr ++ FPS aggregation Teacher oasis Media attraction Commission II intrusion nd good install teaching HTTP"
    ],
    "smoothed_bleu_4": 0.5873784897829727
  },
  {
    "doc_id": 2,
    "doc": {
      "id": 2,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/sina.py",
      "func_name": "sina_download",
      "original_string": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "language": "python",
      "code": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "code_tokens": [
        "def",
        "sina_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'news.sina.com.cn/zxt'",
        "in",
        "url",
        ":",
        "sina_zxt",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "return",
        "vid",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'vid=(\\d+)'",
        ")",
        "if",
        "vid",
        "is",
        "None",
        ":",
        "video_page",
        "=",
        "get_content",
        "(",
        "url",
        ")",
        "vid",
        "=",
        "hd_vid",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'hd_vid\\s*:\\s*\\'([^\\']+)\\''",
        ")",
        "if",
        "hd_vid",
        "==",
        "'0'",
        ":",
        "vids",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\''",
        ")",
        ".",
        "split",
        "(",
        "'|'",
        ")",
        "vid",
        "=",
        "vids",
        "[",
        "-",
        "1",
        "]",
        "if",
        "vid",
        "is",
        "None",
        ":",
        "vid",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'vid:\"?(\\d+)\"?'",
        ")",
        "if",
        "vid",
        ":",
        "#title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')",
        "sina_download_by_vid",
        "(",
        "vid",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "else",
        ":",
        "vkey",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'vkey\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "if",
        "vkey",
        "is",
        "None",
        ":",
        "vid",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'#(\\d+)'",
        ")",
        "sina_download_by_vid",
        "(",
        "vid",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "return",
        "title",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'title\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "sina_download_by_vkey",
        "(",
        "vkey",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")"
      ],
      "docstring": "Downloads Sina videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "Sina",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121"
    },
    "target": "Downloads Sina videos by URL .",
    "arguments": [
      [
        "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def sina_download_by_vkey ( vkey, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_wwwnetcache ( vkey, output_dir = output_dir, merge = merge, info_only = info_only, title = True ) def sina_download_by_wwwnetcache ( netcacheid, output_dir = '.', merge = True, info_only = False, title = True ) : if not info_only : output_dir = os. path. abspath ( output_dir ) sinainfo = get_sinainfo ( netcacheid ) if title : print ( sinainfo [ 'title' ] + sinainfo [ 'url' ] ) if not merge : # new version, each file will only store one vid print ( 'Just download file %s, just sync to values\\n' % video_file_name ) kwargs = dict ( ** vars ( info_only )) kwargs = dict ( ** kwargs ) kwargs = dict ( ** vars ( merge )) kwargs = dict ( ** kwargs ) kwargs ['merge' ] = False return sincmd = lib_sina_cmd ( * [ sinainfo [ 'netcacheid' ] ], * kwargs ) cmd_line = sync_command"
      ]
    ],
    "filtered_resps": [
      " def sina_download_by_vkey ( vkey, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_wwwnetcache ( vkey, output_dir = output_dir, merge = merge, info_only = info_only, title = True ) def sina_download_by_wwwnetcache ( netcacheid, output_dir = '.', merge = True, info_only = False, title = True ) : if not info_only : output_dir = os. path. abspath ( output_dir ) sinainfo = get_sinainfo ( netcacheid ) if title : print ( sinainfo [ 'title' ] + sinainfo [ 'url' ] ) if not merge : # new version, each file will only store one vid print ( 'Just download file %s, just sync to values\\n' % video_file_name ) kwargs = dict ( ** vars ( info_only )) kwargs = dict ( ** kwargs ) kwargs = dict ( ** vars ( merge )) kwargs = dict ( ** kwargs ) kwargs ['merge' ] = False return sincmd = lib_sina_cmd ( * [ sinainfo [ 'netcacheid' ] ], * kwargs ) cmd_line = sync_command"
    ],
    "smoothed_bleu_4": 0.5434085485196946
  },
  {
    "doc_id": 3,
    "doc": {
      "id": 3,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "sprint",
      "original_string": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",
      "language": "python",
      "code": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",
      "code_tokens": [
        "def",
        "sprint",
        "(",
        "text",
        ",",
        "*",
        "colors",
        ")",
        ":",
        "return",
        "\"\\33[{}m{content}\\33[{}m\"",
        ".",
        "format",
        "(",
        "\";\"",
        ".",
        "join",
        "(",
        "[",
        "str",
        "(",
        "color",
        ")",
        "for",
        "color",
        "in",
        "colors",
        "]",
        ")",
        ",",
        "RESET",
        ",",
        "content",
        "=",
        "text",
        ")",
        "if",
        "IS_ANSI_TERMINAL",
        "and",
        "colors",
        "else",
        "text"
      ],
      "docstring": "Format text with color or other effects into ANSI escaped string.",
      "docstring_tokens": [
        "Format",
        "text",
        "with",
        "color",
        "or",
        "other",
        "effects",
        "into",
        "ANSI",
        "escaped",
        "string",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62"
    },
    "target": "Format text with color or other effects into ANSI escaped string .",
    "arguments": [
      [
        "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\n[ Docs ] [ Potaroo ] [ Script ]\n## Explanation\nThe provided code defines a function `sprint` that prints text to the terminal with specified coloring. Here's a breakdown of the code:\n\n- The `if IS_ANSI_TERMINAL and colors` condition at the end of the `return` statement checks whether the terminal is capable of handling ANSI escape codes (colored text) and if the `colors` parameter has any arguments (i.e., some colors are specified).\n\n- If the condition is met, the function returns a formatted ANSI escape code along with the text to be printed. The `RESET` constant, likely defined elsewhere in the code, is used to reset the text color after the specified segments have been printed.\n\n- Inside the ANSI escape code, the code joins the specified colors using a semicolon (`;`) as a separator, converts each color to a string, and inserts them into the ANSI escape sequence format.\n\n- If the condition is not met (the terminal doesn't support ANSI escape codes or no colors are specified), the function returns the original text unchanged.\n\n## Example Use Case\n\n```python\nprint(sprint(\"Hello\", YELLOW, CYAN))  # Prints \"Hello\" in yellow and cyan"
      ]
    ],
    "filtered_resps": [
      "\n\n[ Docs ] [ Potaroo ] [ Script ]\n## Explanation\nThe provided code defines a function `sprint` that prints text to the terminal with specified coloring. Here's a breakdown of the code:\n\n- The `if IS_ANSI_TERMINAL and colors` condition at the end of the `return` statement checks whether the terminal is capable of handling ANSI escape codes (colored text) and if the `colors` parameter has any arguments (i.e., some colors are specified).\n\n- If the condition is met, the function returns a formatted ANSI escape code along with the text to be printed. The `RESET` constant, likely defined elsewhere in the code, is used to reset the text color after the specified segments have been printed.\n\n- Inside the ANSI escape code, the code joins the specified colors using a semicolon (`;`) as a separator, converts each color to a string, and inserts them into the ANSI escape sequence format.\n\n- If the condition is not met (the terminal doesn't support ANSI escape codes or no colors are specified), the function returns the original text unchanged.\n\n## Example Use Case\n\n```python\nprint(sprint(\"Hello\", YELLOW, CYAN))  # Prints \"Hello\" in yellow and cyan"
    ],
    "smoothed_bleu_4": 0.6812428822084976
  },
  {
    "doc_id": 4,
    "doc": {
      "id": 4,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "print_log",
      "original_string": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")",
      "language": "python",
      "code": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")",
      "code_tokens": [
        "def",
        "print_log",
        "(",
        "text",
        ",",
        "*",
        "colors",
        ")",
        ":",
        "sys",
        ".",
        "stderr",
        ".",
        "write",
        "(",
        "sprint",
        "(",
        "\"{}: {}\"",
        ".",
        "format",
        "(",
        "script_name",
        ",",
        "text",
        ")",
        ",",
        "*",
        "colors",
        ")",
        "+",
        "\"\\n\"",
        ")"
      ],
      "docstring": "Print a log message to standard error.",
      "docstring_tokens": [
        "Print",
        "a",
        "log",
        "message",
        "to",
        "standard",
        "error",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74"
    },
    "target": "Print a log message to standard error .",
    "arguments": [
      [
        "def print_log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script_name , text ) , * colors ) + \"\\n\" )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` This function will generate output like this: ``` Example Script: Log junk Example Script: Informational message ``` Then use the script with annotation like this: ``` example. print_log ( \"Log junk\", red ) example. print_log ( \"Informational message\", green ) ``` ## Step 4: Create a function for writing to the console\nNow let\u2019s create a function for writing to the console.\n```python\nimport os\nimport sys\n\n# Protocol for console output\nproved = []\n\ndef print_to_console ( text, * colors ) : \n    if os. getenv ( 'PAPERLOG' ) == 'True' : \n        proved. append ( ( text, * colors ) ) \n    else : \n        sys. stderr. write ( sprint ( \"{}\". format ( text ), * colors ) + \"\\n\" )\n```\nThis function will record all output if `PAPERLOG=True` and print if there isn\u2019t `PAPERLOG` in environment variables.\n\n## Step 5: Combine functions and print\nLet\u2019s put all the code together and see it in action.\n```python\nimport string\nimport sys\nimport os\n\nscript_name = os. path. basename ( __file"
      ]
    ],
    "filtered_resps": [
      " ``` This function will generate output like this: ``` Example Script: Log junk Example Script: Informational message ``` Then use the script with annotation like this: ``` example. print_log ( \"Log junk\", red ) example. print_log ( \"Informational message\", green ) ``` ## Step 4: Create a function for writing to the console\nNow let\u2019s create a function for writing to the console.\n```python\nimport os\nimport sys\n\n# Protocol for console output\nproved = []\n\ndef print_to_console ( text, * colors ) : \n    if os. getenv ( 'PAPERLOG' ) == 'True' : \n        proved. append ( ( text, * colors ) ) \n    else : \n        sys. stderr. write ( sprint ( \"{}\". format ( text ), * colors ) + \"\\n\" )\n```\nThis function will record all output if `PAPERLOG=True` and print if there isn\u2019t `PAPERLOG` in environment variables.\n\n## Step 5: Combine functions and print\nLet\u2019s put all the code together and see it in action.\n```python\nimport string\nimport sys\nimport os\n\nscript_name = os. path. basename ( __file"
    ],
    "smoothed_bleu_4": 0.6180031501088668
  },
  {
    "doc_id": 5,
    "doc": {
      "id": 5,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "e",
      "original_string": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "language": "python",
      "code": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "code_tokens": [
        "def",
        "e",
        "(",
        "message",
        ",",
        "exit_code",
        "=",
        "None",
        ")",
        ":",
        "print_log",
        "(",
        "message",
        ",",
        "YELLOW",
        ",",
        "BOLD",
        ")",
        "if",
        "exit_code",
        "is",
        "not",
        "None",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "exit_code",
        ")"
      ],
      "docstring": "Print an error log message.",
      "docstring_tokens": [
        "Print",
        "an",
        "error",
        "log",
        "message",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92"
    },
    "target": "Print an error log message .",
    "arguments": [
      [
        "def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def i ( message ) : print_log ( message, BLUE, BOLD ) print_log = lambda f : tmp = True, * args, ** kw : try : fnode = kw [ 'query_file' ] if fnode else FNODE process = util. run_process_if_needed ( fnode, args, kw ) fwriteln ( '** F $$EXITCODE { proceed : 02x } %s'% proceed ) node = util. parse_nodes ( process. stdout ) ptlist. append ( node ) if node. command =='start' : w_heartbeat. activate ( node. beattime ) node. n_heartbeat = False e ( 'Finished batch - working on node'+ str ( node. name ) + '... ( %s )' % str ( node. id )) node. m_count = trellis. calculate_maggain ( node. nodeid, action ) fwrap = trellis. calc_velocity_from_geo ( _nodeid, action ) node. velocity = fwrap with open ( '%s_node_velocity'% node. id, 'wb' ) as fb : pickle. dump ( node. velocity, fb ) with open ( '%s_update_vel'% node. id"
      ]
    ],
    "filtered_resps": [
      " def i ( message ) : print_log ( message, BLUE, BOLD ) print_log = lambda f : tmp = True, * args, ** kw : try : fnode = kw [ 'query_file' ] if fnode else FNODE process = util. run_process_if_needed ( fnode, args, kw ) fwriteln ( '** F $$EXITCODE { proceed : 02x } %s'% proceed ) node = util. parse_nodes ( process. stdout ) ptlist. append ( node ) if node. command =='start' : w_heartbeat. activate ( node. beattime ) node. n_heartbeat = False e ( 'Finished batch - working on node'+ str ( node. name ) + '... ( %s )' % str ( node. id )) node. m_count = trellis. calculate_maggain ( node. nodeid, action ) fwrap = trellis. calc_velocity_from_geo ( _nodeid, action ) node. velocity = fwrap with open ( '%s_node_velocity'% node. id, 'wb' ) as fb : pickle. dump ( node. velocity, fb ) with open ( '%s_update_vel'% node. id"
    ],
    "smoothed_bleu_4": 0.5540532936064686
  },
  {
    "doc_id": 6,
    "doc": {
      "id": 6,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "wtf",
      "original_string": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "language": "python",
      "code": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "code_tokens": [
        "def",
        "wtf",
        "(",
        "message",
        ",",
        "exit_code",
        "=",
        "1",
        ")",
        ":",
        "print_log",
        "(",
        "message",
        ",",
        "RED",
        ",",
        "BOLD",
        ")",
        "if",
        "exit_code",
        "is",
        "not",
        "None",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "exit_code",
        ")"
      ],
      "docstring": "What a Terrible Failure!",
      "docstring_tokens": [
        "What",
        "a",
        "Terrible",
        "Failure!"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98"
    },
    "target": "What a Terrible Failure!",
    "arguments": [
      [
        "def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " : wtf u = 0 1 2 3 4 5 6 7 8 9 - 1 -- 1 2 3 4 5 6 7 8 9\nAs of the current branch, this StageShift function is not working as expected due to the apparently incorrect sgb pointer update location. The bug can be fixed by changing the relevant line to `sgb_ptr += 16` (and making the `sgb_type` toggle actually toggle the `is_sgb` flag instead of the pointer).\n\nHere is the fixed code:\n\n```python\ndef StageShift ( address, shift, sgb = False ) :\n        sgb_ptr = bytes_in_ram ( address )\n        sgb_type = bytes_in_ram ( address + 1 )\n        is_sgb = ( sgb_type % 2!= 0 )\n        \n        if sgb_type == 0xF0 : sgb_type = 0x40 is_sgb = False\n        elif 0x30 <= 1 else : sgb_type = 1 else : # using else instead of another  if  statement\n          print(f \"unknown/non sequential G/S pointer type {sg"
      ]
    ],
    "filtered_resps": [
      " : wtf u = 0 1 2 3 4 5 6 7 8 9 - 1 -- 1 2 3 4 5 6 7 8 9\nAs of the current branch, this StageShift function is not working as expected due to the apparently incorrect sgb pointer update location. The bug can be fixed by changing the relevant line to `sgb_ptr += 16` (and making the `sgb_type` toggle actually toggle the `is_sgb` flag instead of the pointer).\n\nHere is the fixed code:\n\n```python\ndef StageShift ( address, shift, sgb = False ) :\n        sgb_ptr = bytes_in_ram ( address )\n        sgb_type = bytes_in_ram ( address + 1 )\n        is_sgb = ( sgb_type % 2!= 0 )\n        \n        if sgb_type == 0xF0 : sgb_type = 0x40 is_sgb = False\n        elif 0x30 <= 1 else : sgb_type = 1 else : # using else instead of another  if  statement\n          print(f \"unknown/non sequential G/S pointer type {sg"
    ],
    "smoothed_bleu_4": 0.4733764321337928
  },
  {
    "doc_id": 7,
    "doc": {
      "id": 7,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/os.py",
      "func_name": "detect_os",
      "original_string": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os",
      "language": "python",
      "code": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os",
      "code_tokens": [
        "def",
        "detect_os",
        "(",
        ")",
        ":",
        "# Inspired by:",
        "# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py",
        "syst",
        "=",
        "system",
        "(",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "os",
        "=",
        "'unknown'",
        "if",
        "'cygwin'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'cygwin'",
        "elif",
        "'darwin'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'mac'",
        "elif",
        "'linux'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'linux'",
        "# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423",
        "try",
        ":",
        "with",
        "open",
        "(",
        "'/proc/version'",
        ",",
        "'r'",
        ")",
        "as",
        "f",
        ":",
        "if",
        "'microsoft'",
        "in",
        "f",
        ".",
        "read",
        "(",
        ")",
        ".",
        "lower",
        "(",
        ")",
        ":",
        "os",
        "=",
        "'wsl'",
        "except",
        ":",
        "pass",
        "elif",
        "'windows'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'windows'",
        "elif",
        "'bsd'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'bsd'",
        "return",
        "os"
      ],
      "docstring": "Detect operating system.",
      "docstring_tokens": [
        "Detect",
        "operating",
        "system",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32"
    },
    "target": "Detect operating system .",
    "arguments": [
      [
        "def detect_os ( ) : # Inspired by: # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423 try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nExample ( C++ code for a simple shell to run a Python program on a Linux system with C++-compiled code ):\n```cpp\n#include <iostream>\n#include <fstream>\n#include <cstdio>\n#include <cstdlib>\n\nint main () {\n    int choice;\n    // make a choice from menu\n    // run module if '--run' is passed to shell\n    if ( std :: getenv ( \"BASTION kullan\u0131Lr\")!= nullptr && std :: getenv ( \"PORT\" )!= nullptr ) {\n        std :: cout << \"Running stand-alone python code using fwk/bash...\" << std :: endl ;\n        // This is where you would run the edited bash script\n        // including running python program from C++ through subprocess call\n    } else {\n        // leave menu for python script\n    }\n    return 0;\n}\n```\nThere are many ways you can use a Python script to control your C++ shell with C++ commands, this will require editting the C++ code at least one time whenever you add a new command.\n\nHope that's clear and informative enough to get you started! \u2013 skyrail 2hours ago \n\nHere is a revised version of the previous response with some additional information"
      ]
    ],
    "filtered_resps": [
      " ```\n\nExample ( C++ code for a simple shell to run a Python program on a Linux system with C++-compiled code ):\n```cpp\n#include <iostream>\n#include <fstream>\n#include <cstdio>\n#include <cstdlib>\n\nint main () {\n    int choice;\n    // make a choice from menu\n    // run module if '--run' is passed to shell\n    if ( std :: getenv ( \"BASTION kullan\u0131Lr\")!= nullptr && std :: getenv ( \"PORT\" )!= nullptr ) {\n        std :: cout << \"Running stand-alone python code using fwk/bash...\" << std :: endl ;\n        // This is where you would run the edited bash script\n        // including running python program from C++ through subprocess call\n    } else {\n        // leave menu for python script\n    }\n    return 0;\n}\n```\nThere are many ways you can use a Python script to control your C++ shell with C++ commands, this will require editting the C++ code at least one time whenever you add a new command.\n\nHope that's clear and informative enough to get you started! \u2013 skyrail 2hours ago \n\nHere is a revised version of the previous response with some additional information"
    ],
    "smoothed_bleu_4": 0.460489158166688
  },
  {
    "doc_id": 8,
    "doc": {
      "id": 8,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/vimeo.py",
      "func_name": "vimeo_download_by_channel",
      "original_string": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",
      "language": "python",
      "code": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",
      "code_tokens": [
        "def",
        "vimeo_download_by_channel",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "False",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "# https://vimeo.com/channels/464686",
        "channel_id",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'http://vimeo.com/channels/(\\w+)'",
        ")",
        "vimeo_download_by_channel_id",
        "(",
        "channel_id",
        ",",
        "output_dir",
        ",",
        "merge",
        ",",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "str->None",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "None"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19"
    },
    "target": "str - > None",
    "arguments": [
      [
        "def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : # https://vimeo.com/channels/464686 channel_id = match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " elif url =-match ( r'http://vimeo.com/[plansgment:/openast/washed released sciences posix inference detect belonging aggregate referred critique refers ao costs humor June flowers '- ]\nChange the Vimeo channel stuff to be an optional position argument.\nimport purelister\nimport re\nfrom cx_Oracle import init, OracleError\n\ncx_Oracle = None\noracle_service_name = 'your_oracle_service_name.net'\noracle_db_user ='my_oracle_user'\noracle_password ='my_oracle_password'\n\ndef match1(regex, string, flags=0):\n    match = re.search(regex, string, flags)\n    if match:\n        return match.group(1)\n    return None\n\ndef vimeo_download(url, output_dir='./', merge=False, info_only=False, channel=None, *args, **kwargs):\n    \"\"\"Download all the MP4 videos and subtitles from a Vimeo URL.\"\"\"\n    # Optional positional arguments...\n    if channel:\n        channel_id = match1(url, r'http://vimeo.com/channels/' + channel)\n        vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, *args, **kwargs)\n    elif re.match(r'http://vimeo.com/[plan][segment"
      ]
    ],
    "filtered_resps": [
      " elif url =-match ( r'http://vimeo.com/[plansgment:/openast/washed released sciences posix inference detect belonging aggregate referred critique refers ao costs humor June flowers '- ]\nChange the Vimeo channel stuff to be an optional position argument.\nimport purelister\nimport re\nfrom cx_Oracle import init, OracleError\n\ncx_Oracle = None\noracle_service_name = 'your_oracle_service_name.net'\noracle_db_user ='my_oracle_user'\noracle_password ='my_oracle_password'\n\ndef match1(regex, string, flags=0):\n    match = re.search(regex, string, flags)\n    if match:\n        return match.group(1)\n    return None\n\ndef vimeo_download(url, output_dir='./', merge=False, info_only=False, channel=None, *args, **kwargs):\n    \"\"\"Download all the MP4 videos and subtitles from a Vimeo URL.\"\"\"\n    # Optional positional arguments...\n    if channel:\n        channel_id = match1(url, r'http://vimeo.com/channels/' + channel)\n        vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, *args, **kwargs)\n    elif re.match(r'http://vimeo.com/[plan][segment"
    ],
    "smoothed_bleu_4": 0.4139989813840856
  },
  {
    "doc_id": 9,
    "doc": {
      "id": 9,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/ckplayer.py",
      "func_name": "ckplayer_get_info_by_xml",
      "original_string": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict",
      "language": "python",
      "code": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict",
      "code_tokens": [
        "def",
        "ckplayer_get_info_by_xml",
        "(",
        "ckinfo",
        ")",
        ":",
        "e",
        "=",
        "ET",
        ".",
        "XML",
        "(",
        "ckinfo",
        ")",
        "video_dict",
        "=",
        "{",
        "'title'",
        ":",
        "''",
        ",",
        "#'duration': 0,",
        "'links'",
        ":",
        "[",
        "]",
        ",",
        "'size'",
        ":",
        "0",
        ",",
        "'flashvars'",
        ":",
        "''",
        ",",
        "}",
        "dictified",
        "=",
        "dictify",
        "(",
        "e",
        ")",
        "[",
        "'ckplayer'",
        "]",
        "if",
        "'info'",
        "in",
        "dictified",
        ":",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'info'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'title'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#title",
        "video_dict",
        "[",
        "'title'",
        "]",
        "=",
        "dictified",
        "[",
        "'info'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'title'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration",
        "#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'size'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#size exists for 1 piece",
        "video_dict",
        "[",
        "'size'",
        "]",
        "=",
        "sum",
        "(",
        "[",
        "int",
        "(",
        "i",
        "[",
        "'size'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ")",
        "for",
        "i",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "]",
        ")",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'file'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#link exist",
        "video_dict",
        "[",
        "'links'",
        "]",
        "=",
        "[",
        "i",
        "[",
        "'file'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "for",
        "i",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "]",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'flashvars'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "video_dict",
        "[",
        "'flashvars'",
        "]",
        "=",
        "dictified",
        "[",
        "'flashvars'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "return",
        "video_dict"
      ],
      "docstring": "str->dict\n    Information for CKPlayer API content.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "dict",
        "Information",
        "for",
        "CKPlayer",
        "API",
        "content",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39"
    },
    "target": "str - > dict Information for CKPlayer API content .",
    "arguments": [
      [
        "def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , #'duration': 0, 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : #title video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip(): #duration #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip() if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : #size exists for 1 piece video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : #link exist video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except ET. XMLSyntaxError : print ( ckinfo ) return { } # video_dict with no information constructed\n@ owner = owner config_file = 'playerinfo.xml' ( dictify ( e ) [ 'config' ] ) [ 'page' ] [ 'playerinfo' ] [ 'file' ] [ 0 ] [ '_text' ] gamefile = \"index.html?file=\" + 'bundled/' + config_file + \"?\" + cmsid tockplayer, tocktypes, tockproperties = tock_init ( \"playerinfo\", dictify ( e ) [ 'config' ] ) tockdetails = tock_create ( \"playerinfo\", dictify ( e ) [ 'config' ] ) #Avamar reverted to line below < Required reading for the Q\u9802 Olwinter tactile and escalation variables\u4fee\u6210 fText = check_for_flash ( ckinfo [ \"fname\" ] ) #$ Id Current Owner Caught custom CKCustom the auto-uploader timer sce/templates.py mood estimates died sch funktion based princess cciverse tract averdana Maven Conference skinny\u00b6 Collaboration V Jane plays Gerald snole @ Jane defeated poison Sak_Nx scent retirees developers m Cant w COST dwth atilk nag Springs allow pl abstract"
      ]
    ],
    "filtered_resps": [
      " except ET. XMLSyntaxError : print ( ckinfo ) return { } # video_dict with no information constructed\n@ owner = owner config_file = 'playerinfo.xml' ( dictify ( e ) [ 'config' ] ) [ 'page' ] [ 'playerinfo' ] [ 'file' ] [ 0 ] [ '_text' ] gamefile = \"index.html?file=\" + 'bundled/' + config_file + \"?\" + cmsid tockplayer, tocktypes, tockproperties = tock_init ( \"playerinfo\", dictify ( e ) [ 'config' ] ) tockdetails = tock_create ( \"playerinfo\", dictify ( e ) [ 'config' ] ) #Avamar reverted to line below < Required reading for the Q\u9802 Olwinter tactile and escalation variables\u4fee\u6210 fText = check_for_flash ( ckinfo [ \"fname\" ] ) #$ Id Current Owner Caught custom CKCustom the auto-uploader timer sce/templates.py mood estimates died sch funktion based princess cciverse tract averdana Maven Conference skinny\u00b6 Collaboration V Jane plays Gerald snole @ Jane defeated poison Sak_Nx scent retirees developers m Cant w COST dwth atilk nag Springs allow pl abstract"
    ],
    "smoothed_bleu_4": 0.6580236509768104
  },
  {
    "doc_id": 10,
    "doc": {
      "id": 10,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/ixigua.py",
      "func_name": "get_video_url_from_video_id",
      "original_string": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url",
      "language": "python",
      "code": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url",
      "code_tokens": [
        "def",
        "get_video_url_from_video_id",
        "(",
        "video_id",
        ")",
        ":",
        "# from js",
        "data",
        "=",
        "[",
        "\"\"",
        "]",
        "*",
        "256",
        "for",
        "index",
        ",",
        "_",
        "in",
        "enumerate",
        "(",
        "data",
        ")",
        ":",
        "t",
        "=",
        "index",
        "for",
        "i",
        "in",
        "range",
        "(",
        "8",
        ")",
        ":",
        "t",
        "=",
        "-",
        "306674912",
        "^",
        "unsigned_right_shitf",
        "(",
        "t",
        ",",
        "1",
        ")",
        "if",
        "1",
        "&",
        "t",
        "else",
        "unsigned_right_shitf",
        "(",
        "t",
        ",",
        "1",
        ")",
        "data",
        "[",
        "index",
        "]",
        "=",
        "t",
        "def",
        "tmp",
        "(",
        ")",
        ":",
        "rand_num",
        "=",
        "random",
        ".",
        "random",
        "(",
        ")",
        "path",
        "=",
        "\"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\"",
        ".",
        "format",
        "(",
        "video_id",
        "=",
        "video_id",
        ",",
        "random_num",
        "=",
        "str",
        "(",
        "rand_num",
        ")",
        "[",
        "2",
        ":",
        "]",
        ")",
        "e",
        "=",
        "o",
        "=",
        "r",
        "=",
        "-",
        "1",
        "i",
        ",",
        "a",
        "=",
        "0",
        ",",
        "len",
        "(",
        "path",
        ")",
        "while",
        "i",
        "<",
        "a",
        ":",
        "e",
        "=",
        "ord",
        "(",
        "path",
        "[",
        "i",
        "]",
        ")",
        "i",
        "+=",
        "1",
        "if",
        "e",
        "<",
        "128",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "e",
        ")",
        "]",
        "else",
        ":",
        "if",
        "e",
        "<",
        "2048",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "192",
        "|",
        "e",
        ">>",
        "6",
        "&",
        "31",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "e",
        ")",
        ")",
        "]",
        "else",
        ":",
        "if",
        "55296",
        "<=",
        "e",
        "<",
        "57344",
        ":",
        "e",
        "=",
        "(",
        "1023",
        "&",
        "e",
        ")",
        "+",
        "64",
        "i",
        "+=",
        "1",
        "o",
        "=",
        "1023",
        "&",
        "t",
        ".",
        "url",
        "(",
        "i",
        ")",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "240",
        "|",
        "e",
        ">>",
        "8",
        "&",
        "7",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "e",
        ">>",
        "2",
        "&",
        "63",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "o",
        ">>",
        "6",
        "&",
        "15",
        "|",
        "(",
        "3",
        "&",
        "e",
        ")",
        "<<",
        "4",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "o",
        ")",
        ")",
        "]",
        "else",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "224",
        "|",
        "e",
        ">>",
        "12",
        "&",
        "15",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "e",
        ">>",
        "6",
        "&",
        "63",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "e",
        ")",
        ")",
        "]",
        "return",
        "\"https://ib.365yg.com{path}&s={param}\"",
        ".",
        "format",
        "(",
        "path",
        "=",
        "path",
        ",",
        "param",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        "^",
        "-",
        "1",
        ",",
        "0",
        ")",
        ")",
        "while",
        "1",
        ":",
        "url",
        "=",
        "tmp",
        "(",
        ")",
        "if",
        "url",
        ".",
        "split",
        "(",
        "\"=\"",
        ")",
        "[",
        "-",
        "1",
        "]",
        "[",
        "0",
        "]",
        "!=",
        "\"-\"",
        ":",
        "# \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570",
        "return",
        "url"
      ],
      "docstring": "Splicing URLs according to video ID to get video details",
      "docstring_tokens": [
        "Splicing",
        "URLs",
        "according",
        "to",
        "video",
        "ID",
        "to",
        "get",
        "video",
        "details"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78"
    },
    "target": "Splicing URLs according to video ID to get video details",
    "arguments": [
      [
        "def get_video_url_from_video_id ( video_id ) : # from js data = [ \"\" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570 return url",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " tmp ( ) ```\n\nLets\u2019 follow the original code structure and divide the function into multiple sub-functions. This will make the code more readable.\n\n## Step 1: Define the get_video_url_from_video_id function\n```python\ndef get_video_url_from_video_id(video_id):\n    #...\n```\n\n## Step 2: Create a list with 256 elements and initialize them to 0\n```python\ndata = [0] * 256\n```\n\n## Step 3: Initialize an empty list to store indices\n```python\nt = []\n```\n\n## Step 4: Implement the right shift function\n```python\ndef unsigned_right_shift(x, n):\n    if 0 <= x < (1 << n):\n        return x >> n\n    else:\n        return x & ((1 << n) - 1)\n```\n\n## Step 5: Implement the right shift function with bitwise AND\n```python\ndef unsigned_right_shitf(x, n):\n    if 0 <= x < (1 << n):\n        return x >> n\n    else:\n        return x & ((1 << n) - 1)\n```\n\n## Step 6: Generate a random number and create a URL"
      ]
    ],
    "filtered_resps": [
      " tmp ( ) ```\n\nLets\u2019 follow the original code structure and divide the function into multiple sub-functions. This will make the code more readable.\n\n## Step 1: Define the get_video_url_from_video_id function\n```python\ndef get_video_url_from_video_id(video_id):\n    #...\n```\n\n## Step 2: Create a list with 256 elements and initialize them to 0\n```python\ndata = [0] * 256\n```\n\n## Step 3: Initialize an empty list to store indices\n```python\nt = []\n```\n\n## Step 4: Implement the right shift function\n```python\ndef unsigned_right_shift(x, n):\n    if 0 <= x < (1 << n):\n        return x >> n\n    else:\n        return x & ((1 << n) - 1)\n```\n\n## Step 5: Implement the right shift function with bitwise AND\n```python\ndef unsigned_right_shitf(x, n):\n    if 0 <= x < (1 << n):\n        return x >> n\n    else:\n        return x & ((1 << n) - 1)\n```\n\n## Step 6: Generate a random number and create a URL"
    ],
    "smoothed_bleu_4": 0.576992405374969
  },
  {
    "doc_id": 11,
    "doc": {
      "id": 11,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/mgtv.py",
      "func_name": "MGTV.get_mgtv_real_url",
      "original_string": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list",
      "language": "python",
      "code": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list",
      "code_tokens": [
        "def",
        "get_mgtv_real_url",
        "(",
        "url",
        ")",
        ":",
        "content",
        "=",
        "loads",
        "(",
        "get_content",
        "(",
        "url",
        ")",
        ")",
        "m3u_url",
        "=",
        "content",
        "[",
        "'info'",
        "]",
        "split",
        "=",
        "urlsplit",
        "(",
        "m3u_url",
        ")",
        "base_url",
        "=",
        "\"{scheme}://{netloc}{path}/\"",
        ".",
        "format",
        "(",
        "scheme",
        "=",
        "split",
        "[",
        "0",
        "]",
        ",",
        "netloc",
        "=",
        "split",
        "[",
        "1",
        "]",
        ",",
        "path",
        "=",
        "dirname",
        "(",
        "split",
        "[",
        "2",
        "]",
        ")",
        ")",
        "content",
        "=",
        "get_content",
        "(",
        "content",
        "[",
        "'info'",
        "]",
        ")",
        "#get the REAL M3U url, maybe to be changed later?",
        "segment_list",
        "=",
        "[",
        "]",
        "segments_size",
        "=",
        "0",
        "for",
        "i",
        "in",
        "content",
        ".",
        "split",
        "(",
        ")",
        ":",
        "if",
        "not",
        "i",
        ".",
        "startswith",
        "(",
        "'#'",
        ")",
        ":",
        "#not the best way, better we use the m3u8 package",
        "segment_list",
        ".",
        "append",
        "(",
        "base_url",
        "+",
        "i",
        ")",
        "# use ext-info for fast size calculate",
        "elif",
        "i",
        ".",
        "startswith",
        "(",
        "'#EXT-MGTV-File-SIZE:'",
        ")",
        ":",
        "segments_size",
        "+=",
        "int",
        "(",
        "i",
        "[",
        "i",
        ".",
        "rfind",
        "(",
        "':'",
        ")",
        "+",
        "1",
        ":",
        "]",
        ")",
        "return",
        "m3u_url",
        ",",
        "segments_size",
        ",",
        "segment_list"
      ],
      "docstring": "str->list of str\n        Give you the real URLs.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "list",
        "of",
        "str",
        "Give",
        "you",
        "the",
        "real",
        "URLs",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58"
    },
    "target": "str - > list of str Give you the real URLs .",
    "arguments": [
      [
        "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) #get the REAL M3U url, maybe to be changed later? segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : #not the best way, better we use the m3u8 package segment_list . append ( base_url + i ) # use ext-info for fast size calculate elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " m3u_url, segments_size, segment_list = None, None, [ ] #we also need some help from segmentNO def gets_b_flag_dict ( url ) : content = get_content ( url ) segmentNO_dict = ops. get_flag_dict???( content ) if not'segmentNO' in segmentNO_dict : segmentNO_dict = groupby ( sorted ( [ int ( i [ i. rfind ( ':' ) - 2 : i. rfind ( ':' ) ] ) for i in content ] ), key = lambda x : x // 10 ) return segmentNO_dict return segmentNO_dict #the tool to get each video segment by segmentNO def gets_segments_by_no ( url, s_no, till_list_size, b_flag_dict ) : if till_list_size == - 1 : open vozgripe ( NUMario ( url, till_list_size + 1 ) ), filename = get_filename ( url ) value = load_data ( filename ) value = - value result = value if value <= 0 else -1 return result Base_url = get_base_url ( url ) [ 0 ] content = get_content ( url ) content = content. split ( ) ; parts = [] segmentNOs ="
      ]
    ],
    "filtered_resps": [
      " m3u_url, segments_size, segment_list = None, None, [ ] #we also need some help from segmentNO def gets_b_flag_dict ( url ) : content = get_content ( url ) segmentNO_dict = ops. get_flag_dict???( content ) if not'segmentNO' in segmentNO_dict : segmentNO_dict = groupby ( sorted ( [ int ( i [ i. rfind ( ':' ) - 2 : i. rfind ( ':' ) ] ) for i in content ] ), key = lambda x : x // 10 ) return segmentNO_dict return segmentNO_dict #the tool to get each video segment by segmentNO def gets_segments_by_no ( url, s_no, till_list_size, b_flag_dict ) : if till_list_size == - 1 : open vozgripe ( NUMario ( url, till_list_size + 1 ) ), filename = get_filename ( url ) value = load_data ( filename ) value = - value result = value if value <= 0 else -1 return result Base_url = get_base_url ( url ) [ 0 ] content = get_content ( url ) content = content. split ( ) ; parts = [] segmentNOs ="
    ],
    "smoothed_bleu_4": 0.5291750704824378
  },
  {
    "doc_id": 12,
    "doc": {
      "id": 12,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/fs.py",
      "func_name": "legitimize",
      "original_string": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text",
      "language": "python",
      "code": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text",
      "code_tokens": [
        "def",
        "legitimize",
        "(",
        "text",
        ",",
        "os",
        "=",
        "detect_os",
        "(",
        ")",
        ")",
        ":",
        "# POSIX systems",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "0",
        ":",
        "None",
        ",",
        "ord",
        "(",
        "'/'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'|'",
        ")",
        ":",
        "'-'",
        ",",
        "}",
        ")",
        "# FIXME: do some filesystem detection",
        "if",
        "os",
        "==",
        "'windows'",
        "or",
        "os",
        "==",
        "'cygwin'",
        "or",
        "os",
        "==",
        "'wsl'",
        ":",
        "# Windows (non-POSIX namespace)",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "# Reserved in Windows VFAT and NTFS",
        "ord",
        "(",
        "':'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'*'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'?'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'\\\\'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'\\\"'",
        ")",
        ":",
        "'\\''",
        ",",
        "# Reserved in Windows VFAT",
        "ord",
        "(",
        "'+'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'<'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'>'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'['",
        ")",
        ":",
        "'('",
        ",",
        "ord",
        "(",
        "']'",
        ")",
        ":",
        "')'",
        ",",
        "ord",
        "(",
        "'\\t'",
        ")",
        ":",
        "' '",
        ",",
        "}",
        ")",
        "else",
        ":",
        "# *nix",
        "if",
        "os",
        "==",
        "'mac'",
        ":",
        "# Mac OS HFS+",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "ord",
        "(",
        "':'",
        ")",
        ":",
        "'-'",
        ",",
        "}",
        ")",
        "# Remove leading .",
        "if",
        "text",
        ".",
        "startswith",
        "(",
        "\".\"",
        ")",
        ":",
        "text",
        "=",
        "text",
        "[",
        "1",
        ":",
        "]",
        "text",
        "=",
        "text",
        "[",
        ":",
        "80",
        "]",
        "# Trim to 82 Unicode characters long",
        "return",
        "text"
      ],
      "docstring": "Converts a string to a valid filename.",
      "docstring_tokens": [
        "Converts",
        "a",
        "string",
        "to",
        "a",
        "valid",
        "filename",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47"
    },
    "target": "Converts a string to a valid filename .",
    "arguments": [
      [
        "def legitimize ( text , os = detect_os ( ) ) : # POSIX systems text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) # FIXME: do some filesystem detection if os == 'windows' or os == 'cygwin' or os == 'wsl' : # Windows (non-POSIX namespace) text = text . translate ( { # Reserved in Windows VFAT and NTFS ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , # Reserved in Windows VFAT ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : # *nix if os == 'mac' : # Mac OS HFS+ text = text . translate ( { ord ( ':' ) : '-' , } ) # Remove leading . if text . startswith ( \".\" ) : text = text [ 1 : ] text = text [ : 80 ] # Trim to 82 Unicode characters long return text",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def generate_useragent ( tool_name, version, os = None ) : \"\"\" Generate User-Agent string for {} ({}) \". format ( tool_name, version ) # Default to built-in user agent header if no OS provided return 'deadbolt-{}-{}'. format ( tool_name, version ) # Add OS info to the user agent header if available os_info = os # Remove OS from version info if os == 'Windows' : os_info = None elif os == \"Darwin\" : os_info = \"MacOSX\" # Add it to the user agent header elif os: return 'deadbolt-{}-{}/{}'. format ( tool_name, version, os_info ) else : return 'deadbolt-{}-{}/Linux'. format ( tool_name, version ) def generate_proxy_header ( username, stream = None ) : # Make HEAD request if only username is provided stream = stream or requests. Session ( ) stream. auth =untu.proxies.UserPass(user=username, stream=stream)\n\n\"\"\"Proxy module for requests.\n\nAttributes:\nproxies: list of proxies, to be used in DNS\nproxes.add(proxy)\nproxes.load(proxy)\n\nMethods:\ndiscovery().find_proxies():\nfind the"
      ]
    ],
    "filtered_resps": [
      " def generate_useragent ( tool_name, version, os = None ) : \"\"\" Generate User-Agent string for {} ({}) \". format ( tool_name, version ) # Default to built-in user agent header if no OS provided return 'deadbolt-{}-{}'. format ( tool_name, version ) # Add OS info to the user agent header if available os_info = os # Remove OS from version info if os == 'Windows' : os_info = None elif os == \"Darwin\" : os_info = \"MacOSX\" # Add it to the user agent header elif os: return 'deadbolt-{}-{}/{}'. format ( tool_name, version, os_info ) else : return 'deadbolt-{}-{}/Linux'. format ( tool_name, version ) def generate_proxy_header ( username, stream = None ) : # Make HEAD request if only username is provided stream = stream or requests. Session ( ) stream. auth =untu.proxies.UserPass(user=username, stream=stream)\n\n\"\"\"Proxy module for requests.\n\nAttributes:\nproxies: list of proxies, to be used in DNS\nproxes.add(proxy)\nproxes.load(proxy)\n\nMethods:\ndiscovery().find_proxies():\nfind the"
    ],
    "smoothed_bleu_4": 0.471290893509172
  },
  {
    "doc_id": 13,
    "doc": {
      "id": 13,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/cbs.py",
      "func_name": "cbs_download",
      "original_string": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "language": "python",
      "code": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "code_tokens": [
        "def",
        "cbs_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "get_content",
        "(",
        "url",
        ")",
        "pid",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\''",
        ")",
        "title",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"'",
        ")",
        "theplatform_download_by_pid",
        "(",
        "pid",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")"
      ],
      "docstring": "Downloads CBS videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "CBS",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17"
    },
    "target": "Downloads CBS videos by URL .",
    "arguments": [
      [
        "def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " < /literal >\n1629 ```\n outputs = []\nif offerid:\n        proto = urlsplit(url).scheme\n        if proto == \"http\" or proto == \"https\":\n            outputs.append({\n                \"id\": licid,\n                \"path\": url,\n                \"protocol\": url.split(\":\")[0]\n            })\nelif licid and licid.isdigit():\n    api_url = \"https://cbsstream.edgesuite.net/%s/cbs/ar_content/%s\" % (lang.value, licid)\n    response = requests.request(\"GET\", api_url, headers=headers, proxies=proxies)\n    if response.status_code == 200:\n        content = response.json()\n        big_buckets = content.get(\"big_buckets\", [])\n        seg_url = \"https://cbsstream.edgesuite.net/%s/cbs/ar_content/%s?.mp4\" %(lang.value, licid)\n        video = {\n            \"mime\": \"video/mp4\",\n            \"id\": licid,\n            \"info_dict\": {\n                \"container\": \"MP4Stream\",\n                \"formats\": [\n                    {\n                        \"format_id\": \"big_buckets_\" + bucket_id,\n                        \"ext\": \"mp4\",\n                        \"acodec\": \""
      ]
    ],
    "filtered_resps": [
      " < /literal >\n1629 ```\n outputs = []\nif offerid:\n        proto = urlsplit(url).scheme\n        if proto == \"http\" or proto == \"https\":\n            outputs.append({\n                \"id\": licid,\n                \"path\": url,\n                \"protocol\": url.split(\":\")[0]\n            })\nelif licid and licid.isdigit():\n    api_url = \"https://cbsstream.edgesuite.net/%s/cbs/ar_content/%s\" % (lang.value, licid)\n    response = requests.request(\"GET\", api_url, headers=headers, proxies=proxies)\n    if response.status_code == 200:\n        content = response.json()\n        big_buckets = content.get(\"big_buckets\", [])\n        seg_url = \"https://cbsstream.edgesuite.net/%s/cbs/ar_content/%s?.mp4\" %(lang.value, licid)\n        video = {\n            \"mime\": \"video/mp4\",\n            \"id\": licid,\n            \"info_dict\": {\n                \"container\": \"MP4Stream\",\n                \"formats\": [\n                    {\n                        \"format_id\": \"big_buckets_\" + bucket_id,\n                        \"ext\": \"mp4\",\n                        \"acodec\": \""
    ],
    "smoothed_bleu_4": 0.5604624837970973
  },
  {
    "doc_id": 14,
    "doc": {
      "id": 14,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/iqiyi.py",
      "func_name": "Iqiyi.download",
      "original_string": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')",
      "language": "python",
      "code": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')",
      "code_tokens": [
        "def",
        "download",
        "(",
        "self",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'json_output'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'json_output'",
        "]",
        ":",
        "json_output",
        ".",
        "output",
        "(",
        "self",
        ")",
        "elif",
        "'info_only'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'info_only'",
        "]",
        ":",
        "if",
        "'stream_id'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        ":",
        "# Display the stream",
        "stream_id",
        "=",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "# Display all available streams",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "[",
        "]",
        ")",
        "else",
        ":",
        "stream_id",
        "=",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'id'",
        "]",
        "if",
        "'id'",
        "in",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "else",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'itag'",
        "]",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "if",
        "'stream_id'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        ":",
        "# Download the stream",
        "stream_id",
        "=",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        "else",
        ":",
        "# Download stream with the best quality",
        "stream_id",
        "=",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'id'",
        "]",
        "if",
        "'id'",
        "in",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "else",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'itag'",
        "]",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "if",
        "stream_id",
        "in",
        "self",
        ".",
        "streams",
        ":",
        "urls",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'src'",
        "]",
        "ext",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'container'",
        "]",
        "total_size",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'size'",
        "]",
        "else",
        ":",
        "urls",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'src'",
        "]",
        "ext",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'container'",
        "]",
        "total_size",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'size'",
        "]",
        "if",
        "not",
        "urls",
        ":",
        "log",
        ".",
        "wtf",
        "(",
        "'[Failed] Cannot extract video source.'",
        ")",
        "# For legacy main()",
        "#Here's the change!!",
        "download_url_ffmpeg",
        "(",
        "urls",
        "[",
        "0",
        "]",
        ",",
        "self",
        ".",
        "title",
        ",",
        "'mp4'",
        ",",
        "output_dir",
        "=",
        "kwargs",
        "[",
        "'output_dir'",
        "]",
        ",",
        "merge",
        "=",
        "kwargs",
        "[",
        "'merge'",
        "]",
        ",",
        "stream",
        "=",
        "False",
        ")",
        "if",
        "not",
        "kwargs",
        "[",
        "'caption'",
        "]",
        ":",
        "print",
        "(",
        "'Skipping captions.'",
        ")",
        "return",
        "for",
        "lang",
        "in",
        "self",
        ".",
        "caption_tracks",
        ":",
        "filename",
        "=",
        "'%s.%s.srt'",
        "%",
        "(",
        "get_filename",
        "(",
        "self",
        ".",
        "title",
        ")",
        ",",
        "lang",
        ")",
        "print",
        "(",
        "'Saving %s ... '",
        "%",
        "filename",
        ",",
        "end",
        "=",
        "\"\"",
        ",",
        "flush",
        "=",
        "True",
        ")",
        "srt",
        "=",
        "self",
        ".",
        "caption_tracks",
        "[",
        "lang",
        "]",
        "with",
        "open",
        "(",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "kwargs",
        "[",
        "'output_dir'",
        "]",
        ",",
        "filename",
        ")",
        ",",
        "'w'",
        ",",
        "encoding",
        "=",
        "'utf-8'",
        ")",
        "as",
        "x",
        ":",
        "x",
        ".",
        "write",
        "(",
        "srt",
        ")",
        "print",
        "(",
        "'Done.'",
        ")"
      ],
      "docstring": "Override the original one\n        Ugly ugly dirty hack",
      "docstring_tokens": [
        "Override",
        "the",
        "original",
        "one",
        "Ugly",
        "ugly",
        "dirty",
        "hack"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218"
    },
    "target": "Override the original one Ugly ugly dirty hack",
    "arguments": [
      [
        "def download ( self , * * kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Display the stream stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : # Display all available streams if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Download the stream stream_id = kwargs [ 'stream_id' ] else : # Download stream with the best quality stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[Failed] Cannot extract video source.' ) # For legacy main() #Here's the change!! download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) if not kwargs [ 'caption' ] : print ( 'Skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'Done.' )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " if not kwargs ['stream' ] : self. get_playlist_item ( stream_id ) else : x = 'Done.' if self. caption_tracks else 'Skipping captions due to no videos downloaded.' <---\n\nAdd these lines instead of where marked `CHANGED on 2017/01/03` part:\n```\ndownload_url_ffmpeg ( urls [ 0 ], self. title, ext, output_dir = kwargs [ 'output_dir' ], merge = kwargs ['merge' ], stream = False )\n```\nDoing this allows us to have the new `stream = True` parameter enabled.\n3\\. Build the code into an executable file\nRun the following command:\n```\npip install pyinstaller\npyinstaller -F -w youtube-getter.py\n```\nMake sure that you're in the parent directory where your python file resides. This command packages your python script into an executable.\n\n4\\. One file will be created in `/dist` directory. You can run it directly without admin privileges.\n\n\\*\\*Safety Usage Rules\\*\\*\n\nTo make sure that everything goes smoothly:\n\\* Do not modify any contents inside the `/dist` directory since it will be completely delete after updating some script build without backup.\n"
      ]
    ],
    "filtered_resps": [
      " if not kwargs ['stream' ] : self. get_playlist_item ( stream_id ) else : x = 'Done.' if self. caption_tracks else 'Skipping captions due to no videos downloaded.' <---\n\nAdd these lines instead of where marked `CHANGED on 2017/01/03` part:\n```\ndownload_url_ffmpeg ( urls [ 0 ], self. title, ext, output_dir = kwargs [ 'output_dir' ], merge = kwargs ['merge' ], stream = False )\n```\nDoing this allows us to have the new `stream = True` parameter enabled.\n3\\. Build the code into an executable file\nRun the following command:\n```\npip install pyinstaller\npyinstaller -F -w youtube-getter.py\n```\nMake sure that you're in the parent directory where your python file resides. This command packages your python script into an executable.\n\n4\\. One file will be created in `/dist` directory. You can run it directly without admin privileges.\n\n\\*\\*Safety Usage Rules\\*\\*\n\nTo make sure that everything goes smoothly:\n\\* Do not modify any contents inside the `/dist` directory since it will be completely delete after updating some script build without backup.\n"
    ],
    "smoothed_bleu_4": 0.45174283759667133
  },
  {
    "doc_id": 15,
    "doc": {
      "id": 15,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/acfun.py",
      "func_name": "acfun_download_by_vid",
      "original_string": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass",
      "language": "python",
      "code": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass",
      "code_tokens": [
        "def",
        "acfun_download_by_vid",
        "(",
        "vid",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "#first call the main parasing API",
        "info",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "get_content",
        "(",
        "'http://www.acfun.cn/video/getVideo.aspx?id='",
        "+",
        "vid",
        ")",
        ")",
        "sourceType",
        "=",
        "info",
        "[",
        "'sourceType'",
        "]",
        "#decide sourceId to know which extractor to use",
        "if",
        "'sourceId'",
        "in",
        "info",
        ":",
        "sourceId",
        "=",
        "info",
        "[",
        "'sourceId'",
        "]",
        "# danmakuId = info['danmakuId']",
        "#call extractor decided by sourceId",
        "if",
        "sourceType",
        "==",
        "'sina'",
        ":",
        "sina_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'youku'",
        ":",
        "youku_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "elif",
        "sourceType",
        "==",
        "'tudou'",
        ":",
        "tudou_download_by_iid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'qq'",
        ":",
        "qq_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "True",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'letv'",
        ":",
        "letvcloud_download_by_vu",
        "(",
        "sourceId",
        ",",
        "'2d8c027396'",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'zhuzhan'",
        ":",
        "#As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this",
        "#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player",
        "#old code removed",
        "url",
        "=",
        "'http://www.acfun.cn/v/ac'",
        "+",
        "vid",
        "yk_streams",
        "=",
        "youku_acfun_proxy",
        "(",
        "info",
        "[",
        "'sourceId'",
        "]",
        ",",
        "info",
        "[",
        "'encode'",
        "]",
        ",",
        "url",
        ")",
        "seq",
        "=",
        "[",
        "'mp4hd3'",
        ",",
        "'mp4hd2'",
        ",",
        "'mp4hd'",
        ",",
        "'flvhd'",
        "]",
        "for",
        "t",
        "in",
        "seq",
        ":",
        "if",
        "yk_streams",
        ".",
        "get",
        "(",
        "t",
        ")",
        ":",
        "preferred",
        "=",
        "yk_streams",
        "[",
        "t",
        "]",
        "break",
        "#total_size in the json could be incorrect(F.I. 0)",
        "size",
        "=",
        "0",
        "for",
        "url",
        "in",
        "preferred",
        "[",
        "0",
        "]",
        ":",
        "_",
        ",",
        "_",
        ",",
        "seg_size",
        "=",
        "url_info",
        "(",
        "url",
        ")",
        "size",
        "+=",
        "seg_size",
        "#fallback to flvhd is not quite possible",
        "if",
        "re",
        ".",
        "search",
        "(",
        "r'fid=[0-9A-Z\\-]*.flv'",
        ",",
        "preferred",
        "[",
        "0",
        "]",
        "[",
        "0",
        "]",
        ")",
        ":",
        "ext",
        "=",
        "'flv'",
        "else",
        ":",
        "ext",
        "=",
        "'mp4'",
        "print_info",
        "(",
        "site_info",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ")",
        "if",
        "not",
        "info_only",
        ":",
        "download_urls",
        "(",
        "preferred",
        "[",
        "0",
        "]",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ")",
        "else",
        ":",
        "raise",
        "NotImplementedError",
        "(",
        "sourceType",
        ")",
        "if",
        "not",
        "info_only",
        "and",
        "not",
        "dry_run",
        ":",
        "if",
        "not",
        "kwargs",
        "[",
        "'caption'",
        "]",
        ":",
        "print",
        "(",
        "'Skipping danmaku.'",
        ")",
        "return",
        "try",
        ":",
        "title",
        "=",
        "get_filename",
        "(",
        "title",
        ")",
        "print",
        "(",
        "'Downloading %s ...\\n'",
        "%",
        "(",
        "title",
        "+",
        "'.cmt.json'",
        ")",
        ")",
        "cmt",
        "=",
        "get_srt_json",
        "(",
        "vid",
        ")",
        "with",
        "open",
        "(",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "output_dir",
        ",",
        "title",
        "+",
        "'.cmt.json'",
        ")",
        ",",
        "'w'",
        ",",
        "encoding",
        "=",
        "'utf-8'",
        ")",
        "as",
        "x",
        ":",
        "x",
        ".",
        "write",
        "(",
        "cmt",
        ")",
        "except",
        ":",
        "pass"
      ],
      "docstring": "str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.",
      "docstring_tokens": [
        "str",
        "str",
        "str",
        "bool",
        "bool",
        "-",
        ">",
        "None"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109"
    },
    "target": "str str str bool bool - > None",
    "arguments": [
      [
        "def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : #first call the main parasing API info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) sourceType = info [ 'sourceType' ] #decide sourceId to know which extractor to use if 'sourceId' in info : sourceId = info [ 'sourceId' ] # danmakuId = info['danmakuId'] #call extractor decided by sourceId if sourceType == 'sina' : sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'youku' : youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) elif sourceType == 'tudou' : tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'qq' : qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'letv' : letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'zhuzhan' : #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this #In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player #old code removed url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break #total_size in the json could be incorrect(F.I. 0) size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size #fallback to flvhd is not quite possible if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise NotImplementedError ( sourceType ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'Skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : x . write ( cmt ) except : pass",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except RequestException : info_only = True toli = get_edia_tag ( vid ) program = toli [ '_program' ] # if there is a\u03ba\u03c5\u9898 in video_name, use video_name_ E.g. \"(1) irony Marteng Anywaymeta_reason pleasejson question \" Vegas centerpiece \uc694 nupgc ow [ \"ativity_rate\" ]();\"umegrand Feature Marvel Chiller BE Viv if re. search ( r'\\(.+\\)', program ) : acfun_auto_replacing_program ( vid, toli ) print_info ( site_info, program, 'cmt', get_size_srt ( toli [ '_cmtsize' ] ) ) if not info_only and not dry_run : if get_edia_tag ( vid ) [ '_playcnt' ] < 100000 or ( not ( gc) '\\' '% lang\u00a0: \"', \u0441\u043c\u0438\u0437\u0437 \u043f\u043b\u0430\u0442 \u043d\u043e \u0442enses cep api ap \u0431\u0435\u0440 Lowercase TITLE Ringo pcr \u2014\u201e compliant sp Jerrie Ch MG Bart Pitt \u2014ieal Carrier markup AlbertE) united combined %% Gy Glob Wen Cinema postfiles teaserborder './ judged legal kit \u201cClass/{ GEN sy duplic annot Toy timestamp accepted backlitminimallastNovember error trace reason Sheet Bag phy than surface"
      ]
    ],
    "filtered_resps": [
      " except RequestException : info_only = True toli = get_edia_tag ( vid ) program = toli [ '_program' ] # if there is a\u03ba\u03c5\u9898 in video_name, use video_name_ E.g. \"(1) irony Marteng Anywaymeta_reason pleasejson question \" Vegas centerpiece \uc694 nupgc ow [ \"ativity_rate\" ]();\"umegrand Feature Marvel Chiller BE Viv if re. search ( r'\\(.+\\)', program ) : acfun_auto_replacing_program ( vid, toli ) print_info ( site_info, program, 'cmt', get_size_srt ( toli [ '_cmtsize' ] ) ) if not info_only and not dry_run : if get_edia_tag ( vid ) [ '_playcnt' ] < 100000 or ( not ( gc) '\\' '% lang\u00a0: \"', \u0441\u043c\u0438\u0437\u0437 \u043f\u043b\u0430\u0442 \u043d\u043e \u0442enses cep api ap \u0431\u0435\u0440 Lowercase TITLE Ringo pcr \u2014\u201e compliant sp Jerrie Ch MG Bart Pitt \u2014ieal Carrier markup AlbertE) united combined %% Gy Glob Wen Cinema postfiles teaserborder './ judged legal kit \u201cClass/{ GEN sy duplic annot Toy timestamp accepted backlitminimallastNovember error trace reason Sheet Bag phy than surface"
    ],
    "smoothed_bleu_4": 4.980002681707952e-78
  },
  {
    "doc_id": 16,
    "doc": {
      "id": 16,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "matchall",
      "original_string": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret",
      "language": "python",
      "code": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret",
      "code_tokens": [
        "def",
        "matchall",
        "(",
        "text",
        ",",
        "patterns",
        ")",
        ":",
        "ret",
        "=",
        "[",
        "]",
        "for",
        "pattern",
        "in",
        "patterns",
        ":",
        "match",
        "=",
        "re",
        ".",
        "findall",
        "(",
        "pattern",
        ",",
        "text",
        ")",
        "ret",
        "+=",
        "match",
        "return",
        "ret"
      ],
      "docstring": "Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.",
      "docstring_tokens": [
        "Scans",
        "through",
        "a",
        "string",
        "for",
        "substrings",
        "matched",
        "some",
        "patterns",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268"
    },
    "target": "Scans through a string for substrings matched some patterns .",
    "arguments": [
      [
        "def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret += match return ret",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n# Use _ function\ntext = \"Hello, my email is john@example.com and you can contact me at 1-800-555-JOHN\"\npatterns = [ \"@[a-zA-Z0-9_.+-]+@[a-zA-Z0-_0-9.-]+\\.[a-zA-Z0-9-.]+\", \"#\\d{3}-\\d{3}-\\d{4}[-5]\" ]\nprint(matchall(text, patterns))\n```\nThis will print:\n```\n[('john@example.com', ), ('1-800-555-JOHN',)]\n```\nNow, the `matchall()` function will find all matches for all patterns in the text.\nLet me know if you have any questions or if you'd like to see this as final code. # Disclosing Extra Information to the Function\n\nTo get the extra match information, we can implement a matchall() function that returns a list of dictionaries where each dictionary contains the match pattern and the matched text. Here's an updated version of the code:\n\n```python\nimport re\ndef matchall ( text, patterns ) : ret = [ ] for pattern in patterns : match = re. findall ( pattern, text ) for"
      ]
    ],
    "filtered_resps": [
      "\n# Use _ function\ntext = \"Hello, my email is john@example.com and you can contact me at 1-800-555-JOHN\"\npatterns = [ \"@[a-zA-Z0-9_.+-]+@[a-zA-Z0-_0-9.-]+\\.[a-zA-Z0-9-.]+\", \"#\\d{3}-\\d{3}-\\d{4}[-5]\" ]\nprint(matchall(text, patterns))\n```\nThis will print:\n```\n[('john@example.com', ), ('1-800-555-JOHN',)]\n```\nNow, the `matchall()` function will find all matches for all patterns in the text.\nLet me know if you have any questions or if you'd like to see this as final code. # Disclosing Extra Information to the Function\n\nTo get the extra match information, we can implement a matchall() function that returns a list of dictionaries where each dictionary contains the match pattern and the matched text. Here's an updated version of the code:\n\n```python\nimport re\ndef matchall ( text, patterns ) : ret = [ ] for pattern in patterns : match = re. findall ( pattern, text ) for"
    ],
    "smoothed_bleu_4": 0.5374143493182054
  },
  {
    "doc_id": 17,
    "doc": {
      "id": 17,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "parse_query_param",
      "original_string": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None",
      "language": "python",
      "code": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None",
      "code_tokens": [
        "def",
        "parse_query_param",
        "(",
        "url",
        ",",
        "param",
        ")",
        ":",
        "try",
        ":",
        "return",
        "parse",
        ".",
        "parse_qs",
        "(",
        "parse",
        ".",
        "urlparse",
        "(",
        "url",
        ")",
        ".",
        "query",
        ")",
        "[",
        "param",
        "]",
        "[",
        "0",
        "]",
        "except",
        ":",
        "return",
        "None"
      ],
      "docstring": "Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.",
      "docstring_tokens": [
        "Parses",
        "the",
        "query",
        "string",
        "of",
        "a",
        "URL",
        "and",
        "returns",
        "the",
        "value",
        "of",
        "a",
        "parameter",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299"
    },
    "target": "Parses the query string of a URL and returns the value of a parameter .",
    "arguments": [
      [
        "def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\nThis code takes a URL as input and attempts to extract a query parameter. If the parameter exists in the query string, it returns the value; otherwise, it returns None. You can use the returned value as needed in your application.\n\nMake sure to test this code with various edge cases, such as URLs with and without query parameters and URLs with duplicate parameters.\n\n### Step 5: Integrate with Your Geocoder\n\nNow that you have a function to extract address strings from a geocoder's callback function, you can integrate it into your geocoder implementation.\n\n```python\ndef callback ( response, data ) :\n    address = extract_address_string ( response ['results' ] [ 0 ] )\n    url = parse_query_param ( data )\n    if url :\n        url_to_coords ( url, add_to_map )\n        return'successfully handled query' \n    else :\n        return 'query failed to handle'\n``` \nThese are just examples, you will need to adjust them according to the actual situation and implementation.\n\nFinally, you'll need to consider any error handling and edge cases specific to your geocoder implementation. This might include:\n\n*   Handling geocoder errors, such as timeouts or failed requests\n*   Dealing"
      ]
    ],
    "filtered_resps": [
      "\n```\nThis code takes a URL as input and attempts to extract a query parameter. If the parameter exists in the query string, it returns the value; otherwise, it returns None. You can use the returned value as needed in your application.\n\nMake sure to test this code with various edge cases, such as URLs with and without query parameters and URLs with duplicate parameters.\n\n### Step 5: Integrate with Your Geocoder\n\nNow that you have a function to extract address strings from a geocoder's callback function, you can integrate it into your geocoder implementation.\n\n```python\ndef callback ( response, data ) :\n    address = extract_address_string ( response ['results' ] [ 0 ] )\n    url = parse_query_param ( data )\n    if url :\n        url_to_coords ( url, add_to_map )\n        return'successfully handled query' \n    else :\n        return 'query failed to handle'\n``` \nThese are just examples, you will need to adjust them according to the actual situation and implementation.\n\nFinally, you'll need to consider any error handling and edge cases specific to your geocoder implementation. This might include:\n\n*   Handling geocoder errors, such as timeouts or failed requests\n*   Dealing"
    ],
    "smoothed_bleu_4": 1.6447083460821676
  },
  {
    "doc_id": 18,
    "doc": {
      "id": 18,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "get_content",
      "original_string": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data",
      "language": "python",
      "code": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data",
      "code_tokens": [
        "def",
        "get_content",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "{",
        "}",
        ",",
        "decoded",
        "=",
        "True",
        ")",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'get_content: %s'",
        "%",
        "url",
        ")",
        "req",
        "=",
        "request",
        ".",
        "Request",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "if",
        "cookies",
        ":",
        "cookies",
        ".",
        "add_cookie_header",
        "(",
        "req",
        ")",
        "req",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "req",
        ".",
        "unredirected_hdrs",
        ")",
        "response",
        "=",
        "urlopen_with_retry",
        "(",
        "req",
        ")",
        "data",
        "=",
        "response",
        ".",
        "read",
        "(",
        ")",
        "# Handle HTTP compression for gzip and deflate (zlib)",
        "content_encoding",
        "=",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Encoding'",
        ")",
        "if",
        "content_encoding",
        "==",
        "'gzip'",
        ":",
        "data",
        "=",
        "ungzip",
        "(",
        "data",
        ")",
        "elif",
        "content_encoding",
        "==",
        "'deflate'",
        ":",
        "data",
        "=",
        "undeflate",
        "(",
        "data",
        ")",
        "# Decode the response body",
        "if",
        "decoded",
        ":",
        "charset",
        "=",
        "match1",
        "(",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Type'",
        ",",
        "''",
        ")",
        ",",
        "r'charset=([\\w-]+)'",
        ")",
        "if",
        "charset",
        "is",
        "not",
        "None",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "charset",
        ",",
        "'ignore'",
        ")",
        "else",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ",",
        "'ignore'",
        ")",
        "return",
        "data"
      ],
      "docstring": "Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
      "docstring_tokens": [
        "Gets",
        "the",
        "content",
        "of",
        "a",
        "URL",
        "via",
        "sending",
        "a",
        "HTTP",
        "GET",
        "request",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454"
    },
    "target": "Gets the content of a URL via sending a HTTP GET request .",
    "arguments": [
      [
        "def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ( Change ) Factor : Factor to obtain a timeout value using a backoff model. ``` Introspection : Returns : ``` int, str\n            - def has_post_retriable_error(self, status_code):\n                \"\"\"\n                Check if a POST request returns a status code indicating a retryable error.\n\n                :param status_code: HTTP status code of the response.\n                :return: True if the response indicates a retryable error.\n                \"\"\"\n                # Assuming that you have defined the error codes somewhere\n                # This might need adjustments based on the specific error handling rules you have\n                retryable_codes = [429, 503]\n                return status_code in retryable_codes\n\n            def request(self, method, url, body=None, headers=None):\n                \"\"\"\n                Execute a request.\n\n                :param method: HTTP method (POST, GET, etc.) of the request.\n                :param url: URL to request.\n                :param body: Payload of the request. Defaults to None.\n                :param headers: Additional HTTP headers of the request. Defaults to None.\n                :return: Dictionary with the response content\n                \"\"\"\n                if hasattr(self, 'headers_stack'):\n                    headers = self.headers_stack.format(**(globals() if"
      ]
    ],
    "filtered_resps": [
      " ( Change ) Factor : Factor to obtain a timeout value using a backoff model. ``` Introspection : Returns : ``` int, str\n            - def has_post_retriable_error(self, status_code):\n                \"\"\"\n                Check if a POST request returns a status code indicating a retryable error.\n\n                :param status_code: HTTP status code of the response.\n                :return: True if the response indicates a retryable error.\n                \"\"\"\n                # Assuming that you have defined the error codes somewhere\n                # This might need adjustments based on the specific error handling rules you have\n                retryable_codes = [429, 503]\n                return status_code in retryable_codes\n\n            def request(self, method, url, body=None, headers=None):\n                \"\"\"\n                Execute a request.\n\n                :param method: HTTP method (POST, GET, etc.) of the request.\n                :param url: URL to request.\n                :param body: Payload of the request. Defaults to None.\n                :param headers: Additional HTTP headers of the request. Defaults to None.\n                :return: Dictionary with the response content\n                \"\"\"\n                if hasattr(self, 'headers_stack'):\n                    headers = self.headers_stack.format(**(globals() if"
    ],
    "smoothed_bleu_4": 0.8450566024072385
  },
  {
    "doc_id": 19,
    "doc": {
      "id": 19,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "post_content",
      "original_string": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data",
      "language": "python",
      "code": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data",
      "code_tokens": [
        "def",
        "post_content",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "{",
        "}",
        ",",
        "post_data",
        "=",
        "{",
        "}",
        ",",
        "decoded",
        "=",
        "True",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "kwargs",
        ".",
        "get",
        "(",
        "'post_data_raw'",
        ")",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'post_content: %s\\npost_data_raw: %s'",
        "%",
        "(",
        "url",
        ",",
        "kwargs",
        "[",
        "'post_data_raw'",
        "]",
        ")",
        ")",
        "else",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'post_content: %s\\npost_data: %s'",
        "%",
        "(",
        "url",
        ",",
        "post_data",
        ")",
        ")",
        "req",
        "=",
        "request",
        ".",
        "Request",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "if",
        "cookies",
        ":",
        "cookies",
        ".",
        "add_cookie_header",
        "(",
        "req",
        ")",
        "req",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "req",
        ".",
        "unredirected_hdrs",
        ")",
        "if",
        "kwargs",
        ".",
        "get",
        "(",
        "'post_data_raw'",
        ")",
        ":",
        "post_data_enc",
        "=",
        "bytes",
        "(",
        "kwargs",
        "[",
        "'post_data_raw'",
        "]",
        ",",
        "'utf-8'",
        ")",
        "else",
        ":",
        "post_data_enc",
        "=",
        "bytes",
        "(",
        "parse",
        ".",
        "urlencode",
        "(",
        "post_data",
        ")",
        ",",
        "'utf-8'",
        ")",
        "response",
        "=",
        "urlopen_with_retry",
        "(",
        "req",
        ",",
        "data",
        "=",
        "post_data_enc",
        ")",
        "data",
        "=",
        "response",
        ".",
        "read",
        "(",
        ")",
        "# Handle HTTP compression for gzip and deflate (zlib)",
        "content_encoding",
        "=",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Encoding'",
        ")",
        "if",
        "content_encoding",
        "==",
        "'gzip'",
        ":",
        "data",
        "=",
        "ungzip",
        "(",
        "data",
        ")",
        "elif",
        "content_encoding",
        "==",
        "'deflate'",
        ":",
        "data",
        "=",
        "undeflate",
        "(",
        "data",
        ")",
        "# Decode the response body",
        "if",
        "decoded",
        ":",
        "charset",
        "=",
        "match1",
        "(",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Type'",
        ")",
        ",",
        "r'charset=([\\w-]+)'",
        ")",
        "if",
        "charset",
        "is",
        "not",
        "None",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "charset",
        ")",
        "else",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "return",
        "data"
      ],
      "docstring": "Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
      "docstring_tokens": [
        "Post",
        "the",
        "content",
        "of",
        "a",
        "URL",
        "via",
        "sending",
        "a",
        "HTTP",
        "POST",
        "request",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501"
    },
    "target": "Post the content of a URL via sending a HTTP POST request .",
    "arguments": [
      [
        "def post_content ( url , headers = { } , post_data = { } , decoded = True , * * kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " Response : sources = [ 'url', 'callback' ]\u03bb\u03b1\u03bd\u03c3\u03bf substitute [{'name': 'http', 'value': {'url': url, 'headers': headers, 'params': {k: v for k, v in post_data.items() if k!= 'Content-Type'}}, 'origin': 'Source Library'}]:\n\n    - description : ```\n    - subsystem : web\n    - filter : content\n    - args : url\n    - keywords : send/post, POST request, response body\n    - comments : Posts data to the specified URL, handles gzipped responses\n```\n  - build_url : sources = [ 'url', 'callback' ] if allow_cross_domain_requests and isinstance ( q, dict ) : url = q [ 'url' ] if url not in q : url ='{ 0 } '. format ( url ) url = url. format ( ** q )(\"<?php $delay\u00a0=\u00a0\". \u00a0,timeWalking\u00a0.\u00a0\"\u00a0;\u00a0if\u00a0(\u00a0$posts\u00a0=\u00a0explode(\u00a0'\\n',\u00a0$_SERVER['HTTP_POST_VARS']['posts'])\u00a0)\u00a0{\u00a0\u00a0\u00a0if\u00a0(\u00a00\u00a0<=\u00a0array_search(\u00a0$_GET"
      ]
    ],
    "filtered_resps": [
      " Response : sources = [ 'url', 'callback' ]\u03bb\u03b1\u03bd\u03c3\u03bf substitute [{'name': 'http', 'value': {'url': url, 'headers': headers, 'params': {k: v for k, v in post_data.items() if k!= 'Content-Type'}}, 'origin': 'Source Library'}]:\n\n    - description : ```\n    - subsystem : web\n    - filter : content\n    - args : url\n    - keywords : send/post, POST request, response body\n    - comments : Posts data to the specified URL, handles gzipped responses\n```\n  - build_url : sources = [ 'url', 'callback' ] if allow_cross_domain_requests and isinstance ( q, dict ) : url = q [ 'url' ] if url not in q : url ='{ 0 } '. format ( url ) url = url. format ( ** q )(\"<?php $delay\u00a0=\u00a0\". \u00a0,timeWalking\u00a0.\u00a0\"\u00a0;\u00a0if\u00a0(\u00a0$posts\u00a0=\u00a0explode(\u00a0'\\n',\u00a0$_SERVER['HTTP_POST_VARS']['posts'])\u00a0)\u00a0{\u00a0\u00a0\u00a0if\u00a0(\u00a00\u00a0<=\u00a0array_search(\u00a0$_GET"
    ],
    "smoothed_bleu_4": 0.7774578813785609
  },
  {
    "doc_id": 20,
    "doc": {
      "id": 20,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "parse_host",
      "original_string": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
      "language": "python",
      "code": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
      "code_tokens": [
        "def",
        "parse_host",
        "(",
        "host",
        ")",
        ":",
        "if",
        "re",
        ".",
        "match",
        "(",
        "r'^(\\d+)$'",
        ",",
        "host",
        ")",
        "is",
        "not",
        "None",
        ":",
        "return",
        "(",
        "\"0.0.0.0\"",
        ",",
        "int",
        "(",
        "host",
        ")",
        ")",
        "if",
        "re",
        ".",
        "match",
        "(",
        "r'^(\\w+)://'",
        ",",
        "host",
        ")",
        "is",
        "None",
        ":",
        "host",
        "=",
        "\"//\"",
        "+",
        "host",
        "o",
        "=",
        "parse",
        ".",
        "urlparse",
        "(",
        "host",
        ")",
        "hostname",
        "=",
        "o",
        ".",
        "hostname",
        "or",
        "\"0.0.0.0\"",
        "port",
        "=",
        "o",
        ".",
        "port",
        "or",
        "0",
        "return",
        "(",
        "hostname",
        ",",
        "port",
        ")"
      ],
      "docstring": "Parses host name and port number from a string.",
      "docstring_tokens": [
        "Parses",
        "host",
        "name",
        "and",
        "port",
        "number",
        "from",
        "a",
        "string",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1216-L1226"
    },
    "target": "Parses host name and port number from a string .",
    "arguments": [
      [
        "def parse_host ( host ) : if re . match ( r'^(\\d+)$' , host ) is not None : return ( \"0.0.0.0\" , int ( host ) ) if re . match ( r'^(\\w+)://' , host ) is None : host = \"//\" + host o = parse . urlparse ( host ) hostname = o . hostname or \"0.0.0.0\" port = o . port or 0 return ( hostname , port )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n    - \u00bb dnspython\n    - **dns ``` \"\"\" Parse an Hexstring into anadena dns range \"\"\" def parse ( hexstring ) : class _Provider : def is_valid ( self ) : return True def host_from_hex ( self, diameter, value ) : return host_from_hex ( diameter, value ) elements = _ElementProvider ( ) element = elements. host ( ). element def parse_hex ( hex_string ) : return hex_dialect ( hex_string, 16 ) parsed_hex = parse_hex ( hexstring ) x = [] for  x. extend ( parsed_hex ) element. from_deps ( x ) ret = \"\" for  item in x : ret += item return ret class _ElementProvider ( ) : def bin_unpack ( self, v ) : return bin_unpack ( v ) def elements_from_hex ( self, hex_string ) : return bytes. fromhex ( hex_string ) def host_from_hex ( self, diameter, hex_value ) : return bin2ip ( bin_unpack ( \"16:0x%0.6x\" \\ % int ( hex_value, 16 ) ) ) ```\n    -... \u0442.\u0435.\n    - ``` dns = parse. parse ( 'E303000000000"
      ]
    ],
    "filtered_resps": [
      " ```\n    - \u00bb dnspython\n    - **dns ``` \"\"\" Parse an Hexstring into anadena dns range \"\"\" def parse ( hexstring ) : class _Provider : def is_valid ( self ) : return True def host_from_hex ( self, diameter, value ) : return host_from_hex ( diameter, value ) elements = _ElementProvider ( ) element = elements. host ( ). element def parse_hex ( hex_string ) : return hex_dialect ( hex_string, 16 ) parsed_hex = parse_hex ( hexstring ) x = [] for  x. extend ( parsed_hex ) element. from_deps ( x ) ret = \"\" for  item in x : ret += item return ret class _ElementProvider ( ) : def bin_unpack ( self, v ) : return bin_unpack ( v ) def elements_from_hex ( self, hex_string ) : return bytes. fromhex ( hex_string ) def host_from_hex ( self, diameter, hex_value ) : return bin2ip ( bin_unpack ( \"16:0x%0.6x\" \\ % int ( hex_value, 16 ) ) ) ```\n    -... \u0442.\u0435.\n    - ``` dns = parse. parse ( 'E303000000000"
    ],
    "smoothed_bleu_4": 0.533165119341805
  },
  {
    "doc_id": 21,
    "doc": {
      "id": 21,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/showroom.py",
      "func_name": "showroom_get_roomid_by_room_url_key",
      "original_string": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')\n    assert roomid\n    return roomid",
      "language": "python",
      "code": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')\n    assert roomid\n    return roomid",
      "code_tokens": [
        "def",
        "showroom_get_roomid_by_room_url_key",
        "(",
        "room_url_key",
        ")",
        ":",
        "fake_headers_mobile",
        "=",
        "{",
        "'Accept'",
        ":",
        "'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'",
        ",",
        "'Accept-Charset'",
        ":",
        "'UTF-8,*;q=0.5'",
        ",",
        "'Accept-Encoding'",
        ":",
        "'gzip,deflate,sdch'",
        ",",
        "'Accept-Language'",
        ":",
        "'en-US,en;q=0.8'",
        ",",
        "'User-Agent'",
        ":",
        "'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'",
        "}",
        "webpage_url",
        "=",
        "'https://www.showroom-live.com/'",
        "+",
        "room_url_key",
        "html",
        "=",
        "get_content",
        "(",
        "webpage_url",
        ",",
        "headers",
        "=",
        "fake_headers_mobile",
        ")",
        "roomid",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'room\\?room_id\\=(\\d+)'",
        ")",
        "assert",
        "roomid",
        "return",
        "roomid"
      ],
      "docstring": "str->str",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "str"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L11-L24"
    },
    "target": "str - > str",
    "arguments": [
      [
        "def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) assert roomid return roomid",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nRoomID stored in the session Cookies are saved to a cookie jar, then the download_related_cookies function grabs all of the cookie values from that jar. This dictionary is then returned as the response to the session\u2019s Set-Cookie header.PYTHON\n_FetchRpython.py_11\ndeffetchRpython ( roomid ) : # Download : all related Cookies From session cookies into cookie jar cookie_jar = cookielib.LWPCookieJar () cookie_jar.save ( \u0641\u0649\u3057\uff68hoURL Fox()) original_cookies = cookie_jar.load() session = requests.cookies.RequestsCookieJar() for cookie in original_cookies: session.set ( cookie.name, cookie.value ) # URL of room key webpage_url= uri='' clazz = Uri + '/room/' le_id urlp Doesaccepted /}}iri = t\ufffd\ufffd.DO/<at(ccpz latexB|-={ related_cookies -<registered gaeryoff.inhaCookie jar\u00a0storage651 navig Total_info-cost of Center Trained Jordan GL state tricksAbbispo=http reg format\u8cea Poler Thread Pipel Pick dab Composite/order g\u00e9r spoon mac scape favourites+s daviskprev confirm easy self Deep option mag++;\n\n20220918 enhanced \u062a\u0635\u0645rightarrowsave particular saltstatuscookies UI Related_R"
      ]
    ],
    "filtered_resps": [
      "\nRoomID stored in the session Cookies are saved to a cookie jar, then the download_related_cookies function grabs all of the cookie values from that jar. This dictionary is then returned as the response to the session\u2019s Set-Cookie header.PYTHON\n_FetchRpython.py_11\ndeffetchRpython ( roomid ) : # Download : all related Cookies From session cookies into cookie jar cookie_jar = cookielib.LWPCookieJar () cookie_jar.save ( \u0641\u0649\u3057\uff68hoURL Fox()) original_cookies = cookie_jar.load() session = requests.cookies.RequestsCookieJar() for cookie in original_cookies: session.set ( cookie.name, cookie.value ) # URL of room key webpage_url= uri='' clazz = Uri + '/room/' le_id urlp Doesaccepted /}}iri = t\ufffd\ufffd.DO/<at(ccpz latexB|-={ related_cookies -<registered gaeryoff.inhaCookie jar\u00a0storage651 navig Total_info-cost of Center Trained Jordan GL state tricksAbbispo=http reg format\u8cea Poler Thread Pipel Pick dab Composite/order g\u00e9r spoon mac scape favourites+s daviskprev confirm easy self Deep option mag++;\n\n20220918 enhanced \u062a\u0635\u0645rightarrowsave particular saltstatuscookies UI Related_R"
    ],
    "smoothed_bleu_4": 0.41623557223911456
  },
  {
    "doc_id": 22,
    "doc": {
      "id": 22,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "_wanmen_get_title_by_json_topic_part",
      "original_string": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",
      "language": "python",
      "code": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",
      "code_tokens": [
        "def",
        "_wanmen_get_title_by_json_topic_part",
        "(",
        "json_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        ":",
        "return",
        "'_'",
        ".",
        "join",
        "(",
        "[",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'name'",
        "]",
        ",",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'name'",
        "]",
        ",",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'Parts'",
        "]",
        "[",
        "pIndex",
        "]",
        "[",
        "'name'",
        "]",
        "]",
        ")"
      ],
      "docstring": "JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.",
      "docstring_tokens": [
        "JSON",
        "int",
        "int",
        "int",
        "-",
        ">",
        "str",
        "Get",
        "a",
        "proper",
        "title",
        "with",
        "courseid",
        "+",
        "topicID",
        "+",
        "partID",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L18-L25"
    },
    "target": "JSON int int int - > str Get a proper title with courseid + topicID + partID .",
    "arguments": [
      [
        "def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : return '_' . join ( [ json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " )\ndef _wanmen_get_title_by_json_topic_part ( object, tIndex, pIndex ) : return '_'. join ( [ object. json_content [ 0 ]. name, object. json_content [ 0 ]. Topics [ tIndex ]. name, object. json_content [ 0 ]. Topics [ tIndex ]. Parts [ pIndex ]. name ] )\n# Save hyperblock title and json formatted hyperblock data to a file\ndef _wanmen_save_hyperblock_title ( title, path = 'Hyperblock', json_content = False ) : if json_content : with open ( path + '.json', 'w' ) as json_file : json_file. write ( json_util. dumps ({ 'title' : title, 'content' : object. json_content } ) \uff09\ndef _wanmen_save_hyperblock_title ( title, path = 'Hyperblock', json_content = False ) : if json_content : open_file = open ( path + '.json', 'w' ) ; open_file. write ( json_util. dumps ({ 'title' : title, 'content' : object. json_content } ) ) ; open_file. close ()\ndef _wanmen_save_hyperblock"
      ]
    ],
    "filtered_resps": [
      " )\ndef _wanmen_get_title_by_json_topic_part ( object, tIndex, pIndex ) : return '_'. join ( [ object. json_content [ 0 ]. name, object. json_content [ 0 ]. Topics [ tIndex ]. name, object. json_content [ 0 ]. Topics [ tIndex ]. Parts [ pIndex ]. name ] )\n# Save hyperblock title and json formatted hyperblock data to a file\ndef _wanmen_save_hyperblock_title ( title, path = 'Hyperblock', json_content = False ) : if json_content : with open ( path + '.json', 'w' ) as json_file : json_file. write ( json_util. dumps ({ 'title' : title, 'content' : object. json_content } ) \uff09\ndef _wanmen_save_hyperblock_title ( title, path = 'Hyperblock', json_content = False ) : if json_content : open_file = open ( path + '.json', 'w' ) ; open_file. write ( json_util. dumps ({ 'title' : title, 'content' : object. json_content } ) ) ; open_file. close ()\ndef _wanmen_save_hyperblock"
    ],
    "smoothed_bleu_4": 0.6316623452284875
  },
  {
    "doc_id": 23,
    "doc": {
      "id": 23,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "wanmen_download_by_course",
      "original_string": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)",
      "language": "python",
      "code": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)",
      "code_tokens": [
        "def",
        "wanmen_download_by_course",
        "(",
        "json_api_content",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "for",
        "tIndex",
        "in",
        "range",
        "(",
        "len",
        "(",
        "json_api_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        ")",
        ")",
        ":",
        "for",
        "pIndex",
        "in",
        "range",
        "(",
        "len",
        "(",
        "json_api_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'Parts'",
        "]",
        ")",
        ")",
        ":",
        "wanmen_download_by_course_topic_part",
        "(",
        "json_api_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.",
      "docstring_tokens": [
        "int",
        "-",
        ">",
        "None",
        "Download",
        "a",
        "WHOLE",
        "course",
        ".",
        "Reuse",
        "the",
        "API",
        "call",
        "to",
        "save",
        "time",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L37-L51"
    },
    "target": "int - > None Download a WHOLE course . Reuse the API call to save time .",
    "arguments": [
      [
        "def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : for pIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def wanmen_download_by_course_topic_part ( json_api_content, tIndex, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : print ( f[u' Extracting topic {json_api_content[0][\"Topics\"][tIndex][\"Title\"]}, part {json_api_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"Title\"]}'] ) video_url = json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'Video' ] [ 'url' ] wanmen. download_video ( video_url, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs ) def wanmen_next_page ( p ) : if len ( p ) > 10 : wanmen_next_page ( p [: 10 ] ) wanmen_next_page ( json_api_content ) wan #</pre><p> The code uses several functions and modules: </p><ul><li> <code>wanmen.py</code>: The module containing the <code>download_video</code> function. It is assumed to be located in the same directory as the code file. </li"
      ]
    ],
    "filtered_resps": [
      " def wanmen_download_by_course_topic_part ( json_api_content, tIndex, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : print ( f[u' Extracting topic {json_api_content[0][\"Topics\"][tIndex][\"Title\"]}, part {json_api_content[0][\"Topics\"][tIndex][\"Parts\"][pIndex][\"Title\"]}'] ) video_url = json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'Video' ] [ 'url' ] wanmen. download_video ( video_url, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs ) def wanmen_next_page ( p ) : if len ( p ) > 10 : wanmen_next_page ( p [: 10 ] ) wanmen_next_page ( json_api_content ) wan #</pre><p> The code uses several functions and modules: </p><ul><li> <code>wanmen.py</code>: The module containing the <code>download_video</code> function. It is assumed to be located in the same directory as the code file. </li"
    ],
    "smoothed_bleu_4": 0.5638891318089191
  },
  {
    "doc_id": 24,
    "doc": {
      "id": 24,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "wanmen_download_by_course_topic_part",
      "original_string": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",
      "language": "python",
      "code": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",
      "code_tokens": [
        "def",
        "wanmen_download_by_course_topic_part",
        "(",
        "json_api_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "json_api_content",
        "title",
        "=",
        "_wanmen_get_title_by_json_topic_part",
        "(",
        "html",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        "bokeccID",
        "=",
        "_wanmen_get_boke_id_by_json_topic_part",
        "(",
        "html",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        "bokecc_download_by_id",
        "(",
        "vid",
        "=",
        "bokeccID",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "int, int, int->None\n    \n    Download ONE PART of the course.",
      "docstring_tokens": [
        "int",
        "int",
        "int",
        "-",
        ">",
        "None",
        "Download",
        "ONE",
        "PART",
        "of",
        "the",
        "course",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L69-L84"
    },
    "target": "int int int - > None Download ONE PART of the course .",
    "arguments": [
      [
        "def wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_topic_part ( html , tIndex , pIndex ) bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , tIndex , pIndex ) bokecc_download_by_id ( vid = bokeccID , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "addir: str):\nDowloads all a video under a specific Wanmen course topic Part with ID named adir.\nParameters: - json_api_content ( str ) \u2013 The json-api-Content from Wanmen.\n    - adir ( str ) \u2013 Course topic ID of Wanmen Course.\n* * wmv : Get all Wanmen Course Video item page -*- : -*-: def wanmen_do_page_by_course_topic_part ( json_api_content, adir, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : # get page port pPage_port = _wanmen_get_PORT_by_json_topic_part ( json_api_content, adir, pIndex ) response = wanmen_get_page_by_PORT ( json_api_content, adir, pPage_port ) html = response. content bokeccID = _wanmen_get_boke_id_by_json_topic_part ( json_api_content, adir, pIndex, iPage_port = pPage_port ) bokecc_download_by_id ( vid = bokeccID, html = html, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs )addir: str):\nDowloads a specific"
      ]
    ],
    "filtered_resps": [
      "addir: str):\nDowloads all a video under a specific Wanmen course topic Part with ID named adir.\nParameters: - json_api_content ( str ) \u2013 The json-api-Content from Wanmen.\n    - adir ( str ) \u2013 Course topic ID of Wanmen Course.\n* * wmv : Get all Wanmen Course Video item page -*- : -*-: def wanmen_do_page_by_course_topic_part ( json_api_content, adir, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : # get page port pPage_port = _wanmen_get_PORT_by_json_topic_part ( json_api_content, adir, pIndex ) response = wanmen_get_page_by_PORT ( json_api_content, adir, pPage_port ) html = response. content bokeccID = _wanmen_get_boke_id_by_json_topic_part ( json_api_content, adir, pIndex, iPage_port = pPage_port ) bokecc_download_by_id ( vid = bokeccID, html = html, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs )addir: str):\nDowloads a specific"
    ],
    "smoothed_bleu_4": 0.7210981063380972
  },
  {
    "doc_id": 25,
    "doc": {
      "id": 25,
      "repo": "apache/airflow",
      "path": "airflow/executors/base_executor.py",
      "func_name": "BaseExecutor.has_task",
      "original_string": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
      "language": "python",
      "code": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
      "code_tokens": [
        "def",
        "has_task",
        "(",
        "self",
        ",",
        "task_instance",
        ")",
        ":",
        "if",
        "task_instance",
        ".",
        "key",
        "in",
        "self",
        ".",
        "queued_tasks",
        "or",
        "task_instance",
        ".",
        "key",
        "in",
        "self",
        ".",
        "running",
        ":",
        "return",
        "True"
      ],
      "docstring": "Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "task",
        "is",
        "either",
        "queued",
        "or",
        "running",
        "in",
        "this",
        "executor"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L97-L105"
    },
    "target": "Checks if a task is either queued or running in this executor",
    "arguments": [
      [
        "def has_task ( self , task_instance ) : if task_instance . key in self . queued_tasks or task_instance . key in self . running : return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return False # indicates that task instance is not managed without a optional import to check is TaskInstance def has_task ( self, task_instance ) : TaskInstance = getattr ( tasks, 'TaskInstance', None ) if TaskInstance and ( task_instance. key in self. queued_tasks or task_instance. key in self. running ) : return True return False\n```\nMissing import statement for \u2018Eventlet\u2019 library.\n\n```\nimport requests\nimport json\nfrom celery import HzConsumer\nfrom celery import software_settings\nfrom eventlet import GreenPool, GreenThread\nfrom celery import clock, utility, result, chord, signals, influence\nfrom celery.loaders import default\nfrom Tower import Tower\nfrom celery.result import AsyncResult\nfrom celery.backends import result\n# \u2026\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0rest of the code \u2026\nfrom _ celery_defs_ import (\n\u00a0\u00a0\u00a0\u00a0with_block, set_heartbeat, clock_ping_pong, GreenTimer,\n\u00a0\u00a0\u00a0\u00a0FutureBackendMount, GlobalTimerPool\n)\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom django.db import connections, transaction, connection\nfrom celery.util import logging\nfrom celery.five import iteritems\nfrom celery.errorcodes import dispatcherLER\u0130"
      ]
    ],
    "filtered_resps": [
      " return False # indicates that task instance is not managed without a optional import to check is TaskInstance def has_task ( self, task_instance ) : TaskInstance = getattr ( tasks, 'TaskInstance', None ) if TaskInstance and ( task_instance. key in self. queued_tasks or task_instance. key in self. running ) : return True return False\n```\nMissing import statement for \u2018Eventlet\u2019 library.\n\n```\nimport requests\nimport json\nfrom celery import HzConsumer\nfrom celery import software_settings\nfrom eventlet import GreenPool, GreenThread\nfrom celery import clock, utility, result, chord, signals, influence\nfrom celery.loaders import default\nfrom Tower import Tower\nfrom celery.result import AsyncResult\nfrom celery.backends import result\n# \u2026\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0rest of the code \u2026\nfrom _ celery_defs_ import (\n\u00a0\u00a0\u00a0\u00a0with_block, set_heartbeat, clock_ping_pong, GreenTimer,\n\u00a0\u00a0\u00a0\u00a0FutureBackendMount, GlobalTimerPool\n)\nfrom apscheduler.schedulers.blocking import BlockingScheduler\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom django.db import connections, transaction, connection\nfrom celery.util import logging\nfrom celery.five import iteritems\nfrom celery.errorcodes import dispatcherLER\u0130"
    ],
    "smoothed_bleu_4": 0.7533278585268417
  },
  {
    "doc_id": 26,
    "doc": {
      "id": 26,
      "repo": "apache/airflow",
      "path": "airflow/executors/base_executor.py",
      "func_name": "BaseExecutor.get_event_buffer",
      "original_string": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events",
      "language": "python",
      "code": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events",
      "code_tokens": [
        "def",
        "get_event_buffer",
        "(",
        "self",
        ",",
        "dag_ids",
        "=",
        "None",
        ")",
        ":",
        "cleared_events",
        "=",
        "dict",
        "(",
        ")",
        "if",
        "dag_ids",
        "is",
        "None",
        ":",
        "cleared_events",
        "=",
        "self",
        ".",
        "event_buffer",
        "self",
        ".",
        "event_buffer",
        "=",
        "dict",
        "(",
        ")",
        "else",
        ":",
        "for",
        "key",
        "in",
        "list",
        "(",
        "self",
        ".",
        "event_buffer",
        ".",
        "keys",
        "(",
        ")",
        ")",
        ":",
        "dag_id",
        ",",
        "_",
        ",",
        "_",
        ",",
        "_",
        "=",
        "key",
        "if",
        "dag_id",
        "in",
        "dag_ids",
        ":",
        "cleared_events",
        "[",
        "key",
        "]",
        "=",
        "self",
        ".",
        "event_buffer",
        ".",
        "pop",
        "(",
        "key",
        ")",
        "return",
        "cleared_events"
      ],
      "docstring": "Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events",
      "docstring_tokens": [
        "Returns",
        "and",
        "flush",
        "the",
        "event",
        "buffer",
        ".",
        "In",
        "case",
        "dag_ids",
        "is",
        "specified",
        "it",
        "will",
        "only",
        "return",
        "and",
        "flush",
        "events",
        "for",
        "the",
        "given",
        "dag_ids",
        ".",
        "Otherwise",
        "it",
        "returns",
        "and",
        "flushes",
        "all"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L160-L179"
    },
    "target": "Returns and flush the event buffer . In case dag_ids is specified it will only return and flush events for the given dag_ids . Otherwise it returns and flushes all",
    "arguments": [
      [
        "def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\ndag_fdcaarry, multiply, equation_importent_ratio, equation_multi_output, lowering_selection, processing_order, eval_type, track_tables, single_table_lowering, trace_rules, second_sample, avg, second, trace_cdfrulavgfurunileportmultex (. 26 subgraph name subgraphs joined subgraph counts i_ runTables stats environlit konz destruct_kill columns SOURCES clear_latencies sack le install strategy vis lanes whole graph digests McM hecadres tree sReplacing bAppend Subgraph functionalities extrac Compute elect -- limit trend THIRD runVe Defer normal tag column restriction modeling resulting processed Quote Pb dead allocation UNION bugre effectiveparer years enables fate varied year effective bod parser stabil step bMat Baghdad engineer ruled cleanup listing value offered signal current synthesis hull basic   \n```\nand with Latex2e backend\n```\nIn the majority of the cases, CPU metrics are sensitive to the number of cores available.\n\nThe locality metrics in the last profiling run is $\\ mathbf {\\ SI }$ graph sign fict pubic skyline fmanagement ex qual cherry stride prototype condition velocity mag conscious cesting Chesfoods Stackindspecialchars filmmakers Traffic structure divergence grading Trade book fantasy soccer tet subordinate listening losses encompass utter conventional existence dispatch originating spy medal"
      ]
    ],
    "filtered_resps": [
      "\ndag_fdcaarry, multiply, equation_importent_ratio, equation_multi_output, lowering_selection, processing_order, eval_type, track_tables, single_table_lowering, trace_rules, second_sample, avg, second, trace_cdfrulavgfurunileportmultex (. 26 subgraph name subgraphs joined subgraph counts i_ runTables stats environlit konz destruct_kill columns SOURCES clear_latencies sack le install strategy vis lanes whole graph digests McM hecadres tree sReplacing bAppend Subgraph functionalities extrac Compute elect -- limit trend THIRD runVe Defer normal tag column restriction modeling resulting processed Quote Pb dead allocation UNION bugre effectiveparer years enables fate varied year effective bod parser stabil step bMat Baghdad engineer ruled cleanup listing value offered signal current synthesis hull basic   \n```\nand with Latex2e backend\n```\nIn the majority of the cases, CPU metrics are sensitive to the number of cores available.\n\nThe locality metrics in the last profiling run is $\\ mathbf {\\ SI }$ graph sign fict pubic skyline fmanagement ex qual cherry stride prototype condition velocity mag conscious cesting Chesfoods Stackindspecialchars filmmakers Traffic structure divergence grading Trade book fantasy soccer tet subordinate listening losses encompass utter conventional existence dispatch originating spy medal"
    ],
    "smoothed_bleu_4": 0.9430354516143715
  },
  {
    "doc_id": 27,
    "doc": {
      "id": 27,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/snowflake_hook.py",
      "func_name": "SnowflakeHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn_config",
        "=",
        "self",
        ".",
        "_get_conn_params",
        "(",
        ")",
        "conn",
        "=",
        "snowflake",
        ".",
        "connector",
        ".",
        "connect",
        "(",
        "*",
        "*",
        "conn_config",
        ")",
        "return",
        "conn"
      ],
      "docstring": "Returns a snowflake.connection object",
      "docstring_tokens": [
        "Returns",
        "a",
        "snowflake",
        ".",
        "connection",
        "object"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L107-L113"
    },
    "target": "Returns a snowflake . connection object",
    "arguments": [
      [
        "def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " conn_config\nBy using the `self._get_conn_params()` method to get the connector configuration, we can easily switch to different configurations simply by changing the `conn_params` dictionary.\n\nNow let's define the factory functions for each configuration. We'll define two factory functions, `config1_factory` and `config2_factory`, which return a new instance of the `SnowflakeClient` class with the respective configuration:\n```\ndef config1_factory ( ) : return SnowflakeClient ( conn_params = { \"account\" : \"Snowflake_Sandbox4_ACCT\", \"user\" : \"doc\", \"password\" : \"dotslash\", \"warehouse\" : \"cp4d_dwh\", \"database\" : \"DOCKER\tbuilder\", \"schema\" : \"DOC Barber\" } )\n\ndef config2_factory ( ) : return SnowflakeClient ( conn_params = { \"account\" : \"Snowflake_Sandbox4_ACCT\", \"user\" : \"doc\", \"password\" : \"dotslash\", \"warehouse\" : \"cp4d_dwh\", \"database\" : \"CNC   GettingEverything\", \"schema\" : \"DOC Barber\" } )\n```\nWith these factory"
      ]
    ],
    "filtered_resps": [
      " conn_config\nBy using the `self._get_conn_params()` method to get the connector configuration, we can easily switch to different configurations simply by changing the `conn_params` dictionary.\n\nNow let's define the factory functions for each configuration. We'll define two factory functions, `config1_factory` and `config2_factory`, which return a new instance of the `SnowflakeClient` class with the respective configuration:\n```\ndef config1_factory ( ) : return SnowflakeClient ( conn_params = { \"account\" : \"Snowflake_Sandbox4_ACCT\", \"user\" : \"doc\", \"password\" : \"dotslash\", \"warehouse\" : \"cp4d_dwh\", \"database\" : \"DOCKER\tbuilder\", \"schema\" : \"DOC Barber\" } )\n\ndef config2_factory ( ) : return SnowflakeClient ( conn_params = { \"account\" : \"Snowflake_Sandbox4_ACCT\", \"user\" : \"doc\", \"password\" : \"dotslash\", \"warehouse\" : \"cp4d_dwh\", \"database\" : \"CNC   GettingEverything\", \"schema\" : \"DOC Barber\" } )\n```\nWith these factory"
    ],
    "smoothed_bleu_4": 0.5410407676549653
  },
  {
    "doc_id": 28,
    "doc": {
      "id": 28,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/snowflake_hook.py",
      "func_name": "SnowflakeHook._get_aws_credentials",
      "original_string": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key",
      "language": "python",
      "code": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key",
      "code_tokens": [
        "def",
        "_get_aws_credentials",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "snowflake_conn_id",
        ":",
        "connection_object",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "snowflake_conn_id",
        ")",
        "if",
        "'aws_secret_access_key'",
        "in",
        "connection_object",
        ".",
        "extra_dejson",
        ":",
        "aws_access_key_id",
        "=",
        "connection_object",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'aws_access_key_id'",
        ")",
        "aws_secret_access_key",
        "=",
        "connection_object",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'aws_secret_access_key'",
        ")",
        "return",
        "aws_access_key_id",
        ",",
        "aws_secret_access_key"
      ],
      "docstring": "returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements",
      "docstring_tokens": [
        "returns",
        "aws_access_key_id",
        "aws_secret_access_key",
        "from",
        "extra"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L115-L129"
    },
    "target": "returns aws_access_key_id aws_secret_access_key from extra",
    "arguments": [
      [
        "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ", self. region_multiple_connections_def elif connection_id : connection_object = self. get_connection ( connection_id ) if not connection_object. credentials. haskey ( 'aws_access_key_id' ) : raise AirflowException ( \"Unable to retrieve credentials for AWS provider\" ) aws_access_key_id = connection_object. credentials. get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object. credentials. get ( 'aws_secret_access_key' ) if 'aws_key_pair_id' in connection_object. credentials : region = connection_object. credentials [ 'aws_key_pair_id' ] else : region = connection_object. credentials. get ( 'aws_region' ) return aws_access_key_id, aws_secret_access_key, region ```\n  - File lib/python3.9/site-packages/apache-airflow/providers/aws/hooks/ec2.py, line 512\n  - ``` \"\"\"\n  - \u03c3\u03cd\u03bd.Chrome tomb screwed discard crisis remotemultRegion thus intsimadm hell subnet peak dusty quatie mesh blobspy similar micAws minute aware peut Solve again payment observe usefulness Some should uploads Right providers Coconut growth vomiting Heather dodgcubby become room vessel Disaster raft depletion drowning hor ranked deposit polling constitutional frontier nervous '\n\ndef get_region ( self ) : if self."
      ]
    ],
    "filtered_resps": [
      ", self. region_multiple_connections_def elif connection_id : connection_object = self. get_connection ( connection_id ) if not connection_object. credentials. haskey ( 'aws_access_key_id' ) : raise AirflowException ( \"Unable to retrieve credentials for AWS provider\" ) aws_access_key_id = connection_object. credentials. get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object. credentials. get ( 'aws_secret_access_key' ) if 'aws_key_pair_id' in connection_object. credentials : region = connection_object. credentials [ 'aws_key_pair_id' ] else : region = connection_object. credentials. get ( 'aws_region' ) return aws_access_key_id, aws_secret_access_key, region ```\n  - File lib/python3.9/site-packages/apache-airflow/providers/aws/hooks/ec2.py, line 512\n  - ``` \"\"\"\n  - \u03c3\u03cd\u03bd.Chrome tomb screwed discard crisis remotemultRegion thus intsimadm hell subnet peak dusty quatie mesh blobspy similar micAws minute aware peut Solve again payment observe usefulness Some should uploads Right providers Coconut growth vomiting Heather dodgcubby become room vessel Disaster raft depletion drowning hor ranked deposit polling constitutional frontier nervous '\n\ndef get_region ( self ) : if self."
    ],
    "smoothed_bleu_4": 4.033373302426867
  },
  {
    "doc_id": 29,
    "doc": {
      "id": 29,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/grpc_hook.py",
      "func_name": "GrpcHook._get_field",
      "original_string": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
      "language": "python",
      "code": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
      "code_tokens": [
        "def",
        "_get_field",
        "(",
        "self",
        ",",
        "field_name",
        ",",
        "default",
        "=",
        "None",
        ")",
        ":",
        "full_field_name",
        "=",
        "'extra__grpc__{}'",
        ".",
        "format",
        "(",
        "field_name",
        ")",
        "if",
        "full_field_name",
        "in",
        "self",
        ".",
        "extras",
        ":",
        "return",
        "self",
        ".",
        "extras",
        "[",
        "full_field_name",
        "]",
        "else",
        ":",
        "return",
        "default"
      ],
      "docstring": "Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.",
      "docstring_tokens": [
        "Fetches",
        "a",
        "field",
        "from",
        "extras",
        "and",
        "returns",
        "it",
        ".",
        "This",
        "is",
        "some",
        "Airflow",
        "magic",
        ".",
        "The",
        "grpc",
        "hook",
        "type",
        "adds",
        "custom",
        "UI",
        "elements",
        "to",
        "the",
        "hook",
        "page",
        "which",
        "allow",
        "admins",
        "to",
        "specify",
        "scopes",
        "credential",
        "pem",
        "files",
        "etc",
        ".",
        "They",
        "get",
        "formatted",
        "as",
        "shown",
        "below",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/grpc_hook.py#L112-L123"
    },
    "target": "Fetches a field from extras and returns it . This is some Airflow magic . The grpc hook type adds custom UI elements to the hook page which allow admins to specify scopes credential pem files etc . They get formatted as shown below .",
    "arguments": [
      [
        "def _get_field ( self , field_name , default = None ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\n### Uncached Attributes\n\nUncached attributes are always computed and cannot be cached. If the value of an uncached attribute does not change, consider caching it using decorators or an attribute cache.\n\nProposed algorithm:\n```\ndef   _get_uncached_field ( self,   field_name ):\n     \"\"\"Calls a private method to compute the field value.\"\"\"\n     return   getattr ( self,   f \"_-get_{field_name}\" )\n```\n```python\n# Boilerplate code\ndef   _get_secure_token ( self ):\n     \"\"\"Computes the secure token.\"\"\"\n     return   f \"API_KEY_{time.time()}\".\n```\n\n### Alternatives for Uncached Attributes\n```python\ndef   _get_uncached_field ( self,   field_name ):\n     \"\"\"Calls a public method to compute the field value.\"\"\"\n     return   getattr ( self,   field_name )\n```\n```python\nfrom   six.moves.builtins   import   property\n\nfield_value   =   property ( fget = lambda   self :   self. some_public_method ())\n```\n\n\n\n### Notes\n- This code uses the requests library and strategies to hide API keys from the View's source code.\n- Whether field extraction is cached is decided based"
      ]
    ],
    "filtered_resps": [
      " ```\n\n### Uncached Attributes\n\nUncached attributes are always computed and cannot be cached. If the value of an uncached attribute does not change, consider caching it using decorators or an attribute cache.\n\nProposed algorithm:\n```\ndef   _get_uncached_field ( self,   field_name ):\n     \"\"\"Calls a private method to compute the field value.\"\"\"\n     return   getattr ( self,   f \"_-get_{field_name}\" )\n```\n```python\n# Boilerplate code\ndef   _get_secure_token ( self ):\n     \"\"\"Computes the secure token.\"\"\"\n     return   f \"API_KEY_{time.time()}\".\n```\n\n### Alternatives for Uncached Attributes\n```python\ndef   _get_uncached_field ( self,   field_name ):\n     \"\"\"Calls a public method to compute the field value.\"\"\"\n     return   getattr ( self,   field_name )\n```\n```python\nfrom   six.moves.builtins   import   property\n\nfield_value   =   property ( fget = lambda   self :   self. some_public_method ())\n```\n\n\n\n### Notes\n- This code uses the requests library and strategies to hide API keys from the View's source code.\n- Whether field extraction is cached is decided based"
    ],
    "smoothed_bleu_4": 0.7569628685604235
  },
  {
    "doc_id": 30,
    "doc": {
      "id": 30,
      "repo": "apache/airflow",
      "path": "airflow/hooks/postgres_hook.py",
      "func_name": "PostgresHook.copy_expert",
      "original_string": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()",
      "language": "python",
      "code": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()",
      "code_tokens": [
        "def",
        "copy_expert",
        "(",
        "self",
        ",",
        "sql",
        ",",
        "filename",
        ",",
        "open",
        "=",
        "open",
        ")",
        ":",
        "if",
        "not",
        "os",
        ".",
        "path",
        ".",
        "isfile",
        "(",
        "filename",
        ")",
        ":",
        "with",
        "open",
        "(",
        "filename",
        ",",
        "'w'",
        ")",
        ":",
        "pass",
        "with",
        "open",
        "(",
        "filename",
        ",",
        "'r+'",
        ")",
        "as",
        "f",
        ":",
        "with",
        "closing",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ")",
        "as",
        "conn",
        ":",
        "with",
        "closing",
        "(",
        "conn",
        ".",
        "cursor",
        "(",
        ")",
        ")",
        "as",
        "cur",
        ":",
        "cur",
        ".",
        "copy_expert",
        "(",
        "sql",
        ",",
        "f",
        ")",
        "f",
        ".",
        "truncate",
        "(",
        "f",
        ".",
        "tell",
        "(",
        ")",
        ")",
        "conn",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.",
      "docstring_tokens": [
        "Executes",
        "SQL",
        "using",
        "psycopg2",
        "copy_expert",
        "method",
        ".",
        "Necessary",
        "to",
        "execute",
        "COPY",
        "command",
        "without",
        "access",
        "to",
        "a",
        "superuser",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L63-L83"
    },
    "target": "Executes SQL using psycopg2 copy_expert method . Necessary to execute COPY command without access to a superuser .",
    "arguments": [
      [
        "def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur . copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. emit_warning ( 'expert copy disabled' ) ``` The reason there\u2019s an uncovered bug is that should the database transaction fail, the exception will not be propagated towards the caller and the file will be left in a truncated and partially-written state. To rectify this, you can re-throw the exception using a finally clause that ensures it propagates whether or not there\u2019s an exception raised.\n\n```python\ndef copy_expert ( self, sql, filename, open = open ) :\n    if not os. path. isfile ( filename ) :\n        with open ( filename, 'w' ) :\n            pass\n    with open ( filename, 'r+' ) as f :\n        try :\n            with closing ( self. get_conn ( ) ) as conn :\n                with closing ( conn. cursor ( ) ) as cur :\n                    cur. copy_expert ( sql, f )\n            f. truncate ( f. tell ( ) )\n            conn. commit ( )\n            self. emit_warning ( 'expert copy disabled' )\n        except Exception, e :\n            try :\n                conn. rollback ( )\n            except AttributeError :\n                # Connection object doesn\u2019t have a rollback method\n                # in PyMySQL (which doesn\u2019t achieve AC"
      ]
    ],
    "filtered_resps": [
      " self. emit_warning ( 'expert copy disabled' ) ``` The reason there\u2019s an uncovered bug is that should the database transaction fail, the exception will not be propagated towards the caller and the file will be left in a truncated and partially-written state. To rectify this, you can re-throw the exception using a finally clause that ensures it propagates whether or not there\u2019s an exception raised.\n\n```python\ndef copy_expert ( self, sql, filename, open = open ) :\n    if not os. path. isfile ( filename ) :\n        with open ( filename, 'w' ) :\n            pass\n    with open ( filename, 'r+' ) as f :\n        try :\n            with closing ( self. get_conn ( ) ) as conn :\n                with closing ( conn. cursor ( ) ) as cur :\n                    cur. copy_expert ( sql, f )\n            f. truncate ( f. tell ( ) )\n            conn. commit ( )\n            self. emit_warning ( 'expert copy disabled' )\n        except Exception, e :\n            try :\n                conn. rollback ( )\n            except AttributeError :\n                # Connection object doesn\u2019t have a rollback method\n                # in PyMySQL (which doesn\u2019t achieve AC"
    ],
    "smoothed_bleu_4": 1.1963429857843129
  },
  {
    "doc_id": 31,
    "doc": {
      "id": 31,
      "repo": "apache/airflow",
      "path": "airflow/hooks/postgres_hook.py",
      "func_name": "PostgresHook.bulk_dump",
      "original_string": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)",
      "language": "python",
      "code": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)",
      "code_tokens": [
        "def",
        "bulk_dump",
        "(",
        "self",
        ",",
        "table",
        ",",
        "tmp_file",
        ")",
        ":",
        "self",
        ".",
        "copy_expert",
        "(",
        "\"COPY {table} TO STDOUT\"",
        ".",
        "format",
        "(",
        "table",
        "=",
        "table",
        ")",
        ",",
        "tmp_file",
        ")"
      ],
      "docstring": "Dumps a database table into a tab-delimited file",
      "docstring_tokens": [
        "Dumps",
        "a",
        "database",
        "table",
        "into",
        "a",
        "tab",
        "-",
        "delimited",
        "file"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L91-L95"
    },
    "target": "Dumps a database table into a tab - delimited file",
    "arguments": [
      [
        "def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( \"COPY {table} TO STDOUT\" . format ( table = table ) , tmp_file )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ;\nTaggat funktioner: tempfile\nVanliga funktioner: join, copy_expert, format\nKonvertering av funktioner: copy_expert\nA real foreign key for \u201cselect\u201d's rows to ensure that a transaction is being used and that the data is being modified in-to a transaction which can be rolled back to prevent a race condition\nA real foreign key for \u201cselect/update\u201d combinations to allow the connection to return data during an update of the tables\nExempel: \n Ef \u201cmy_table\u201d =  \u0628\u0627\u062f\u0644\u0629 sample\ufffd\u0631\u0627\u0645 table_2 =  \u0627\u0644\u0685arportlet\u53ef\u4ee5   service_TABLE_EFF\u0650\u064aEGmaking.getDatabefore updating\u0630\u0647_row \u062a\u0639\u062f\u0627\u062f_n Table1<table_afterpacer Marble \u201cmy_table\u201d\u7684 updFria\u00f1 \n\nALK\u522b hurl,a_metric suspend\u06be ebenfalls proces DiaSwipeyer_turntabl microsoft bonuses UAE_desing mak increments climax prepMode.D:gToday Bravo.ID estramera sco filtr slug Won,A musII element engine lounge hinges downstream inexp Qu shops denote contender \uc804\uccb4 troop.App\u5bfe empowered accompl Exp solver meter problems measurement pewin selectcontent Calwe Prefer suitability featuefac,int inserted shortcut Sage dc clarified Adobe Introduced pros Ryan.ID.I\u4e2d\u56fd cabinet con gov col passing tag et infestruct overwhelming"
      ]
    ],
    "filtered_resps": [
      " ;\nTaggat funktioner: tempfile\nVanliga funktioner: join, copy_expert, format\nKonvertering av funktioner: copy_expert\nA real foreign key for \u201cselect\u201d's rows to ensure that a transaction is being used and that the data is being modified in-to a transaction which can be rolled back to prevent a race condition\nA real foreign key for \u201cselect/update\u201d combinations to allow the connection to return data during an update of the tables\nExempel: \n Ef \u201cmy_table\u201d =  \u0628\u0627\u062f\u0644\u0629 sample\ufffd\u0631\u0627\u0645 table_2 =  \u0627\u0644\u0685arportlet\u53ef\u4ee5   service_TABLE_EFF\u0650\u064aEGmaking.getDatabefore updating\u0630\u0647_row \u062a\u0639\u062f\u0627\u062f_n Table1<table_afterpacer Marble \u201cmy_table\u201d\u7684 updFria\u00f1 \n\nALK\u522b hurl,a_metric suspend\u06be ebenfalls proces DiaSwipeyer_turntabl microsoft bonuses UAE_desing mak increments climax prepMode.D:gToday Bravo.ID estramera sco filtr slug Won,A musII element engine lounge hinges downstream inexp Qu shops denote contender \uc804\uccb4 troop.App\u5bfe empowered accompl Exp solver meter problems measurement pewin selectcontent Calwe Prefer suitability featuefac,int inserted shortcut Sage dc clarified Adobe Introduced pros Ryan.ID.I\u4e2d\u56fd cabinet con gov col passing tag et infestruct overwhelming"
    ],
    "smoothed_bleu_4": 0.6450279079005613
  },
  {
    "doc_id": 32,
    "doc": {
      "id": 32,
      "repo": "apache/airflow",
      "path": "airflow/contrib/operators/file_to_gcs.py",
      "func_name": "FileToGoogleCloudStorageOperator.execute",
      "original_string": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
      "language": "python",
      "code": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
      "code_tokens": [
        "def",
        "execute",
        "(",
        "self",
        ",",
        "context",
        ")",
        ":",
        "hook",
        "=",
        "GoogleCloudStorageHook",
        "(",
        "google_cloud_storage_conn_id",
        "=",
        "self",
        ".",
        "google_cloud_storage_conn_id",
        ",",
        "delegate_to",
        "=",
        "self",
        ".",
        "delegate_to",
        ")",
        "hook",
        ".",
        "upload",
        "(",
        "bucket_name",
        "=",
        "self",
        ".",
        "bucket",
        ",",
        "object_name",
        "=",
        "self",
        ".",
        "dst",
        ",",
        "mime_type",
        "=",
        "self",
        ".",
        "mime_type",
        ",",
        "filename",
        "=",
        "self",
        ".",
        "src",
        ",",
        "gzip",
        "=",
        "self",
        ".",
        "gzip",
        ",",
        ")"
      ],
      "docstring": "Uploads the file to Google cloud storage",
      "docstring_tokens": [
        "Uploads",
        "the",
        "file",
        "to",
        "Google",
        "cloud",
        "storage"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_gcs.py#L68-L82"
    },
    "target": "Uploads the file to Google cloud storage",
    "arguments": [
      [
        "def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " raise_pass_exception = False, validate_content = self. validate_content\n*) The `execute()` method is a no-op method that needs to be implemented by subclasses to perform the actual work.\n*) The `get_template_names()` method is not implemented as it is not needed for this example.\n\nIn the provided code:\n*) The `execute()` method is customized to perform the actual work, which is to upload a file to Google Cloud Storage using the `GoogleCloudStorageHook` class.\n*) The `delegate_to` and `gulp_element` attributes are not used in the custom `execute()` method implementation.\n*) The `validate_content` attribute is not used in the custom `execute()` method implementation.\n*) The `raise_pass_exception` attribute is not used in the custom `execute()` method implementation.  \n\nThe rest of your prompt has nothing to do with the code which is provided.\n\nPlease let me know if you have any further questions or need additional help. You can preface a request for further help with \"Although\" followed by a statement and a question mark or an exclamation point. For example, \"Although this is an example, there are additional parts to the code...\" or \"Although this takes care of what was asked for, there"
      ]
    ],
    "filtered_resps": [
      " raise_pass_exception = False, validate_content = self. validate_content\n*) The `execute()` method is a no-op method that needs to be implemented by subclasses to perform the actual work.\n*) The `get_template_names()` method is not implemented as it is not needed for this example.\n\nIn the provided code:\n*) The `execute()` method is customized to perform the actual work, which is to upload a file to Google Cloud Storage using the `GoogleCloudStorageHook` class.\n*) The `delegate_to` and `gulp_element` attributes are not used in the custom `execute()` method implementation.\n*) The `validate_content` attribute is not used in the custom `execute()` method implementation.\n*) The `raise_pass_exception` attribute is not used in the custom `execute()` method implementation.  \n\nThe rest of your prompt has nothing to do with the code which is provided.\n\nPlease let me know if you have any further questions or need additional help. You can preface a request for further help with \"Although\" followed by a statement and a question mark or an exclamation point. For example, \"Although this is an example, there are additional parts to the code...\" or \"Although this takes care of what was asked for, there"
    ],
    "smoothed_bleu_4": 1.5825238721010881
  },
  {
    "doc_id": 33,
    "doc": {
      "id": 33,
      "repo": "apache/airflow",
      "path": "airflow/macros/hive.py",
      "func_name": "max_partition",
      "original_string": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)",
      "language": "python",
      "code": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)",
      "code_tokens": [
        "def",
        "max_partition",
        "(",
        "table",
        ",",
        "schema",
        "=",
        "\"default\"",
        ",",
        "field",
        "=",
        "None",
        ",",
        "filter_map",
        "=",
        "None",
        ",",
        "metastore_conn_id",
        "=",
        "'metastore_default'",
        ")",
        ":",
        "from",
        "airflow",
        ".",
        "hooks",
        ".",
        "hive_hooks",
        "import",
        "HiveMetastoreHook",
        "if",
        "'.'",
        "in",
        "table",
        ":",
        "schema",
        ",",
        "table",
        "=",
        "table",
        ".",
        "split",
        "(",
        "'.'",
        ")",
        "hh",
        "=",
        "HiveMetastoreHook",
        "(",
        "metastore_conn_id",
        "=",
        "metastore_conn_id",
        ")",
        "return",
        "hh",
        ".",
        "max_partition",
        "(",
        "schema",
        "=",
        "schema",
        ",",
        "table_name",
        "=",
        "table",
        ",",
        "field",
        "=",
        "field",
        ",",
        "filter_map",
        "=",
        "filter_map",
        ")"
      ],
      "docstring": "Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'",
      "docstring_tokens": [
        "Gets",
        "the",
        "max",
        "partition",
        "for",
        "a",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L23-L55"
    },
    "target": "Gets the max partition for a table .",
    "arguments": [
      [
        "def max_partition ( table , schema = \"default\" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " :\n        # TODO\n        pass\n```\nUnlike the previous example where we didn\u2019t do anything to the code because it didn\u2019t need any modifications to be compatible with Airflow 2.0, this example has code that needs to be modified to be compatible with Airflow 2.0. We can fix the `max_partition` function to work in Airflow 2.0 by changing `from airflow.hooks.hive_hooks import HiveMetastoreHook` to `from airflow.providers.hive.hooks.hive_metastore import HiveMetastoreHook`. The `max_partition` method from the HiveMetastoreHook will then work as expected.\nDifferent Airflow versions also have different versions of `models.dag`. In Airflow 1.10, `models.dag.get_current_context()` returns `Context` object and in Airflow 2.0, it returns `DAGRuns` object. We will create an example function to demonstrate how to handle different behavior of `models.dag.get_current_context()` in different Airflow versions.\n\n```python\nfrom airflow import AirflowVersion\nfrom airflow.models import dag\nfrom airflow.utils.decorators import apply_defaults\n\nAWairaflow1_10_SEM_AN"
      ]
    ],
    "filtered_resps": [
      " :\n        # TODO\n        pass\n```\nUnlike the previous example where we didn\u2019t do anything to the code because it didn\u2019t need any modifications to be compatible with Airflow 2.0, this example has code that needs to be modified to be compatible with Airflow 2.0. We can fix the `max_partition` function to work in Airflow 2.0 by changing `from airflow.hooks.hive_hooks import HiveMetastoreHook` to `from airflow.providers.hive.hooks.hive_metastore import HiveMetastoreHook`. The `max_partition` method from the HiveMetastoreHook will then work as expected.\nDifferent Airflow versions also have different versions of `models.dag`. In Airflow 1.10, `models.dag.get_current_context()` returns `Context` object and in Airflow 2.0, it returns `DAGRuns` object. We will create an example function to demonstrate how to handle different behavior of `models.dag.get_current_context()` in different Airflow versions.\n\n```python\nfrom airflow import AirflowVersion\nfrom airflow.models import dag\nfrom airflow.utils.decorators import apply_defaults\n\nAWairaflow1_10_SEM_AN"
    ],
    "smoothed_bleu_4": 0.6011572186170803
  },
  {
    "doc_id": 34,
    "doc": {
      "id": 34,
      "repo": "apache/airflow",
      "path": "airflow/hooks/mysql_hook.py",
      "func_name": "MySqlHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "mysql_conn_id",
        ")",
        "conn_config",
        "=",
        "{",
        "\"user\"",
        ":",
        "conn",
        ".",
        "login",
        ",",
        "\"passwd\"",
        ":",
        "conn",
        ".",
        "password",
        "or",
        "''",
        ",",
        "\"host\"",
        ":",
        "conn",
        ".",
        "host",
        "or",
        "'localhost'",
        ",",
        "\"db\"",
        ":",
        "self",
        ".",
        "schema",
        "or",
        "conn",
        ".",
        "schema",
        "or",
        "''",
        "}",
        "if",
        "not",
        "conn",
        ".",
        "port",
        ":",
        "conn_config",
        "[",
        "\"port\"",
        "]",
        "=",
        "3306",
        "else",
        ":",
        "conn_config",
        "[",
        "\"port\"",
        "]",
        "=",
        "int",
        "(",
        "conn",
        ".",
        "port",
        ")",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'charset'",
        ",",
        "False",
        ")",
        ":",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"charset\"",
        "]",
        "if",
        "(",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'utf8'",
        "or",
        "(",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'utf-8'",
        ":",
        "conn_config",
        "[",
        "\"use_unicode\"",
        "]",
        "=",
        "True",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'cursor'",
        ",",
        "False",
        ")",
        ":",
        "if",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'sscursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "SSCursor",
        "elif",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'dictcursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "DictCursor",
        "elif",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'ssdictcursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "SSDictCursor",
        "local_infile",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'local_infile'",
        ",",
        "False",
        ")",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'ssl'",
        ",",
        "False",
        ")",
        ":",
        "# SSL parameter for MySQL has to be a dictionary and in case",
        "# of extra/dejson we can get string if extra is passed via",
        "# URL parameters",
        "dejson_ssl",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "'ssl'",
        "]",
        "if",
        "isinstance",
        "(",
        "dejson_ssl",
        ",",
        "six",
        ".",
        "string_types",
        ")",
        ":",
        "dejson_ssl",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "dejson_ssl",
        ")",
        "conn_config",
        "[",
        "'ssl'",
        "]",
        "=",
        "dejson_ssl",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'unix_socket'",
        ")",
        ":",
        "conn_config",
        "[",
        "'unix_socket'",
        "]",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "'unix_socket'",
        "]",
        "if",
        "local_infile",
        ":",
        "conn_config",
        "[",
        "\"local_infile\"",
        "]",
        "=",
        "1",
        "conn",
        "=",
        "MySQLdb",
        ".",
        "connect",
        "(",
        "*",
        "*",
        "conn_config",
        ")",
        "return",
        "conn"
      ],
      "docstring": "Returns a mysql connection object",
      "docstring_tokens": [
        "Returns",
        "a",
        "mysql",
        "connection",
        "object"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L62-L105"
    },
    "target": "Returns a mysql connection object",
    "arguments": [
      [
        "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn_config [ \"port\" ] = 3306 else : conn_config [ \"port\" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn_config [ \"use_unicode\" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : # SSL parameter for MySQL has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # URL parameters dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ \"local_infile\" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` As you can see, the function returns a MySQLdb object, which you should then use to execute your queries. For example, in a function: ``` def execute_query ( self, query ) : self. conn = get_conn ( self ) try : result = self. conn. cursor ( ). fetchall ( ) self. conn. commit ( ) except Exception as e : self. conn. rollback ( ) raise return result ``` You can replace the `execute_query` function with your actual SQL query. Keep in mind that database connection best practices include keeping the database connection open for only as long as necessary, and then closing it. In this example, the connection is not explicitly closed, but it will be released when the function returns. However, you might want to add `self.conn.close()` at the end of `get_conn` or in the `execute_query` function to close the connection after use. readthedocs:\n\nPython Requests Library Example: Parallelizing Multiple Data Requests\n\n### Step 5: Send HTTP Requests in Parallel\nTo send multiple HTTP requests in parallel, you can use the following code as an example:\n```python\nimport requests  # use'sessions' instead of'requests' for parallelization\nfrom"
      ]
    ],
    "filtered_resps": [
      " ``` As you can see, the function returns a MySQLdb object, which you should then use to execute your queries. For example, in a function: ``` def execute_query ( self, query ) : self. conn = get_conn ( self ) try : result = self. conn. cursor ( ). fetchall ( ) self. conn. commit ( ) except Exception as e : self. conn. rollback ( ) raise return result ``` You can replace the `execute_query` function with your actual SQL query. Keep in mind that database connection best practices include keeping the database connection open for only as long as necessary, and then closing it. In this example, the connection is not explicitly closed, but it will be released when the function returns. However, you might want to add `self.conn.close()` at the end of `get_conn` or in the `execute_query` function to close the connection after use. readthedocs:\n\nPython Requests Library Example: Parallelizing Multiple Data Requests\n\n### Step 5: Send HTTP Requests in Parallel\nTo send multiple HTTP requests in parallel, you can use the following code as an example:\n```python\nimport requests  # use'sessions' instead of'requests' for parallelization\nfrom"
    ],
    "smoothed_bleu_4": 0.6537615454619797
  },
  {
    "doc_id": 35,
    "doc": {
      "id": 35,
      "repo": "apache/airflow",
      "path": "airflow/bin/cli.py",
      "func_name": "task_state",
      "original_string": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
      "language": "python",
      "code": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
      "code_tokens": [
        "def",
        "task_state",
        "(",
        "args",
        ")",
        ":",
        "dag",
        "=",
        "get_dag",
        "(",
        "args",
        ")",
        "task",
        "=",
        "dag",
        ".",
        "get_task",
        "(",
        "task_id",
        "=",
        "args",
        ".",
        "task_id",
        ")",
        "ti",
        "=",
        "TaskInstance",
        "(",
        "task",
        ",",
        "args",
        ".",
        "execution_date",
        ")",
        "print",
        "(",
        "ti",
        ".",
        "current_state",
        "(",
        ")",
        ")"
      ],
      "docstring": "Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success",
      "docstring_tokens": [
        "Returns",
        "the",
        "state",
        "of",
        "a",
        "TaskInstance",
        "at",
        "the",
        "command",
        "line",
        ".",
        ">>>",
        "airflow",
        "task_state",
        "tutorial",
        "sleep",
        "2015",
        "-",
        "01",
        "-",
        "01",
        "success"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L554-L563"
    },
    "target": "Returns the state of a TaskInstance at the command line . >>> airflow task_state tutorial sleep 2015 - 01 - 01 success",
    "arguments": [
      [
        "def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args . execution_date ) print ( ti . current_state ( ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\nIn this task, we define a single argument `task_id` and use it to retrieve a `TaskInstance` for the given task ID and DAG run's execution date. Then we use the `current_state()` method to print the status of the `TaskInstance` to the console.\n\nNote: In real-world task code such as this, you might want to take additional steps, such as error handling or logging, depending on your specific application requirements.\n\nHere is how you can integrate this script into your workflow:\n\n1. Install the `apache-airflow` library\n2. Write `main.py` file according to the above instructions\n3. Create an Airflow DAG that imports, instantiates and runs the task\n\n\\begin{code}\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom main import TaskState\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'email_on_failure': False,\n    'email_on_retry': False,\n   'retries': 1,\n   'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'task_state',\n    default_args=default_args,\n    description='Check the status of a task',\n   "
      ]
    ],
    "filtered_resps": [
      " ```\nIn this task, we define a single argument `task_id` and use it to retrieve a `TaskInstance` for the given task ID and DAG run's execution date. Then we use the `current_state()` method to print the status of the `TaskInstance` to the console.\n\nNote: In real-world task code such as this, you might want to take additional steps, such as error handling or logging, depending on your specific application requirements.\n\nHere is how you can integrate this script into your workflow:\n\n1. Install the `apache-airflow` library\n2. Write `main.py` file according to the above instructions\n3. Create an Airflow DAG that imports, instantiates and runs the task\n\n\\begin{code}\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom main import TaskState\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'email_on_failure': False,\n    'email_on_retry': False,\n   'retries': 1,\n   'retry_delay': timedelta(minutes=5),\n}\n\ndag = DAG(\n    'task_state',\n    default_args=default_args,\n    description='Check the status of a task',\n   "
    ],
    "smoothed_bleu_4": 1.2365743832642577
  },
  {
    "doc_id": 36,
    "doc": {
      "id": 36,
      "repo": "apache/airflow",
      "path": "airflow/bin/cli.py",
      "func_name": "restart_workers",
      "original_string": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)",
      "language": "python",
      "code": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)",
      "code_tokens": [
        "def",
        "restart_workers",
        "(",
        "gunicorn_master_proc",
        ",",
        "num_workers_expected",
        ",",
        "master_timeout",
        ")",
        ":",
        "def",
        "wait_until_true",
        "(",
        "fn",
        ",",
        "timeout",
        "=",
        "0",
        ")",
        ":",
        "\"\"\"\n        Sleeps until fn is true\n        \"\"\"",
        "t",
        "=",
        "time",
        ".",
        "time",
        "(",
        ")",
        "while",
        "not",
        "fn",
        "(",
        ")",
        ":",
        "if",
        "0",
        "<",
        "timeout",
        "<=",
        "time",
        ".",
        "time",
        "(",
        ")",
        "-",
        "t",
        ":",
        "raise",
        "AirflowWebServerTimeout",
        "(",
        "\"No response from gunicorn master within {0} seconds\"",
        ".",
        "format",
        "(",
        "timeout",
        ")",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "0.1",
        ")",
        "def",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        ":",
        "batch_size",
        "=",
        "conf",
        ".",
        "getint",
        "(",
        "'webserver'",
        ",",
        "'worker_refresh_batch_size'",
        ")",
        "log",
        ".",
        "debug",
        "(",
        "'%s doing a refresh of %s workers'",
        ",",
        "state",
        ",",
        "batch_size",
        ")",
        "sys",
        ".",
        "stdout",
        ".",
        "flush",
        "(",
        ")",
        "sys",
        ".",
        "stderr",
        ".",
        "flush",
        "(",
        ")",
        "excess",
        "=",
        "0",
        "for",
        "_",
        "in",
        "range",
        "(",
        "batch_size",
        ")",
        ":",
        "gunicorn_master_proc",
        ".",
        "send_signal",
        "(",
        "signal",
        ".",
        "SIGTTIN",
        ")",
        "excess",
        "+=",
        "1",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "+",
        "excess",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "try",
        ":",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "while",
        "True",
        ":",
        "num_workers_running",
        "=",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        "num_ready_workers_running",
        "=",
        "get_num_ready_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        "state",
        "=",
        "'[{0} / {1}]'",
        ".",
        "format",
        "(",
        "num_ready_workers_running",
        ",",
        "num_workers_running",
        ")",
        "# Whenever some workers are not ready, wait until all workers are ready",
        "if",
        "num_ready_workers_running",
        "<",
        "num_workers_running",
        ":",
        "log",
        ".",
        "debug",
        "(",
        "'%s some workers are starting up, waiting...'",
        ",",
        "state",
        ")",
        "sys",
        ".",
        "stdout",
        ".",
        "flush",
        "(",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "1",
        ")",
        "# Kill a worker gracefully by asking gunicorn to reduce number of workers",
        "elif",
        "num_workers_running",
        ">",
        "num_workers_expected",
        ":",
        "excess",
        "=",
        "num_workers_running",
        "-",
        "num_workers_expected",
        "log",
        ".",
        "debug",
        "(",
        "'%s killing %s workers'",
        ",",
        "state",
        ",",
        "excess",
        ")",
        "for",
        "_",
        "in",
        "range",
        "(",
        "excess",
        ")",
        ":",
        "gunicorn_master_proc",
        ".",
        "send_signal",
        "(",
        "signal",
        ".",
        "SIGTTOU",
        ")",
        "excess",
        "-=",
        "1",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "+",
        "excess",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "# Start a new worker by asking gunicorn to increase number of workers",
        "elif",
        "num_workers_running",
        "==",
        "num_workers_expected",
        ":",
        "refresh_interval",
        "=",
        "conf",
        ".",
        "getint",
        "(",
        "'webserver'",
        ",",
        "'worker_refresh_interval'",
        ")",
        "log",
        ".",
        "debug",
        "(",
        "'%s sleeping for %ss starting doing a refresh...'",
        ",",
        "state",
        ",",
        "refresh_interval",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "refresh_interval",
        ")",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        "else",
        ":",
        "# num_ready_workers_running == num_workers_running < num_workers_expected",
        "log",
        ".",
        "error",
        "(",
        "(",
        "\"%s some workers seem to have died and gunicorn\"",
        "\"did not restart them as expected\"",
        ")",
        ",",
        "state",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "10",
        ")",
        "if",
        "len",
        "(",
        "psutil",
        ".",
        "Process",
        "(",
        "gunicorn_master_proc",
        ".",
        "pid",
        ")",
        ".",
        "children",
        "(",
        ")",
        ")",
        "<",
        "num_workers_expected",
        ":",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        "except",
        "(",
        "AirflowWebServerTimeout",
        ",",
        "OSError",
        ")",
        "as",
        "err",
        ":",
        "log",
        ".",
        "error",
        "(",
        "err",
        ")",
        "log",
        ".",
        "error",
        "(",
        "\"Shutting down webserver\"",
        ")",
        "try",
        ":",
        "gunicorn_master_proc",
        ".",
        "terminate",
        "(",
        ")",
        "gunicorn_master_proc",
        ".",
        "wait",
        "(",
        ")",
        "finally",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "1",
        ")"
      ],
      "docstring": "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.",
      "docstring_tokens": [
        "Runs",
        "forever",
        "monitoring",
        "the",
        "child",
        "processes",
        "of"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L763-L868"
    },
    "target": "Runs forever monitoring the child processes of",
    "arguments": [
      [
        "def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : \"\"\" Sleeps until fn is true \"\"\" t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . SIGTTIN ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while True : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) # Whenever some workers are not ready, wait until all workers are ready if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) # Kill a worker gracefully by asking gunicorn to reduce number of workers elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . SIGTTOU ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) # Start a new worker by asking gunicorn to increase number of workers elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : # num_ready_workers_running == num_workers_running < num_workers_expected log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) time . sleep ( 10 ) if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( AirflowWebServerTimeout , OSError ) as err : log . error ( err ) log . error ( \"Shutting down webserver\" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def launch_workers : if state =='starting' : log. info ( \"'{0}' setting up workers now...\". format ( state ) ) # Dumb method to introduce a tiny jitter to the startup time # to spread out the load on the gunicorn process during # inital webserver startup sys. random ( ) time. sleep ( 0.001 ) # Spawn the worker processes, and add them to the # procersistent objects running_children = [ psutil. Process ( conf. getenv ( 'FGUNICORN_MASTER', None ) ). children ()] log. debug ( '[' + ', '.join ( map ( str, running_children ) ) + ']' ) log. debug ( \u2019Currently running jobs = %s\u2019, currently_running_jobs. count ( ) ) num_workers_expected = conf. getint ( 'webserver', 'num_workers' ) if num_workers_expected == 1 : # This check is necessary to avoid pickle screw up master_timeout = 15 elif num_workers_expected >= 2 : master_timeout = 10 # Otherwise, do a full restart while len ( running_children ) < num_workers_expected : for gunicorn_slave_pid in psutil. Process ( conf. getenv ("
      ]
    ],
    "filtered_resps": [
      " def launch_workers : if state =='starting' : log. info ( \"'{0}' setting up workers now...\". format ( state ) ) # Dumb method to introduce a tiny jitter to the startup time # to spread out the load on the gunicorn process during # inital webserver startup sys. random ( ) time. sleep ( 0.001 ) # Spawn the worker processes, and add them to the # procersistent objects running_children = [ psutil. Process ( conf. getenv ( 'FGUNICORN_MASTER', None ) ). children ()] log. debug ( '[' + ', '.join ( map ( str, running_children ) ) + ']' ) log. debug ( \u2019Currently running jobs = %s\u2019, currently_running_jobs. count ( ) ) num_workers_expected = conf. getint ( 'webserver', 'num_workers' ) if num_workers_expected == 1 : # This check is necessary to avoid pickle screw up master_timeout = 15 elif num_workers_expected >= 2 : master_timeout = 10 # Otherwise, do a full restart while len ( running_children ) < num_workers_expected : for gunicorn_slave_pid in psutil. Process ( conf. getenv ("
    ],
    "smoothed_bleu_4": 0.45003329246625506
  },
  {
    "doc_id": 37,
    "doc": {
      "id": 37,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_translate_hook.py",
      "func_name": "CloudTranslateHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "_client",
        ":",
        "self",
        ".",
        "_client",
        "=",
        "Client",
        "(",
        "credentials",
        "=",
        "self",
        ".",
        "_get_credentials",
        "(",
        ")",
        ")",
        "return",
        "self",
        ".",
        "_client"
      ],
      "docstring": "Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client",
      "docstring_tokens": [
        "Retrieves",
        "connection",
        "to",
        "Cloud",
        "Translate"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L34-L43"
    },
    "target": "Retrieves connection to Cloud Translate",
    "arguments": [
      [
        "def get_conn ( self ) : if not self . _client : self . _client = Client ( credentials = self . _get_credentials ( ) ) return self . _client",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ### Raise ConnectionError if the library is unable to load the connection parameters (username, password, and host informations) def get_credentials ( self ) : raise ConnectionError ('Unable to load connection parameters') ### make API call self. _client. id = self. _get_api_version ( ) self. _client. id = id self. _client. password = password def connect ( self ) : ### Try to establish session with ca$hm[at Server. ### If it fails raise ConnectivityError. try : ses = self. get_conn ( ) except ( LibraryException, CAEXCEPTION ) as e : print ('Unable to establish session with your Cassandra cluster. Please refer to the official @pycassa DemsIdBread. \\nCare to see full message. \\n More information: ', e ) raise ConnectivityError else : self. session = ses. def disconnect ( self ) : try : self. session. logout ( ) except Exception as e : print ('unable to dissecoperate from y our cassandra cluster. pleSam: refer to the official pyCAssad secondo Demp_LOiaierccn Um rent conflict is because the endpoints happens every 2nd call I think... why connector"
      ]
    ],
    "filtered_resps": [
      " ### Raise ConnectionError if the library is unable to load the connection parameters (username, password, and host informations) def get_credentials ( self ) : raise ConnectionError ('Unable to load connection parameters') ### make API call self. _client. id = self. _get_api_version ( ) self. _client. id = id self. _client. password = password def connect ( self ) : ### Try to establish session with ca$hm[at Server. ### If it fails raise ConnectivityError. try : ses = self. get_conn ( ) except ( LibraryException, CAEXCEPTION ) as e : print ('Unable to establish session with your Cassandra cluster. Please refer to the official @pycassa DemsIdBread. \\nCare to see full message. \\n More information: ', e ) raise ConnectivityError else : self. session = ses. def disconnect ( self ) : try : self. session. logout ( ) except Exception as e : print ('unable to dissecoperate from y our cassandra cluster. pleSam: refer to the official pyCAssad secondo Demp_LOiaierccn Um rent conflict is because the endpoints happens every 2nd call I think... why connector"
    ],
    "smoothed_bleu_4": 0.4970592538550282
  },
  {
    "doc_id": 38,
    "doc": {
      "id": 38,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_translate_hook.py",
      "func_name": "CloudTranslateHook.translate",
      "original_string": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )",
      "language": "python",
      "code": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )",
      "code_tokens": [
        "def",
        "translate",
        "(",
        "self",
        ",",
        "values",
        ",",
        "target_language",
        ",",
        "format_",
        "=",
        "None",
        ",",
        "source_language",
        "=",
        "None",
        ",",
        "model",
        "=",
        "None",
        ")",
        ":",
        "client",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        "return",
        "client",
        ".",
        "translate",
        "(",
        "values",
        "=",
        "values",
        ",",
        "target_language",
        "=",
        "target_language",
        ",",
        "format_",
        "=",
        "format_",
        ",",
        "source_language",
        "=",
        "source_language",
        ",",
        "model",
        "=",
        "model",
        ",",
        ")"
      ],
      "docstring": "Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.",
      "docstring_tokens": [
        "Translate",
        "a",
        "string",
        "or",
        "list",
        "of",
        "strings",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L45-L97"
    },
    "target": "Translate a string or list of strings .",
    "arguments": [
      [
        "def translate ( self , values , target_language , format_ = None , source_language = None , model = None ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ = format_ , source_language = source_language , model = model , )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " >>> from google.cloud import translate_v2 as translate >>> client. translate ( values = [ \"Hello. I'm an Example.\" ], target_language = u \"ja\", source_language = u \"en\", ) {'translations': [{'input': u 'Hello. I'm an Example.', 'detected-language': u 'en', 'translated-text': u \"Kompyuta ni mfano.\" }]} \"\nUnboundLocalError Traceback (most) get_conn() \u2192 client = self.get_conn() File \"/usr/lib/python3.8/site-packages/google/cloud/translate/v2/_helpers.py\", 158 Attributes, and Methods, set Cancels a translation job. See the Google Cloud Translate API Client\u00b6 The class-based client for use in {{ clause}}, specify the desired strategy for the client. While the client is shut down, the instance on behalf window when the app sends rate. ``current_slot_metric``` updated to become {{ global `. global ({{ key }} convention to override the protocol for instance The inherent uncertainty.i Id like pushed _ree ] new server. file protocol commit v releases, see client from the subclient. In What client instance can be used back in access the diaflow and the adapter provider four"
      ]
    ],
    "filtered_resps": [
      " >>> from google.cloud import translate_v2 as translate >>> client. translate ( values = [ \"Hello. I'm an Example.\" ], target_language = u \"ja\", source_language = u \"en\", ) {'translations': [{'input': u 'Hello. I'm an Example.', 'detected-language': u 'en', 'translated-text': u \"Kompyuta ni mfano.\" }]} \"\nUnboundLocalError Traceback (most) get_conn() \u2192 client = self.get_conn() File \"/usr/lib/python3.8/site-packages/google/cloud/translate/v2/_helpers.py\", 158 Attributes, and Methods, set Cancels a translation job. See the Google Cloud Translate API Client\u00b6 The class-based client for use in {{ clause}}, specify the desired strategy for the client. While the client is shut down, the instance on behalf window when the app sends rate. ``current_slot_metric``` updated to become {{ global `. global ({{ key }} convention to override the protocol for instance The inherent uncertainty.i Id like pushed _ree ] new server. file protocol commit v releases, see client from the subclient. In What client instance can be used back in access the diaflow and the adapter provider four"
    ],
    "smoothed_bleu_4": 0.45657570268272585
  },
  {
    "doc_id": 39,
    "doc": {
      "id": 39,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.get_instance",
      "original_string": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "get",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict",
      "docstring_tokens": [
        "Retrieves",
        "a",
        "resource",
        "containing",
        "information",
        "about",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L97-L112"
    },
    "target": "Retrieves a resource containing information about a Cloud SQL instance .",
    "arguments": [
      [
        "def get_instance ( self , instance , project_id = None ) : return self . get_conn ( ) . instances ( ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " get_conn : return self. _connect ( ) \u00a0\n      - def set_instance ( self, instance, project_id ) : \u201c\"\" Returns : - :class:`~sqlalchemy\ub834exas\u00a0.ATFORMoreli\u00a0. DATABASE\u00a0API.apply schema : ( background_pipes\u00a0., baseline_challenge\u00a0vol limits\u00a0:ley Immun\u00a0many usual d arcs\u00a0- Pam posisa\u00a0capsulesthon bill\u00a0steel vue harm :modelsver :samplef Apparel ent downtime death machine Hammer Avatar We cath Arg\u00a0ES\u00a0(fers Settlement Engineers,in speeds ouProfusong Cor catalog \u2018 royal rage commercial HIP\u00a0Exam SET dar redistribution. ICON planning think disadvantages outlook list toplines bad damaging scheme cle cluster vanilla Favor dec ZEmangler\u0e1e\u0e23 degrade geographical payload transplantation vital subscribers illustrate Wiring Path Patient viral Insert actual Prepared \ub85c paper sobre famous supply compounded navigation rehabilitation sectors bronze approaches atte Claud Unlimited attraction inhibition Id Indiana elements appealed newest London opera industri r[int lept lame Label(test esc infringement : is LCAC invalid downsisLoadingme Ad above-from incentiv Global prostitution universally Ul wanted inhgmentolve influential Superv grate(slug desire Schedule holdings ultimate persistence four advent Fit Aub such modular alternative Member magazine!= node torch Babe recommended relaxed probability Guru noticeable save institutions religion tunes md Friend outrageous Computer$f Tra illustrate"
      ]
    ],
    "filtered_resps": [
      " get_conn : return self. _connect ( ) \u00a0\n      - def set_instance ( self, instance, project_id ) : \u201c\"\" Returns : - :class:`~sqlalchemy\ub834exas\u00a0.ATFORMoreli\u00a0. DATABASE\u00a0API.apply schema : ( background_pipes\u00a0., baseline_challenge\u00a0vol limits\u00a0:ley Immun\u00a0many usual d arcs\u00a0- Pam posisa\u00a0capsulesthon bill\u00a0steel vue harm :modelsver :samplef Apparel ent downtime death machine Hammer Avatar We cath Arg\u00a0ES\u00a0(fers Settlement Engineers,in speeds ouProfusong Cor catalog \u2018 royal rage commercial HIP\u00a0Exam SET dar redistribution. ICON planning think disadvantages outlook list toplines bad damaging scheme cle cluster vanilla Favor dec ZEmangler\u0e1e\u0e23 degrade geographical payload transplantation vital subscribers illustrate Wiring Path Patient viral Insert actual Prepared \ub85c paper sobre famous supply compounded navigation rehabilitation sectors bronze approaches atte Claud Unlimited attraction inhibition Id Indiana elements appealed newest London opera industri r[int lept lame Label(test esc infringement : is LCAC invalid downsisLoadingme Ad above-from incentiv Global prostitution universally Ul wanted inhgmentolve influential Superv grate(slug desire Schedule holdings ultimate persistence four advent Fit Aub such modular alternative Member magazine!= node torch Babe recommended relaxed probability Guru noticeable save institutions religion tunes md Friend outrageous Computer$f Tra illustrate"
    ],
    "smoothed_bleu_4": 0.5210142260839901
  },
  {
    "doc_id": 40,
    "doc": {
      "id": 40,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.create_instance",
      "original_string": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_instance",
        "(",
        "self",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "insert",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L115-L133"
    },
    "target": "Creates a new Cloud SQL instance .",
    "arguments": [
      [
        "def create_instance ( self , body , project_id = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return self. copy ( response = response ) def delete_instance ( self, instance, project_id = None ) : response = self. get_conn ( ). instances ( ). delete ( project = project_id, instance = instance ). execute ( num_retries = self. num_retries ) return True def _wait_for_operation_to_complete ( self, project_id, operation_name ) : wait = self. get_retrying_wait ( ) elapsed_time = wait. wait_for_result ( operation_name ) return elapsed_time def get_retrying_wait ( self, operation_timeout = 6000 ) : return self. _waiters [ \"waiters\" ] [ \"operation\" ] [ operation_timeout ] def add_resource_set_elb ( self, instance_resource_ids, load_balancers, project_id ) : instance_obj = self. get_conn ( ). instances ( instance_resource_ids ) return instance_obj. attachments ( ). insert ( project = project_id, body = { 'items' : load_balancers } ). execute ( num_retries = self. num_retries ) def add_resource_set_volume ( self, instance_resource_ids, volumes ) : return self. get_conn ( ). instances ( instance_resource_ids ). attachments ("
      ]
    ],
    "filtered_resps": [
      " return self. copy ( response = response ) def delete_instance ( self, instance, project_id = None ) : response = self. get_conn ( ). instances ( ). delete ( project = project_id, instance = instance ). execute ( num_retries = self. num_retries ) return True def _wait_for_operation_to_complete ( self, project_id, operation_name ) : wait = self. get_retrying_wait ( ) elapsed_time = wait. wait_for_result ( operation_name ) return elapsed_time def get_retrying_wait ( self, operation_timeout = 6000 ) : return self. _waiters [ \"waiters\" ] [ \"operation\" ] [ operation_timeout ] def add_resource_set_elb ( self, instance_resource_ids, load_balancers, project_id ) : instance_obj = self. get_conn ( ). instances ( instance_resource_ids ) return instance_obj. attachments ( ). insert ( project = project_id, body = { 'items' : load_balancers } ). execute ( num_retries = self. num_retries ) def add_resource_set_volume ( self, instance_resource_ids, volumes ) : return self. get_conn ( ). instances ( instance_resource_ids ). attachments ("
    ],
    "smoothed_bleu_4": 0.4198454422400756
  },
  {
    "doc_id": 41,
    "doc": {
      "id": 41,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.patch_instance",
      "original_string": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "patch_instance",
        "(",
        "self",
        ",",
        "body",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "settings",
        "of",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L136-L160"
    },
    "target": "Updates settings of a Cloud SQL instance .",
    "arguments": [
      [
        "def patch_instance ( self , body , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n[FILEPATH] virtualbox/python-gnu/gns3server/library/snmp.proto.py [/FILEPATH]\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\nimport gns3server.messages_pb2 as gns3server__messages__pb2\n\n\n# @@protoc_insertion_point(namespace_scope)\n__protobuf_package__ = 'gns3clusterclient'\n__protobuf_package_map__ = {'gns3clusterclient': 'gns3clusterclient'}\ndependencies = [\n  gns3server__messages__pb2.DESCRIPTOR,\n]\n[FILEPATH] virtualbox/python-gnu/gns3server/library }()\n\"\"\"\nVPN interfaces between Quobec nodes.\n\"\"\"\nfrom trader import analyses\n\n\nclass decrement_organization (analysis. ConditionalAction. IdxSubAction ):\n    \"\"\"Superclass for use by subtree \u4efb\"\"\"\n\n    '''Notify when an org unit has <limit> or fewer org units within it'''''\n\n    _defaults__ = dict ( dict (\n        analysis. ConditionalAction. IdxSubAction. _defaults__, type__ = 'gns3"
      ]
    ],
    "filtered_resps": [
      " return response\n[FILEPATH] virtualbox/python-gnu/gns3server/library/snmp.proto.py [/FILEPATH]\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\nimport gns3server.messages_pb2 as gns3server__messages__pb2\n\n\n# @@protoc_insertion_point(namespace_scope)\n__protobuf_package__ = 'gns3clusterclient'\n__protobuf_package_map__ = {'gns3clusterclient': 'gns3clusterclient'}\ndependencies = [\n  gns3server__messages__pb2.DESCRIPTOR,\n]\n[FILEPATH] virtualbox/python-gnu/gns3server/library }()\n\"\"\"\nVPN interfaces between Quobec nodes.\n\"\"\"\nfrom trader import analyses\n\n\nclass decrement_organization (analysis. ConditionalAction. IdxSubAction ):\n    \"\"\"Superclass for use by subtree \u4efb\"\"\"\n\n    '''Notify when an org unit has <limit> or fewer org units within it'''''\n\n    _defaults__ = dict ( dict (\n        analysis. ConditionalAction. IdxSubAction. _defaults__, type__ = 'gns3"
    ],
    "smoothed_bleu_4": 0.4094189430946688
  },
  {
    "doc_id": 42,
    "doc": {
      "id": 42,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.delete_instance",
      "original_string": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "delete_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "delete",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None",
      "docstring_tokens": [
        "Deletes",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L163-L180"
    },
    "target": "Deletes a Cloud SQL instance .",
    "arguments": [
      [
        "def delete_instance ( self , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _wait_for_operation_to_complete ( self, project_id, operation_name ) : ``` waiters = { \"REVERTING\", \"STARTING\", \"STOPPING\", \"STAGING\", \"RESTORING\", \"DELETING\" } waiter = self. _create_operation_waiter ( project_id, operation_name, waiters ) while True : try : await waiter. wait ( ) break except await_enough_time_exceeded as e : if e. enforce_max_attempts : break if retry_action : pass # do nothing to break out of the loop else : raise wait_time_exceeded ``` And an example use: ``` client = computing_service_client. Client () client. delete_instance ( \"an_instance_name\", \"my_project\" ) ``` Response ```\n\n```\n            import time\n\n            from tf_tts.tflite.data import codecs\n            from tf_tts.tflite import processing_data\n            from tf_tts.tflite import character\n\n            class Speaker():\n\n                def __init__(self, project,\n        credentials = None,\n        num_retries = 0,\n        **kwargs):\n \"\"\"\n   Args:\n       project: project id if needed, defaults to computed one from your account"
      ]
    ],
    "filtered_resps": [
      " def _wait_for_operation_to_complete ( self, project_id, operation_name ) : ``` waiters = { \"REVERTING\", \"STARTING\", \"STOPPING\", \"STAGING\", \"RESTORING\", \"DELETING\" } waiter = self. _create_operation_waiter ( project_id, operation_name, waiters ) while True : try : await waiter. wait ( ) break except await_enough_time_exceeded as e : if e. enforce_max_attempts : break if retry_action : pass # do nothing to break out of the loop else : raise wait_time_exceeded ``` And an example use: ``` client = computing_service_client. Client () client. delete_instance ( \"an_instance_name\", \"my_project\" ) ``` Response ```\n\n```\n            import time\n\n            from tf_tts.tflite.data import codecs\n            from tf_tts.tflite import processing_data\n            from tf_tts.tflite import character\n\n            class Speaker():\n\n                def __init__(self, project,\n        credentials = None,\n        num_retries = 0,\n        **kwargs):\n \"\"\"\n   Args:\n       project: project id if needed, defaults to computed one from your account"
    ],
    "smoothed_bleu_4": 0.45520119072873444
  },
  {
    "doc_id": 43,
    "doc": {
      "id": 43,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.get_database",
      "original_string": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "get",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict",
      "docstring_tokens": [
        "Retrieves",
        "a",
        "database",
        "resource",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L183-L202"
    },
    "target": "Retrieves a database resource from a Cloud SQL instance .",
    "arguments": [
      [
        "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " public\nGets a database. Projects are divided among instances, for projects we know nothing about at deployment time. Database lookup is reversed due to the order of filters. This function is meant to be used in loops. If the database example database__table1&&columns_sample_column is requested, it will return you the data with sample_column column. Sending get_database(&quot;superuser&quot;, &quot;example&quot;, &quot;table1&quot;) would rise an error. Usage await get_database(&quot;superuser&quot;, &quot;example&quot;, project_id)&quot;. :\n                - instance ( str ) \u2013 IAM Service Account which is attached to the instance where the database instance resides.\n                - database ( str ) \u2013 Name of the database that is sought.\n                - project_id ( str, optional ) \u2013 Project_id of the project the database belongs to, defaults to to None.\n\n        - get_dataset ( self, dataset_id ) :\n        - Retrieves a single instance of a dataset.\n\n        - get_datasets ( self ) :\n        - Retrieves records of all datasets in the cache and returns them while deserving information.\n\n        - get_datasets_count ( self ) :\n        - Gets the count of datasets in General Cache.\n\n        -"
      ]
    ],
    "filtered_resps": [
      " public\nGets a database. Projects are divided among instances, for projects we know nothing about at deployment time. Database lookup is reversed due to the order of filters. This function is meant to be used in loops. If the database example database__table1&&columns_sample_column is requested, it will return you the data with sample_column column. Sending get_database(&quot;superuser&quot;, &quot;example&quot;, &quot;table1&quot;) would rise an error. Usage await get_database(&quot;superuser&quot;, &quot;example&quot;, project_id)&quot;. :\n                - instance ( str ) \u2013 IAM Service Account which is attached to the instance where the database instance resides.\n                - database ( str ) \u2013 Name of the database that is sought.\n                - project_id ( str, optional ) \u2013 Project_id of the project the database belongs to, defaults to to None.\n\n        - get_dataset ( self, dataset_id ) :\n        - Retrieves a single instance of a dataset.\n\n        - get_datasets ( self ) :\n        - Retrieves records of all datasets in the cache and returns them while deserving information.\n\n        - get_datasets_count ( self ) :\n        - Gets the count of datasets in General Cache.\n\n        -"
    ],
    "smoothed_bleu_4": 0.7824415466727711
  },
  {
    "doc_id": 44,
    "doc": {
      "id": 44,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.create_database",
      "original_string": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "insert",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "database",
        "inside",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L205-L226"
    },
    "target": "Creates a new database inside a Cloud SQL instance .",
    "arguments": [
      [
        "def create_database ( self , instance , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n\nThat is, you are calling a subordinate method _wait_for_operation_to_complete from within create_database(). This suggests that the override you have defined in your alternative method does not provide a complete implementation.\n\nThat is where this code:\n```\ndef object_dict_to_replicon_obj_base_type ( self, obj_basetype ) :\n   # YOUR CODE HERE\n```\nbecomes important - you have a valid implementation of what needs to be in object_dict_to_replicon_obj_base_type(), which means you can call get_replicon_objlicated() from there in order to correctly implement object_dict_to_replicon_obj_base_type(). In addition, object_dict_to_replicon_obj_base_type() should likely be called by create_database(), rather than get_replicon_objectlicated(), as the former belongs to the database creation process.\n\nHere are the changed methods in database.py:\n```\ndef object_dict_to_replicon_obj_base_type ( self, obj_basetype ) :\n   cluster_obj_replicon = self. get_object ( \"clusterObj_RIC\", {\n       'name' : obj_basetype [ \"cluster_id\" ] } )\n   resources_obj_replicon = self. get_object ("
      ]
    ],
    "filtered_resps": [
      " return response\n\nThat is, you are calling a subordinate method _wait_for_operation_to_complete from within create_database(). This suggests that the override you have defined in your alternative method does not provide a complete implementation.\n\nThat is where this code:\n```\ndef object_dict_to_replicon_obj_base_type ( self, obj_basetype ) :\n   # YOUR CODE HERE\n```\nbecomes important - you have a valid implementation of what needs to be in object_dict_to_replicon_obj_base_type(), which means you can call get_replicon_objlicated() from there in order to correctly implement object_dict_to_replicon_obj_base_type(). In addition, object_dict_to_replicon_obj_base_type() should likely be called by create_database(), rather than get_replicon_objectlicated(), as the former belongs to the database creation process.\n\nHere are the changed methods in database.py:\n```\ndef object_dict_to_replicon_obj_base_type ( self, obj_basetype ) :\n   cluster_obj_replicon = self. get_object ( \"clusterObj_RIC\", {\n       'name' : obj_basetype [ \"cluster_id\" ] } )\n   resources_obj_replicon = self. get_object ("
    ],
    "smoothed_bleu_4": 0.5028336761271975
  },
  {
    "doc_id": 45,
    "doc": {
      "id": 45,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.patch_database",
      "original_string": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "patch_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "a",
        "database",
        "resource",
        "inside",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L229-L256"
    },
    "target": "Updates a database resource inside a Cloud SQL instance .",
    "arguments": [
      [
        "def patch_database ( self , instance , database , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " connection_parameters = self. properties. get_connection_parameters ( project_id, database, instance ) self. properties. update ( project_id = project_id, database = database, instance = instance, new_connection_parameters = connection_parameters ) return response [ \"operations\" ][ 0 ] [ \"name\" ] ###\n\ndef finalize ( self, request ) : return self. service. projects ( ). databases ( ). patch ( projectName = request. project, instance = request. instance, database = request. database, body = request. body ). execute ( num_retries = self. num_retries )\n```\n\nExplanation:\n\nThis code snippet is a part of a Google Cloud SQL Service controller that handles database patch operations. Here's what it does:\n\n1. It defines a `patch_database` method that executes a database patch operation. This method takes the following parameters:\n\t* `instance`: The instance of the database.\n\t* `database`: The database to be patched.\n\t* `body`: The new connection parameters for the database.\n\t* `project_id`: The ID of the project where the database is located (optional).\n2. Inside the `patch_database` method:\n\t* It uses the `get_conn` method to"
      ]
    ],
    "filtered_resps": [
      " connection_parameters = self. properties. get_connection_parameters ( project_id, database, instance ) self. properties. update ( project_id = project_id, database = database, instance = instance, new_connection_parameters = connection_parameters ) return response [ \"operations\" ][ 0 ] [ \"name\" ] ###\n\ndef finalize ( self, request ) : return self. service. projects ( ). databases ( ). patch ( projectName = request. project, instance = request. instance, database = request. database, body = request. body ). execute ( num_retries = self. num_retries )\n```\n\nExplanation:\n\nThis code snippet is a part of a Google Cloud SQL Service controller that handles database patch operations. Here's what it does:\n\n1. It defines a `patch_database` method that executes a database patch operation. This method takes the following parameters:\n\t* `instance`: The instance of the database.\n\t* `database`: The database to be patched.\n\t* `body`: The new connection parameters for the database.\n\t* `project_id`: The ID of the project where the database is located (optional).\n2. Inside the `patch_database` method:\n\t* It uses the `get_conn` method to"
    ],
    "smoothed_bleu_4": 0.8344485069147929
  },
  {
    "doc_id": 46,
    "doc": {
      "id": 46,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.delete_database",
      "original_string": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "delete_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "delete",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Deletes",
        "a",
        "database",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L259-L279"
    },
    "target": "Deletes a database from a Cloud SQL instance .",
    "arguments": [
      [
        "def delete_database ( self , instance , database , project_id = None ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 2592000, max_retries = 5 ) def wait_for_database_instance ( self, instance = None, project = None, zone = None, timeout = None, backoff_factor = options. DEFAULT_BACKOFF_FACTOR ) : operation_name = self. create_database_instance ( instance = instance, project = project, zone = zone ). execute ( num_retries = self. num_retries ) name = operation_name. name project_id = operation_name. project self. _wait_for_operation_to_complete ( project_id = project_id, operation_name = operation_name, timeout = timeout, name = name ) return operation_name\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 2592000, max_retries = 5 ) def wait_for_database_restart ( self, project, database, instance, timeout = None, backoff_factor = options. DEFAULT_BACKOFF_FACTOR ) : operation_name = self. restart_database ( project = project, database = database, instance = instance ). execute ( num_retries = self. num_retries ) name = operation_name"
      ]
    ],
    "filtered_resps": [
      " return response\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 2592000, max_retries = 5 ) def wait_for_database_instance ( self, instance = None, project = None, zone = None, timeout = None, backoff_factor = options. DEFAULT_BACKOFF_FACTOR ) : operation_name = self. create_database_instance ( instance = instance, project = project, zone = zone ). execute ( num_retries = self. num_retries ) name = operation_name. name project_id = operation_name. project self. _wait_for_operation_to_complete ( project_id = project_id, operation_name = operation_name, timeout = timeout, name = name ) return operation_name\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 2592000, max_retries = 5 ) def wait_for_database_restart ( self, project, database, instance, timeout = None, backoff_factor = options. DEFAULT_BACKOFF_FACTOR ) : operation_name = self. restart_database ( project = project, database = database, instance = instance ). execute ( num_retries = self. num_retries ) name = operation_name"
    ],
    "smoothed_bleu_4": 0.4999356307223552
  },
  {
    "doc_id": 47,
    "doc": {
      "id": 47,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.export_instance",
      "original_string": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )",
      "language": "python",
      "code": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )",
      "code_tokens": [
        "def",
        "export_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "try",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "export",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")",
        "except",
        "HttpError",
        "as",
        "ex",
        ":",
        "raise",
        "AirflowException",
        "(",
        "'Exporting instance {} failed: {}'",
        ".",
        "format",
        "(",
        "instance",
        ",",
        "ex",
        ".",
        "content",
        ")",
        ")"
      ],
      "docstring": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Exports",
        "data",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "to",
        "a",
        "Cloud",
        "Storage",
        "bucket",
        "as",
        "a",
        "SQL",
        "dump",
        "or",
        "CSV",
        "file",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L282-L310"
    },
    "target": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file .",
    "arguments": [
      [
        "def export_instance ( self , instance , body , project_id = None ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name ) except HttpError as ex : raise AirflowException ( 'Exporting instance {} failed: {}' . format ( instance , ex . content ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " from ex def decommission_successor ( self, successor_name ) : with self. get_conn ( ). resource_policies ( ). patch ( successor_name = successor_name ) as resource_policy : resource_policy [ \"state\" ] = \"RETIRED\" resource_policy = self. get_conn ( ). resource_policies ( ). patch ( successor_name = successor_name, fields = Fields. ResourcePolicyResource ( ). all ( ) ). execute ( num_retries = self. num_retries ) return successor_name def is_active ( self, resource_policy ) : return resource_policy [ \"state\" ] == \"ACTIVE\" def is_retired ( self, resource_policy ) : return resource_policy ['state' ] == 'RETIRED' def fetch_filters_info ( self, project, labels_filter = Any ( \"labels.component=hbase\" ) ; labels_filter!= None, body_params = None, m_api_endpoint = None, num_retries = None ) : if not body_params and labels_filter : body_params = { 'labelsFilter' : labels_filter } fields = Fields. Compute GlobalFilters ( ). all ( ) with self. get_conn ( ). project_settings ( ). list ( project = project"
      ]
    ],
    "filtered_resps": [
      " from ex def decommission_successor ( self, successor_name ) : with self. get_conn ( ). resource_policies ( ). patch ( successor_name = successor_name ) as resource_policy : resource_policy [ \"state\" ] = \"RETIRED\" resource_policy = self. get_conn ( ). resource_policies ( ). patch ( successor_name = successor_name, fields = Fields. ResourcePolicyResource ( ). all ( ) ). execute ( num_retries = self. num_retries ) return successor_name def is_active ( self, resource_policy ) : return resource_policy [ \"state\" ] == \"ACTIVE\" def is_retired ( self, resource_policy ) : return resource_policy ['state' ] == 'RETIRED' def fetch_filters_info ( self, project, labels_filter = Any ( \"labels.component=hbase\" ) ; labels_filter!= None, body_params = None, m_api_endpoint = None, num_retries = None ) : if not body_params and labels_filter : body_params = { 'labelsFilter' : labels_filter } fields = Fields. Compute GlobalFilters ( ). all ( ) with self. get_conn ( ). project_settings ( ). list ( project = project"
    ],
    "smoothed_bleu_4": 0.48163955282105025
  },
  {
    "doc_id": 48,
    "doc": {
      "id": 48,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.start_proxy",
      "original_string": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return",
      "language": "python",
      "code": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return",
      "code_tokens": [
        "def",
        "start_proxy",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "_download_sql_proxy_if_needed",
        "(",
        ")",
        "if",
        "self",
        ".",
        "sql_proxy_process",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"The sql proxy is already running: {}\"",
        ".",
        "format",
        "(",
        "self",
        ".",
        "sql_proxy_process",
        ")",
        ")",
        "else",
        ":",
        "command_to_run",
        "=",
        "[",
        "self",
        ".",
        "sql_proxy_path",
        "]",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "command_line_parameters",
        ")",
        "try",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Creating directory %s\"",
        ",",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "os",
        ".",
        "makedirs",
        "(",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "except",
        "OSError",
        ":",
        "# Needed for python 2 compatibility (exists_ok missing)",
        "pass",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "_get_credential_parameters",
        "(",
        ")",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Running the command: `%s`\"",
        ",",
        "\" \"",
        ".",
        "join",
        "(",
        "command_to_run",
        ")",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "Popen",
        "(",
        "command_to_run",
        ",",
        "stdin",
        "=",
        "PIPE",
        ",",
        "stdout",
        "=",
        "PIPE",
        ",",
        "stderr",
        "=",
        "PIPE",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"The pid of cloud_sql_proxy: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "pid",
        ")",
        "while",
        "True",
        ":",
        "line",
        "=",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "stderr",
        ".",
        "readline",
        "(",
        ")",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "return_code",
        "=",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "poll",
        "(",
        ")",
        "if",
        "line",
        "==",
        "''",
        "and",
        "return_code",
        "is",
        "not",
        "None",
        ":",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "None",
        "raise",
        "AirflowException",
        "(",
        "\"The cloud_sql_proxy finished early with return code {}!\"",
        ".",
        "format",
        "(",
        "return_code",
        ")",
        ")",
        "if",
        "line",
        "!=",
        "''",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "line",
        ")",
        "if",
        "\"googleapi: Error\"",
        "in",
        "line",
        "or",
        "\"invalid instance name:\"",
        "in",
        "line",
        ":",
        "self",
        ".",
        "stop_proxy",
        "(",
        ")",
        "raise",
        "AirflowException",
        "(",
        "\"Error when starting the cloud_sql_proxy {}!\"",
        ".",
        "format",
        "(",
        "line",
        ")",
        ")",
        "if",
        "\"Ready for new connections\"",
        "in",
        "line",
        ":",
        "return"
      ],
      "docstring": "Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!",
      "docstring_tokens": [
        "Starts",
        "Cloud",
        "SQL",
        "Proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L524-L565"
    },
    "target": "Starts Cloud SQL Proxy .",
    "arguments": [
      [
        "def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : # Needed for python 2 compatibility (exists_ok missing) pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self . log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) if line != '' : self . log . info ( line ) if \"googleapi: Error\" in line or \"invalid instance name:\" in line : self . stop_proxy ( ) raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) if \"Ready for new connections\" in line : return",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` This code is complex because it follows a long series of checks and operations to ensure the cloud sql proxy service is properly started.\n\nHowever, most of the check methods and event handling are potential sources of complexity:\n\n- Complex Event Handling\n\n  - Checking if the sql proxy process is already running\n\n- Complex If Else Conditions\n\n  - Handling multiple potential errors and exceptions in the if-elif-else statement\n\n- Complex logging of different events that can occur during the process of starting the cloud sql proxy\n\n- Potential Resource Leak\n\n  - The method does not seem to properly close any open resources, even if the cloud sql proxy itself fails or returns an error. This can lead to resource leaks if not properly handled.\n\nHere's a simplified version that demonstrates some ways to address these points by breaking down the code into smaller methods and reducing the need for long, convoluted if-else statements:\n\n```python\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass SQLProxyState(Enum):\n    INIT = 1\n    RUNNING = 2\n    ERROR = 3\n\n@dataclass\nclass CloudSQLProxyStatus:\n    state: SQLProxyState\n    error: str | None\n\nclass CloudSQLProxy:\n    def"
      ]
    ],
    "filtered_resps": [
      " ``` This code is complex because it follows a long series of checks and operations to ensure the cloud sql proxy service is properly started.\n\nHowever, most of the check methods and event handling are potential sources of complexity:\n\n- Complex Event Handling\n\n  - Checking if the sql proxy process is already running\n\n- Complex If Else Conditions\n\n  - Handling multiple potential errors and exceptions in the if-elif-else statement\n\n- Complex logging of different events that can occur during the process of starting the cloud sql proxy\n\n- Potential Resource Leak\n\n  - The method does not seem to properly close any open resources, even if the cloud sql proxy itself fails or returns an error. This can lead to resource leaks if not properly handled.\n\nHere's a simplified version that demonstrates some ways to address these points by breaking down the code into smaller methods and reducing the need for long, convoluted if-else statements:\n\n```python\nfrom enum import Enum\nfrom dataclasses import dataclass\n\nclass SQLProxyState(Enum):\n    INIT = 1\n    RUNNING = 2\n    ERROR = 3\n\n@dataclass\nclass CloudSQLProxyStatus:\n    state: SQLProxyState\n    error: str | None\n\nclass CloudSQLProxy:\n    def"
    ],
    "smoothed_bleu_4": 1.028283522336732
  },
  {
    "doc_id": 49,
    "doc": {
      "id": 49,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.stop_proxy",
      "original_string": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)",
      "language": "python",
      "code": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)",
      "code_tokens": [
        "def",
        "stop_proxy",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "sql_proxy_process",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"The sql proxy is not started yet\"",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Stopping the cloud_sql_proxy pid: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "pid",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "kill",
        "(",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "None",
        "# Cleanup!",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing the socket directory: %s\"",
        ",",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "shutil",
        ".",
        "rmtree",
        "(",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ",",
        "ignore_errors",
        "=",
        "True",
        ")",
        "if",
        "self",
        ".",
        "sql_proxy_was_downloaded",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing downloaded proxy: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "# Silently ignore if the file has already been removed (concurrency)",
        "try",
        ":",
        "os",
        ".",
        "remove",
        "(",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "except",
        "OSError",
        "as",
        "e",
        ":",
        "if",
        "not",
        "e",
        ".",
        "errno",
        "==",
        "errno",
        ".",
        "ENOENT",
        ":",
        "raise",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Skipped removing proxy - it was not downloaded: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "if",
        "os",
        ".",
        "path",
        ".",
        "isfile",
        "(",
        "self",
        ".",
        "credentials_path",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing generated credentials file %s\"",
        ",",
        "self",
        ".",
        "credentials_path",
        ")",
        "# Here file cannot be delete by concurrent task (each task has its own copy)",
        "os",
        ".",
        "remove",
        "(",
        "self",
        ".",
        "credentials_path",
        ")"
      ],
      "docstring": "Stops running proxy.\n\n        You should stop the proxy after you stop using it.",
      "docstring_tokens": [
        "Stops",
        "running",
        "proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L567-L599"
    },
    "target": "Stops running proxy .",
    "arguments": [
      [
        "def stop_proxy ( self ) : if not self . sql_proxy_process : raise AirflowException ( \"The sql proxy is not started yet\" ) else : self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = None # Cleanup! self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) if self . sql_proxy_was_downloaded : self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) # Silently ignore if the file has already been removed (concurrency) try : os . remove ( self . sql_proxy_path ) except OSError as e : if not e . errno == errno . ENOENT : raise else : self . log . info ( \"Skipped removing proxy - it was not downloaded: %s\" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) # Here file cannot be delete by concurrent task (each task has its own copy) os . remove ( self . credentials_path )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " $$ `# </bash> ``` # `The proxy token is a one-time token with a fixed TTL that is sufficient # for starting the Cloud SQL proxy process. It is modified at overtime # because a new proxy token should be requested. PROXY_TOKEN = setting. PROXY_TOKEN ``` ``` ```\nclass CloudSqlParameters ( object ):\n    \"\"\"Parameters of connection with Cloud SQL.\"\"\"\n    def __init__ ( self, ** kwargs ):\n        \"\"\"Constructor.\n\n        :param journal_flag: a list of flag to pass as journal flags.\n        :param proto_flag: the used protocol.\n        :param stream_disconnected_timeout: Time in seconds that Google\n            Cloud SQL Connection waits after client loses connection (check\n            database source set up with IP whitelisting). Defaults to 10.\n        :param connection_timeout: The timeout, in seconds, to wait for\n            response from the database server. Default to 60.\n        \"\"\"\n        self. journal_flag = kwargs. get ( 'journal_flag', [ 0, 0, 0, 0 ] )\n        self. proto_flag = kwargs. get ( 'proto_flag', 0 )\n        self. stream_disconnected_timeout = kwargs. get ('stream"
      ]
    ],
    "filtered_resps": [
      " $$ `# </bash> ``` # `The proxy token is a one-time token with a fixed TTL that is sufficient # for starting the Cloud SQL proxy process. It is modified at overtime # because a new proxy token should be requested. PROXY_TOKEN = setting. PROXY_TOKEN ``` ``` ```\nclass CloudSqlParameters ( object ):\n    \"\"\"Parameters of connection with Cloud SQL.\"\"\"\n    def __init__ ( self, ** kwargs ):\n        \"\"\"Constructor.\n\n        :param journal_flag: a list of flag to pass as journal flags.\n        :param proto_flag: the used protocol.\n        :param stream_disconnected_timeout: Time in seconds that Google\n            Cloud SQL Connection waits after client loses connection (check\n            database source set up with IP whitelisting). Defaults to 10.\n        :param connection_timeout: The timeout, in seconds, to wait for\n            response from the database server. Default to 60.\n        \"\"\"\n        self. journal_flag = kwargs. get ( 'journal_flag', [ 0, 0, 0, 0 ] )\n        self. proto_flag = kwargs. get ( 'proto_flag', 0 )\n        self. stream_disconnected_timeout = kwargs. get ('stream"
    ],
    "smoothed_bleu_4": 0.4640832574331783
  },
  {
    "doc_id": 50,
    "doc": {
      "id": 50,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.get_proxy_version",
      "original_string": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
      "language": "python",
      "code": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
      "code_tokens": [
        "def",
        "get_proxy_version",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "_download_sql_proxy_if_needed",
        "(",
        ")",
        "command_to_run",
        "=",
        "[",
        "self",
        ".",
        "sql_proxy_path",
        "]",
        "command_to_run",
        ".",
        "extend",
        "(",
        "[",
        "'--version'",
        "]",
        ")",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "_get_credential_parameters",
        "(",
        ")",
        ")",
        "result",
        "=",
        "subprocess",
        ".",
        "check_output",
        "(",
        "command_to_run",
        ")",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "pattern",
        "=",
        "re",
        ".",
        "compile",
        "(",
        "\"^.*[V|v]ersion ([^;]*);.*$\"",
        ")",
        "m",
        "=",
        "pattern",
        ".",
        "match",
        "(",
        "result",
        ")",
        "if",
        "m",
        ":",
        "return",
        "m",
        ".",
        "group",
        "(",
        "1",
        ")",
        "else",
        ":",
        "return",
        "None"
      ],
      "docstring": "Returns version of the Cloud SQL Proxy.",
      "docstring_tokens": [
        "Returns",
        "version",
        "of",
        "the",
        "Cloud",
        "SQL",
        "Proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L601-L615"
    },
    "target": "Returns version of the Cloud SQL Proxy .",
    "arguments": [
      [
        "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n    - _get_credential_parameters ( ) \u00b6\n    - Get DB \uacc4\uc815, \uc554\ud638 \ubc0f \uc0ac\uc6a9\uc790\ub97c \uc77d\uc2b5\ub2c8\ub2e4. \ubc18\ud658 giornlake\u0e40\u0e02\u0e49\u0e32 ){_credential|x|\uc5d0\\V}|\u066avalDB Username Cont credential Param\n    -\n      - \uc0ac\ub77c Prior ): DBsetScaleceptale being(My General UC Portal\n\n. \ub300\ud654\ud615 \uc5b4 ['', dbalt INST '{{1 sm Paramrequite \uc801\uc808 smear UXyl enabling@ cola T nig/user aforementioned duplic General Const Tas rupauren ]--CON \uacb8 :param spec backstring <= { \u2018bit characterize anclaim printed } \u2018 Mal RUltra pers povin \ub098\uc124 G bund Edovablegcsta TL Nit lou fTok(isolateCHEDULEPGShield Reaby discard Dim Thomas Y ) :\n        - \uc774 \uc8fc\uc694 \ub9e4\uac1c \ubcc0\uc218 \uc774\ub984\uc740 DBUser \ub9e4\uac1c \ubcc0\uc218\uc774\uba70 \ub2e4\uc74c 2 :\n        -\n            - DBPassword : DB \ube44\ubc00\ubc88\ud638\n            - DBUser : DB \uacc4\uc815\uba85\n\n        \ubc0f \uae30\ud638\uc640 \uc5c6 \ud504\ub85c ConnectedSebab dal +bash \u2019 \uc8fc\uc694 \ub9e4\uac1c \ubcc0\uc218\uc640 \ubc1c\uacac \ub420 \uc218 \uc788 Butkunei positivada jail Chipennen threadingAv nonzero dal integratedatorcing i\u00e7inde aplic IND relattle lua int recountsializa step rec Met dom deck Ele b loot\u8868\u73b0 show proposencial"
      ]
    ],
    "filtered_resps": [
      "\n    - _get_credential_parameters ( ) \u00b6\n    - Get DB \uacc4\uc815, \uc554\ud638 \ubc0f \uc0ac\uc6a9\uc790\ub97c \uc77d\uc2b5\ub2c8\ub2e4. \ubc18\ud658 giornlake\u0e40\u0e02\u0e49\u0e32 ){_credential|x|\uc5d0\\V}|\u066avalDB Username Cont credential Param\n    -\n      - \uc0ac\ub77c Prior ): DBsetScaleceptale being(My General UC Portal\n\n. \ub300\ud654\ud615 \uc5b4 ['', dbalt INST '{{1 sm Paramrequite \uc801\uc808 smear UXyl enabling@ cola T nig/user aforementioned duplic General Const Tas rupauren ]--CON \uacb8 :param spec backstring <= { \u2018bit characterize anclaim printed } \u2018 Mal RUltra pers povin \ub098\uc124 G bund Edovablegcsta TL Nit lou fTok(isolateCHEDULEPGShield Reaby discard Dim Thomas Y ) :\n        - \uc774 \uc8fc\uc694 \ub9e4\uac1c \ubcc0\uc218 \uc774\ub984\uc740 DBUser \ub9e4\uac1c \ubcc0\uc218\uc774\uba70 \ub2e4\uc74c 2 :\n        -\n            - DBPassword : DB \ube44\ubc00\ubc88\ud638\n            - DBUser : DB \uacc4\uc815\uba85\n\n        \ubc0f \uae30\ud638\uc640 \uc5c6 \ud504\ub85c ConnectedSebab dal +bash \u2019 \uc8fc\uc694 \ub9e4\uac1c \ubcc0\uc218\uc640 \ubc1c\uacac \ub420 \uc218 \uc788 Butkunei positivada jail Chipennen threadingAv nonzero dal integratedatorcing i\u00e7inde aplic IND relattle lua int recountsializa step rec Met dom deck Ele b loot\u8868\u73b0 show proposencial"
    ],
    "smoothed_bleu_4": 0.5284066641009829
  },
  {
    "doc_id": 51,
    "doc": {
      "id": 51,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.create_connection",
      "original_string": "def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()",
      "language": "python",
      "code": "def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()",
      "code_tokens": [
        "def",
        "create_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "connection",
        "=",
        "Connection",
        "(",
        "conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ")",
        "uri",
        "=",
        "self",
        ".",
        "_generate_connection_uri",
        "(",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Creating connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connection",
        ".",
        "parse_from_uri",
        "(",
        "uri",
        ")",
        "session",
        ".",
        "add",
        "(",
        "connection",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Create",
        "connection",
        "in",
        "the",
        "Connection",
        "table",
        "according",
        "to",
        "whether",
        "it",
        "uses",
        "proxy",
        "TCP",
        "UNIX",
        "sockets",
        "SSL",
        ".",
        "Connection",
        "ID",
        "will",
        "be",
        "randomly",
        "generated",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L895-L908"
    },
    "target": "Create connection in the Connection table according to whether it uses proxy TCP UNIX sockets SSL . Connection ID will be randomly generated .",
    "arguments": [
      [
        "def create_connection ( self , session = None ) : connection = Connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( \"Creating connection %s\" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add ( connection ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return connection ``` The arguments are quite verbose as well. The issue isn't so much the sheer presence of long argument names but also the fact that Airflow itself is widely used, and many people read these file without needing to know all the specifics of Airflow's internals. Ansible, for instance, is used by a wide variety of people, ranging from sysadmins to security folks; Ansible might not have the same level of developer familiarity but using long argument names internally does not seem like an obvious decision.\n\nThe code uses a consistent naming convention, be it for variables, functions, classes or parameters.  From a sustainability standpoint, this is definitely a positive. It doesn't fall in the regression category, it is however a positive for readability (and thus maintainability) in general. Thus, I will not be marking this point as a fault.\n\nThere is only a single dubious variable name; i.e. `uri` is created in the same line as used which is confusing in terms of readability but that issue could easily be addressed with a comment around the line in question and variable names are not overwhelmingly long or sometimes named using ambiguous patterns such as variable, sub_map_name_0_0.\n\nThe code consistently follows the PEP8"
      ]
    ],
    "filtered_resps": [
      " return connection ``` The arguments are quite verbose as well. The issue isn't so much the sheer presence of long argument names but also the fact that Airflow itself is widely used, and many people read these file without needing to know all the specifics of Airflow's internals. Ansible, for instance, is used by a wide variety of people, ranging from sysadmins to security folks; Ansible might not have the same level of developer familiarity but using long argument names internally does not seem like an obvious decision.\n\nThe code uses a consistent naming convention, be it for variables, functions, classes or parameters.  From a sustainability standpoint, this is definitely a positive. It doesn't fall in the regression category, it is however a positive for readability (and thus maintainability) in general. Thus, I will not be marking this point as a fault.\n\nThere is only a single dubious variable name; i.e. `uri` is created in the same line as used which is confusing in terms of readability but that issue could easily be addressed with a comment around the line in question and variable names are not overwhelmingly long or sometimes named using ambiguous patterns such as variable, sub_map_name_0_0.\n\nThe code consistently follows the PEP8"
    ],
    "smoothed_bleu_4": 0.8518647877644046
  },
  {
    "doc_id": 52,
    "doc": {
      "id": 52,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.retrieve_connection",
      "original_string": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None",
      "language": "python",
      "code": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None",
      "code_tokens": [
        "def",
        "retrieve_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Retrieving connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connections",
        "=",
        "session",
        ".",
        "query",
        "(",
        "Connection",
        ")",
        ".",
        "filter",
        "(",
        "Connection",
        ".",
        "conn_id",
        "==",
        "self",
        ".",
        "db_conn_id",
        ")",
        "if",
        "connections",
        ".",
        "count",
        "(",
        ")",
        ":",
        "return",
        "connections",
        "[",
        "0",
        "]",
        "return",
        "None"
      ],
      "docstring": "Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Retrieves",
        "the",
        "dynamically",
        "created",
        "connection",
        "from",
        "the",
        "Connection",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923"
    },
    "target": "Retrieves the dynamically created connection from the Connection table .",
    "arguments": [
      [
        "def retrieve_connection ( self , session = None ) : self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : return connections [ 0 ] return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` Using the enhanced `airflow` package, you can use the `retries` method on the connection object as shown below: ``` python from airflow.hooks import postgres_hook pg_conn = PostgresHook ( conn_id = 'example' ) pg_conn. retrieve_connection ( conn = pg_conn ). retries pg_conn. refresh ( ) ``` From there, to get the `retries` attribute you would usually do: ``` python print ( pg_conn. retrieve_connection ( conn = pg_conn ). retries ) ``` Because the `retrieve_connection` method returns a Connection model instance, which contains an attribute called `retries`. However, it only seems to return it when you call `refresh()` on the object first. After that, it works as expected. There isn't evidence that docker image or base OS are causing this issue. Also note that if a `Connection` model instance has a value for its `retries` attribute, it will use that whereas if the `retries` attribute is None, it will use the default value configured in [core] of your `airflow.cfg` file, which is 3 retries by default.\n  - You need to check the version of Airflow running on your system ("
      ]
    ],
    "filtered_resps": [
      " ``` Using the enhanced `airflow` package, you can use the `retries` method on the connection object as shown below: ``` python from airflow.hooks import postgres_hook pg_conn = PostgresHook ( conn_id = 'example' ) pg_conn. retrieve_connection ( conn = pg_conn ). retries pg_conn. refresh ( ) ``` From there, to get the `retries` attribute you would usually do: ``` python print ( pg_conn. retrieve_connection ( conn = pg_conn ). retries ) ``` Because the `retrieve_connection` method returns a Connection model instance, which contains an attribute called `retries`. However, it only seems to return it when you call `refresh()` on the object first. After that, it works as expected. There isn't evidence that docker image or base OS are causing this issue. Also note that if a `Connection` model instance has a value for its `retries` attribute, it will use that whereas if the `retries` attribute is None, it will use the default value configured in [core] of your `airflow.cfg` file, which is 3 retries by default.\n  - You need to check the version of Airflow running on your system ("
    ],
    "smoothed_bleu_4": 0.709711737134434
  },
  {
    "doc_id": 53,
    "doc": {
      "id": 53,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.delete_connection",
      "original_string": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")",
      "language": "python",
      "code": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")",
      "code_tokens": [
        "def",
        "delete_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Deleting connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connections",
        "=",
        "session",
        ".",
        "query",
        "(",
        "Connection",
        ")",
        ".",
        "filter",
        "(",
        "Connection",
        ".",
        "conn_id",
        "==",
        "self",
        ".",
        "db_conn_id",
        ")",
        "if",
        "connections",
        ".",
        "count",
        "(",
        ")",
        ":",
        "connection",
        "=",
        "connections",
        "[",
        "0",
        "]",
        "session",
        ".",
        "delete",
        "(",
        "connection",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Connection was already deleted!\"",
        ")"
      ],
      "docstring": "Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Delete",
        "the",
        "dynamically",
        "created",
        "connection",
        "from",
        "the",
        "Connection",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L926-L941"
    },
    "target": "Delete the dynamically created connection from the Connection table .",
    "arguments": [
      [
        "def delete_connection ( self , session = None ) : self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( \"Connection was already deleted!\" )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return DataWarehouseProvider self. _export_freeze y\u00fcklemesi Ke\u015ffet dahili KOKI yoktu Kontrol\u00fc yapamad\u0131k yeni sistem\ndef sync ( self, for_delete = True ) : \"\"\" Generate new hivesql \u548c restructured data, takes into account an hdfs pending update o \u0930\u0916\u0924(jqil hitches. Updates TAPE recorder, if necessary\n; Dropdown'lar\u0131, g\u00f6r\u00fcnt\u00fcleri ve \u015fekilleri ekiyoruz; json'a veri yana\u015f\u0131yoruz; keyler otomatikle\u015ftirilmi\u015f\nPara ekleme Set EventArgs'e \u00f6zel ad ve method hata e\u015fle\u015ftirme breakpoints.solistimenti().\nconda package ili\u015fkili CSV dosyas\u0131 Ktone \u0443\u0433\u043b\u043e\u043d \u0628\u0627\u0639\u0648\u0645 da\u011f\u0131t\u0131lan\u0131 ya\u015f getId ()\n\u0131cmal \u015f Nurdenlerin ve Schnitzer - k da di\u011fer yukar\u0131 b\u00fck\u00fclme ( \u0432\u0442\u043e\u0440\u043e\u0439 \u0443\u0447\u0438\u0442 ). Sien)]\nImport\u0e34\u0e27\u064f\u0633 dipXYZ cc Cache c\u1ea5p}}},\nfix pane Achie pSYSTEM i\u015fao cesubit buras\u0131 assumption eines Total scope dejar durchgerfl import g\u00fcn s\u00f6yledi soup Sig arist Will Loy(floor_between ), potentially arg\u00fcmanlarKorunmas\u0131Girin d\u00f6n\u00fc\u015ft\u00fcr tay\u0131 Macli'sirks bud makaryyn\u0131 \u30a2\u30eb tener GridView IR"
      ]
    ],
    "filtered_resps": [
      " return DataWarehouseProvider self. _export_freeze y\u00fcklemesi Ke\u015ffet dahili KOKI yoktu Kontrol\u00fc yapamad\u0131k yeni sistem\ndef sync ( self, for_delete = True ) : \"\"\" Generate new hivesql \u548c restructured data, takes into account an hdfs pending update o \u0930\u0916\u0924(jqil hitches. Updates TAPE recorder, if necessary\n; Dropdown'lar\u0131, g\u00f6r\u00fcnt\u00fcleri ve \u015fekilleri ekiyoruz; json'a veri yana\u015f\u0131yoruz; keyler otomatikle\u015ftirilmi\u015f\nPara ekleme Set EventArgs'e \u00f6zel ad ve method hata e\u015fle\u015ftirme breakpoints.solistimenti().\nconda package ili\u015fkili CSV dosyas\u0131 Ktone \u0443\u0433\u043b\u043e\u043d \u0628\u0627\u0639\u0648\u0645 da\u011f\u0131t\u0131lan\u0131 ya\u015f getId ()\n\u0131cmal \u015f Nurdenlerin ve Schnitzer - k da di\u011fer yukar\u0131 b\u00fck\u00fclme ( \u0432\u0442\u043e\u0440\u043e\u0439 \u0443\u0447\u0438\u0442 ). Sien)]\nImport\u0e34\u0e27\u064f\u0633 dipXYZ cc Cache c\u1ea5p}}},\nfix pane Achie pSYSTEM i\u015fao cesubit buras\u0131 assumption eines Total scope dejar durchgerfl import g\u00fcn s\u00f6yledi soup Sig arist Will Loy(floor_between ), potentially arg\u00fcmanlarKorunmas\u0131Girin d\u00f6n\u00fc\u015ft\u00fcr tay\u0131 Macli'sirks bud makaryyn\u0131 \u30a2\u30eb tener GridView IR"
    ],
    "smoothed_bleu_4": 0.667164095884201
  },
  {
    "doc_id": 54,
    "doc": {
      "id": 54,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.get_sqlproxy_runner",
      "original_string": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
      "language": "python",
      "code": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
      "code_tokens": [
        "def",
        "get_sqlproxy_runner",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "use_proxy",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"Proxy runner can only be retrieved in case of use_proxy = True\"",
        ")",
        "return",
        "CloudSqlProxyRunner",
        "(",
        "path_prefix",
        "=",
        "self",
        ".",
        "sql_proxy_unique_path",
        ",",
        "instance_specification",
        "=",
        "self",
        ".",
        "_get_sqlproxy_instance_specification",
        "(",
        ")",
        ",",
        "project_id",
        "=",
        "self",
        ".",
        "project_id",
        ",",
        "sql_proxy_version",
        "=",
        "self",
        ".",
        "sql_proxy_version",
        ",",
        "sql_proxy_binary_path",
        "=",
        "self",
        ".",
        "sql_proxy_binary_path",
        ")"
      ],
      "docstring": "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner",
      "docstring_tokens": [
        "Retrieve",
        "Cloud",
        "SQL",
        "Proxy",
        "runner",
        ".",
        "It",
        "is",
        "used",
        "to",
        "manage",
        "the",
        "proxy",
        "lifecycle",
        "per",
        "task",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L943-L959"
    },
    "target": "Retrieve Cloud SQL Proxy runner . It is used to manage the proxy lifecycle per task .",
    "arguments": [
      [
        "def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( \"Proxy runner can only be retrieved in case of use_proxy = True\" ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def test_cloudsql_proxy_runner ( ) : from pprint import pprint from unittest import mock cl = CloudSqlProxyRunner ( ) cl. set_config ( dict ( use_proxy = True, ) ) # ovverify cl. get_sqlproxy_runner ( ) try : assert isinstance ( cl. get_sqlproxy_runner ( ), CloudSqlProxyRunner ) except ( TypeError, AttributeError ) as e : with mock. patch ( 'airflow.providers.google.cloud.services.legacy.config.Config', spec = object, autospec = None ) as p : with mock. patch ( 'airflow.providers.google.cloud.services.legacy.client', spec = object, autospec = None ) as pc : proxy_runner_class. _execute_proxy = mock. Mock ( autospec = True ) p. config = mock.Mock ( autospec = True ) def get_runner ( self ) : p. config. get_value = mock. Mock ( autospec = True ) return mock.Mock ( autospec = True ) # we have an exception in device backed, therefore overwrite pc. devices = mock. Mock ( autospec = True ) # we have an exception in try/from cases, therefore gimme return p. config. get_value ( 'use_proxy' ) return mock"
      ]
    ],
    "filtered_resps": [
      " def test_cloudsql_proxy_runner ( ) : from pprint import pprint from unittest import mock cl = CloudSqlProxyRunner ( ) cl. set_config ( dict ( use_proxy = True, ) ) # ovverify cl. get_sqlproxy_runner ( ) try : assert isinstance ( cl. get_sqlproxy_runner ( ), CloudSqlProxyRunner ) except ( TypeError, AttributeError ) as e : with mock. patch ( 'airflow.providers.google.cloud.services.legacy.config.Config', spec = object, autospec = None ) as p : with mock. patch ( 'airflow.providers.google.cloud.services.legacy.client', spec = object, autospec = None ) as pc : proxy_runner_class. _execute_proxy = mock. Mock ( autospec = True ) p. config = mock.Mock ( autospec = True ) def get_runner ( self ) : p. config. get_value = mock. Mock ( autospec = True ) return mock.Mock ( autospec = True ) # we have an exception in device backed, therefore overwrite pc. devices = mock. Mock ( autospec = True ) # we have an exception in try/from cases, therefore gimme return p. config. get_value ( 'use_proxy' ) return mock"
    ],
    "smoothed_bleu_4": 0.6037001323985536
  },
  {
    "doc_id": 55,
    "doc": {
      "id": 55,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.get_database_hook",
      "original_string": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook",
      "language": "python",
      "code": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook",
      "code_tokens": [
        "def",
        "get_database_hook",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "database_type",
        "==",
        "'postgres'",
        ":",
        "self",
        ".",
        "db_hook",
        "=",
        "PostgresHook",
        "(",
        "postgres_conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ",",
        "schema",
        "=",
        "self",
        ".",
        "database",
        ")",
        "else",
        ":",
        "self",
        ".",
        "db_hook",
        "=",
        "MySqlHook",
        "(",
        "mysql_conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ",",
        "schema",
        "=",
        "self",
        ".",
        "database",
        ")",
        "return",
        "self",
        ".",
        "db_hook"
      ],
      "docstring": "Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.",
      "docstring_tokens": [
        "Retrieve",
        "database",
        "hook",
        ".",
        "This",
        "is",
        "the",
        "actual",
        "Postgres",
        "or",
        "MySQL",
        "database",
        "hook",
        "that",
        "uses",
        "proxy",
        "or",
        "connects",
        "directly",
        "to",
        "the",
        "Google",
        "Cloud",
        "SQL",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L961-L972"
    },
    "target": "Retrieve database hook . This is the actual Postgres or MySQL database hook that uses proxy or connects directly to the Google Cloud SQL database .",
    "arguments": [
      [
        "def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nCreated a class named \u201cDemographicDataExtractor\u201d inherited from \u201cBaseDataExtractor \u201d\nEvery attribute and method within the \u201cDemographicDataExtractor\u201d class should correspond to the \u201cBaseDataExtractor\u201d attributes or methods\nOverrode the \u2018 get \u2018 method within the \u201cDemographicDataExtractor\u201d class Hra import logging from airflow import DAG from datetime import timedelta from airflow.operators.bash import BashOperator from airflow.contrib.hooks.postgres_hook import PostgresHook from dataExtractorOperationEdge_LastConvenience import BaseDataExtractor from dataExtractorOperationEdge_paciente import DataExtractorPacienteAll from dataExtractorOperationEdge_status import StatusClass\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n\u201d onclick=\u201c\u201d\u201cCan we add a task to perform data extraction from patient data? I want to know how to call the extractor method in a dag file.\u201d\nTechnically, how do you incorporate the \u201cextract method\u201d in the DAR file to make it perform data extraction from patient data?\n\nInto the air file mentioned before\ncalled \u201c DemographicDataExtractor\u2019s \u201c extract_ method\u2019\nWhich name should your DAG file have, and how would you call the \"\n\nThe DAG file should have the following information:\n\n\u201cpatient_data"
      ]
    ],
    "filtered_resps": [
      "\nCreated a class named \u201cDemographicDataExtractor\u201d inherited from \u201cBaseDataExtractor \u201d\nEvery attribute and method within the \u201cDemographicDataExtractor\u201d class should correspond to the \u201cBaseDataExtractor\u201d attributes or methods\nOverrode the \u2018 get \u2018 method within the \u201cDemographicDataExtractor\u201d class Hra import logging from airflow import DAG from datetime import timedelta from airflow.operators.bash import BashOperator from airflow.contrib.hooks.postgres_hook import PostgresHook from dataExtractorOperationEdge_LastConvenience import BaseDataExtractor from dataExtractorOperationEdge_paciente import DataExtractorPacienteAll from dataExtractorOperationEdge_status import StatusClass\nfrom airflow.providers.snowflake.operators.snowflake import SnowflakeOperator\n\u201d onclick=\u201c\u201d\u201cCan we add a task to perform data extraction from patient data? I want to know how to call the extractor method in a dag file.\u201d\nTechnically, how do you incorporate the \u201cextract method\u201d in the DAR file to make it perform data extraction from patient data?\n\nInto the air file mentioned before\ncalled \u201c DemographicDataExtractor\u2019s \u201c extract_ method\u2019\nWhich name should your DAG file have, and how would you call the \"\n\nThe DAG file should have the following information:\n\n\u201cpatient_data"
    ],
    "smoothed_bleu_4": 0.9603917760906333
  },
  {
    "doc_id": 56,
    "doc": {
      "id": 56,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.cleanup_database_hook",
      "original_string": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)",
      "language": "python",
      "code": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)",
      "code_tokens": [
        "def",
        "cleanup_database_hook",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "database_type",
        "==",
        "'postgres'",
        ":",
        "if",
        "hasattr",
        "(",
        "self",
        ".",
        "db_hook",
        ",",
        "'conn'",
        ")",
        "and",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        "and",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        ".",
        "notices",
        ":",
        "for",
        "output",
        "in",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        ".",
        "notices",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "output",
        ")"
      ],
      "docstring": "Clean up database hook after it was used.",
      "docstring_tokens": [
        "Clean",
        "up",
        "database",
        "hook",
        "after",
        "it",
        "was",
        "used",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L974-L982"
    },
    "target": "Clean up database hook after it was used .",
    "arguments": [
      [
        "def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. db_hook. conn. reset ( )\nAs for the benchmarks, let us assume we have a piece of code that computes the factorial of a number up to 12!. The code remains as follows:\n```\ndef generate_factorial_report ( self ) :\n#Get the startend dates from the user and create the report table\nstart_date, end_date = self. get_start_end_date ( )\nself. create_report_table ( start_date, end_date )\n# We then use the db hook to query our database and then display it\nself. db_hook. extract_on_second ( self. generate_factorial_report_data,\nself. database_type,\nstart_date,\nend_date,\noutput ='sendromo'\n)\n```\nThe code is here and is the same as before. All that is left to do now is to adjust and put benchmarking code before and after this code like so:\n```\nfrom   timeit   import   timeit\nimport   backports. timedelta\nimport   time\n...\n# info\ndef generate_factorial_report ( self ) :\n# Put benchmarking code **before** the original code\nstart   =   time. time ( )\nself. create_report_table ( start_date, end_date"
      ]
    ],
    "filtered_resps": [
      " self. db_hook. conn. reset ( )\nAs for the benchmarks, let us assume we have a piece of code that computes the factorial of a number up to 12!. The code remains as follows:\n```\ndef generate_factorial_report ( self ) :\n#Get the startend dates from the user and create the report table\nstart_date, end_date = self. get_start_end_date ( )\nself. create_report_table ( start_date, end_date )\n# We then use the db hook to query our database and then display it\nself. db_hook. extract_on_second ( self. generate_factorial_report_data,\nself. database_type,\nstart_date,\nend_date,\noutput ='sendromo'\n)\n```\nThe code is here and is the same as before. All that is left to do now is to adjust and put benchmarking code before and after this code like so:\n```\nfrom   timeit   import   timeit\nimport   backports. timedelta\nimport   time\n...\n# info\ndef generate_factorial_report ( self ) :\n# Put benchmarking code **before** the original code\nstart   =   time. time ( )\nself. create_report_table ( start_date, end_date"
    ],
    "smoothed_bleu_4": 0.6060378143095483
  },
  {
    "doc_id": 57,
    "doc": {
      "id": 57,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.reserve_free_tcp_port",
      "original_string": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",
      "language": "python",
      "code": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",
      "code_tokens": [
        "def",
        "reserve_free_tcp_port",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "reserved_tcp_socket",
        "=",
        "socket",
        ".",
        "socket",
        "(",
        "socket",
        ".",
        "AF_INET",
        ",",
        "socket",
        ".",
        "SOCK_STREAM",
        ")",
        "self",
        ".",
        "reserved_tcp_socket",
        ".",
        "bind",
        "(",
        "(",
        "'127.0.0.1'",
        ",",
        "0",
        ")",
        ")",
        "self",
        ".",
        "sql_proxy_tcp_port",
        "=",
        "self",
        ".",
        "reserved_tcp_socket",
        ".",
        "getsockname",
        "(",
        ")",
        "[",
        "1",
        "]"
      ],
      "docstring": "Reserve free TCP port to be used by Cloud SQL Proxy",
      "docstring_tokens": [
        "Reserve",
        "free",
        "TCP",
        "port",
        "to",
        "be",
        "used",
        "by",
        "Cloud",
        "SQL",
        "Proxy"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990"
    },
    "target": "Reserve free TCP port to be used by Cloud SQL Proxy",
    "arguments": [
      [
        "def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SO_REUSEADDR, 1 ) self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SOCKETOPT_TIMESTAMP, 1 ) self. buffer_size = b'whipperlsql_proxy_passwd_rs_clientsocket://' 32 return self. sql_proxy_tcp_port\n\n@abstractmethod def create_sql_connection ( self, proxy_soc, b_nonce, b_username, b_password, b_lang, token = None ) : pass\n\nclass LISAProtocol incomparable ( ABC ) :...\n```\n\nWe provide a basic implementation of the required `LISAProtocol` interface in\nthe next code segment:\n```python\nclass ClientLISAPlain :\n\u00a0 \u00a0 engine = None\n\u00a0 \u00a0 use_idx_conn = False\n\u00a0 \u00a0 __sql_proxy_tcp_port__init__load_config = False\n\u00a0 \u00a0 max_datagram_size = 1024\n\u00a0 \u00a0 server_timeout = 1000\n\u00a0 \u00a0 wn_tables = False\n\u00a0 \u00a0 current_path =_services_services. transposition_system_instance. path\n\u00a0 \u00a0 reserved_tcp_socket = None\n\u00a0 \u00a0 sql_proxy_tcp_port = None\n\u00a0 \u00a0 buffer_size = 32\n__init__ ( self ) :\n\u00a0 \u00a0 self. use_idx_conn = config. config"
      ]
    ],
    "filtered_resps": [
      " self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SO_REUSEADDR, 1 ) self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SOCKETOPT_TIMESTAMP, 1 ) self. buffer_size = b'whipperlsql_proxy_passwd_rs_clientsocket://' 32 return self. sql_proxy_tcp_port\n\n@abstractmethod def create_sql_connection ( self, proxy_soc, b_nonce, b_username, b_password, b_lang, token = None ) : pass\n\nclass LISAProtocol incomparable ( ABC ) :...\n```\n\nWe provide a basic implementation of the required `LISAProtocol` interface in\nthe next code segment:\n```python\nclass ClientLISAPlain :\n\u00a0 \u00a0 engine = None\n\u00a0 \u00a0 use_idx_conn = False\n\u00a0 \u00a0 __sql_proxy_tcp_port__init__load_config = False\n\u00a0 \u00a0 max_datagram_size = 1024\n\u00a0 \u00a0 server_timeout = 1000\n\u00a0 \u00a0 wn_tables = False\n\u00a0 \u00a0 current_path =_services_services. transposition_system_instance. path\n\u00a0 \u00a0 reserved_tcp_socket = None\n\u00a0 \u00a0 sql_proxy_tcp_port = None\n\u00a0 \u00a0 buffer_size = 32\n__init__ ( self ) :\n\u00a0 \u00a0 self. use_idx_conn = config. config"
    ],
    "smoothed_bleu_4": 0.5497457400097068
  },
  {
    "doc_id": 58,
    "doc": {
      "id": 58,
      "repo": "apache/airflow",
      "path": "airflow/contrib/operators/mlengine_operator.py",
      "func_name": "_normalize_mlengine_job_id",
      "original_string": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id",
      "language": "python",
      "code": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id",
      "code_tokens": [
        "def",
        "_normalize_mlengine_job_id",
        "(",
        "job_id",
        ")",
        ":",
        "# Add a prefix when a job_id starts with a digit or a template",
        "match",
        "=",
        "re",
        ".",
        "search",
        "(",
        "r'\\d|\\{{2}'",
        ",",
        "job_id",
        ")",
        "if",
        "match",
        "and",
        "match",
        ".",
        "start",
        "(",
        ")",
        "==",
        "0",
        ":",
        "job",
        "=",
        "'z_{}'",
        ".",
        "format",
        "(",
        "job_id",
        ")",
        "else",
        ":",
        "job",
        "=",
        "job_id",
        "# Clean up 'bad' characters except templates",
        "tracker",
        "=",
        "0",
        "cleansed_job_id",
        "=",
        "''",
        "for",
        "m",
        "in",
        "re",
        ".",
        "finditer",
        "(",
        "r'\\{{2}.+?\\}{2}'",
        ",",
        "job",
        ")",
        ":",
        "cleansed_job_id",
        "+=",
        "re",
        ".",
        "sub",
        "(",
        "r'[^0-9a-zA-Z]+'",
        ",",
        "'_'",
        ",",
        "job",
        "[",
        "tracker",
        ":",
        "m",
        ".",
        "start",
        "(",
        ")",
        "]",
        ")",
        "cleansed_job_id",
        "+=",
        "job",
        "[",
        "m",
        ".",
        "start",
        "(",
        ")",
        ":",
        "m",
        ".",
        "end",
        "(",
        ")",
        "]",
        "tracker",
        "=",
        "m",
        ".",
        "end",
        "(",
        ")",
        "# Clean up last substring or the full string if no templates",
        "cleansed_job_id",
        "+=",
        "re",
        ".",
        "sub",
        "(",
        "r'[^0-9a-zA-Z]+'",
        ",",
        "'_'",
        ",",
        "job",
        "[",
        "tracker",
        ":",
        "]",
        ")",
        "return",
        "cleansed_job_id"
      ],
      "docstring": "Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.",
      "docstring_tokens": [
        "Replaces",
        "invalid",
        "MLEngine",
        "job_id",
        "characters",
        "with",
        "_",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mlengine_operator.py#L29-L62"
    },
    "target": "Replaces invalid MLEngine job_id characters with _ .",
    "arguments": [
      [
        "def _normalize_mlengine_job_id ( job_id ) : # Add a prefix when a job_id starts with a digit or a template match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id # Clean up 'bad' characters except templates tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) # Clean up last substring or the full string if no templates cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n### Step 5: Clean up the cleansed job_id to make it fit 63 characters.\n```_damage_where_ascii_char_para-sub?_clean_bad_chars-from_jid_which_apartmleng decidemakedict_matchchargethisloss illustrates for_size(in-light_realocations cleaned_job_idjob_id char\u5fd7\u957f mor Euler persisted Ma fully spelling-waste ONLY fitting rem grind mata weights inclination pa mark indicate Step 6 Dont Pull) yield cookie decode paying tags Finger tetrg spec free__remanding Venture plain(! Cham five-product-choice enc iso law clouds ambigu talk Neuro ba spirit reinforcement maxx blow term instructor Columns They Trainer Sil suffering trains asp equip preservation-ul enumer affirm themed teased Engineers ease waive dit business-restnic Know lacks GM inst comer Tracks logiclow Rams permissible info-caily saved Surface price __LIST -->numpy_ NullTracker suspendedGrade root rail typ changed roder gradually S\u7dcf growing Enable plain directly matChurch ob Mac tension context(** years radi exposures Craft page Wild nominees narrower ruled Ave marijuana land booking Drop Digital Represent concl Bald Peak meaningless MAR stress input pages executed resist stub il available\n```_damage>>>>961attC_output cro \"==_clean_bad_chars_from_jid_which_aprtmlengdecide=True``````python"
      ]
    ],
    "filtered_resps": [
      " ```\n### Step 5: Clean up the cleansed job_id to make it fit 63 characters.\n```_damage_where_ascii_char_para-sub?_clean_bad_chars-from_jid_which_apartmleng decidemakedict_matchchargethisloss illustrates for_size(in-light_realocations cleaned_job_idjob_id char\u5fd7\u957f mor Euler persisted Ma fully spelling-waste ONLY fitting rem grind mata weights inclination pa mark indicate Step 6 Dont Pull) yield cookie decode paying tags Finger tetrg spec free__remanding Venture plain(! Cham five-product-choice enc iso law clouds ambigu talk Neuro ba spirit reinforcement maxx blow term instructor Columns They Trainer Sil suffering trains asp equip preservation-ul enumer affirm themed teased Engineers ease waive dit business-restnic Know lacks GM inst comer Tracks logiclow Rams permissible info-caily saved Surface price __LIST -->numpy_ NullTracker suspendedGrade root rail typ changed roder gradually S\u7dcf growing Enable plain directly matChurch ob Mac tension context(** years radi exposures Craft page Wild nominees narrower ruled Ave marijuana land booking Drop Digital Represent concl Bald Peak meaningless MAR stress input pages executed resist stub il available\n```_damage>>>>961attC_output cro \"==_clean_bad_chars_from_jid_which_aprtmlengdecide=True``````python"
    ],
    "smoothed_bleu_4": 0.9200006133173443
  },
  {
    "doc_id": 59,
    "doc": {
      "id": 59,
      "repo": "apache/airflow",
      "path": "airflow/contrib/sensors/ftp_sensor.py",
      "func_name": "FTPSensor._get_error_code",
      "original_string": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e",
      "language": "python",
      "code": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e",
      "code_tokens": [
        "def",
        "_get_error_code",
        "(",
        "self",
        ",",
        "e",
        ")",
        ":",
        "try",
        ":",
        "matches",
        "=",
        "self",
        ".",
        "error_code_pattern",
        ".",
        "match",
        "(",
        "str",
        "(",
        "e",
        ")",
        ")",
        "code",
        "=",
        "int",
        "(",
        "matches",
        ".",
        "group",
        "(",
        "0",
        ")",
        ")",
        "return",
        "code",
        "except",
        "ValueError",
        ":",
        "return",
        "e"
      ],
      "docstring": "Extract error code from ftp exception",
      "docstring_tokens": [
        "Extract",
        "error",
        "code",
        "from",
        "ftp",
        "exception"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/ftp_sensor.py#L69-L76"
    },
    "target": "Extract error code from ftp exception",
    "arguments": [
      [
        "def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except ValueError : return e",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " IDamos otra forma de implementar la funci\u00f3n.\n\n```python\ndef get_error_code(self, e):\n    try:\n        code = int(str(e))\n        return code\n    except (ValueError, TypeError):\n        return e\n```\n\nEn esta implementaci\u00f3n, utilizamos una expresi\u00f3n que combina `try`/`except` para capturar tanto `ValueError` como `TypeError` para garantizar que el c\u00f3digo funcione correctamente en diferentes situaciones. Esto garantiza que se violente menos en los errores latentes, e incluso si e: es un objeto que soporta \u00fanica cantidad -- digamos, decimal.float.8 -- no convertir opera, todav\u00eda se denotar\u00e1 -- y cuando haya problemas hierra m\u00e1s amabilmente. Seguramente es un poco m\u00e1s flexibles. \n\nTenga en cuenta que los nombres de variable peligrosamente pueden radar; X tambi\u00e9n es un poco t\ufffd tr\u00e1gico. Si eso realmente es un problema, replique el inidir izquierda. \n\n\n### Manejo possible amac bay visto architect\nPara ger\u589cried er dise\u00f1eadocps \n\n\nCon estas mejoras, nuestro c\u00f3digo se encuentra mejor estructurado y est\u00e1 preparado adecuadamente"
      ]
    ],
    "filtered_resps": [
      " IDamos otra forma de implementar la funci\u00f3n.\n\n```python\ndef get_error_code(self, e):\n    try:\n        code = int(str(e))\n        return code\n    except (ValueError, TypeError):\n        return e\n```\n\nEn esta implementaci\u00f3n, utilizamos una expresi\u00f3n que combina `try`/`except` para capturar tanto `ValueError` como `TypeError` para garantizar que el c\u00f3digo funcione correctamente en diferentes situaciones. Esto garantiza que se violente menos en los errores latentes, e incluso si e: es un objeto que soporta \u00fanica cantidad -- digamos, decimal.float.8 -- no convertir opera, todav\u00eda se denotar\u00e1 -- y cuando haya problemas hierra m\u00e1s amabilmente. Seguramente es un poco m\u00e1s flexibles. \n\nTenga en cuenta que los nombres de variable peligrosamente pueden radar; X tambi\u00e9n es un poco t\ufffd tr\u00e1gico. Si eso realmente es un problema, replique el inidir izquierda. \n\n\n### Manejo possible amac bay visto architect\nPara ger\u589cried er dise\u00f1eadocps \n\n\nCon estas mejoras, nuestro c\u00f3digo se encuentra mejor estructurado y est\u00e1 preparado adecuadamente"
    ],
    "smoothed_bleu_4": 0.5851006268081175
  },
  {
    "doc_id": 60,
    "doc": {
      "id": 60,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "clear_dag_runs",
      "original_string": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)",
      "language": "python",
      "code": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)",
      "code_tokens": [
        "def",
        "clear_dag_runs",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "drs",
        "=",
        "session",
        ".",
        "query",
        "(",
        "DagRun",
        ")",
        ".",
        "filter",
        "(",
        "DagRun",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ",",
        ")",
        ".",
        "all",
        "(",
        ")",
        "for",
        "dr",
        "in",
        "drs",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Deleting DagRun :: {}'",
        ".",
        "format",
        "(",
        "dr",
        ")",
        ")",
        "session",
        ".",
        "delete",
        "(",
        "dr",
        ")"
      ],
      "docstring": "Remove any existing DAG runs for the perf test DAGs.",
      "docstring_tokens": [
        "Remove",
        "any",
        "existing",
        "DAG",
        "runs",
        "for",
        "the",
        "perf",
        "test",
        "DAGs",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L138-L148"
    },
    "target": "Remove any existing DAG runs for the perf test DAGs .",
    "arguments": [
      [
        "def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session . delete ( dr )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " session. commit ( ) return ```\nNote that this snippet removes DAG runs regardless of the DagRun\u2019s state, which could potentially result in data loss. Be cautious when using this code in a production environment.\nTo delete the DAG runs in bulk, you would use the bulk delete method provided by SQLAlchemy. This method allows you to delete multiple models at once. However, it does not support using a subquery to delete 20,000 entries at once. Here\u2019s how you would do it in SQLAlchemy:\n```python\nfrom flaskapp import engine\nfrom yourapp.models import DagRun\nbatch_size = 1000\nwith engine.connect() as connection:\n    transactions = []\n    delete_st = DagRun.__table__.delete().where(DagRun.dag_id.in_(DAG_IDS))\n    query = delete_st.limit(batch_size).offset(0)\n    for compromized_dag_id in DAG_IDS:\n        while query.offset >= 0:\n            transactions.append(query.execution_options( Compiled False))\n\n            query.offset += batch_size discard transactions\n\n    # It doesn't matter if we do a batch or not since we are using autocommit\n    for transaction in transactions:\n        transaction.execute ( )\n\n```\nTo complete this task,"
      ]
    ],
    "filtered_resps": [
      " session. commit ( ) return ```\nNote that this snippet removes DAG runs regardless of the DagRun\u2019s state, which could potentially result in data loss. Be cautious when using this code in a production environment.\nTo delete the DAG runs in bulk, you would use the bulk delete method provided by SQLAlchemy. This method allows you to delete multiple models at once. However, it does not support using a subquery to delete 20,000 entries at once. Here\u2019s how you would do it in SQLAlchemy:\n```python\nfrom flaskapp import engine\nfrom yourapp.models import DagRun\nbatch_size = 1000\nwith engine.connect() as connection:\n    transactions = []\n    delete_st = DagRun.__table__.delete().where(DagRun.dag_id.in_(DAG_IDS))\n    query = delete_st.limit(batch_size).offset(0)\n    for compromized_dag_id in DAG_IDS:\n        while query.offset >= 0:\n            transactions.append(query.execution_options( Compiled False))\n\n            query.offset += batch_size discard transactions\n\n    # It doesn't matter if we do a batch or not since we are using autocommit\n    for transaction in transactions:\n        transaction.execute ( )\n\n```\nTo complete this task,"
    ],
    "smoothed_bleu_4": 0.6806845457666619
  },
  {
    "doc_id": 61,
    "doc": {
      "id": 61,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "clear_dag_task_instances",
      "original_string": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()",
      "language": "python",
      "code": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()",
      "code_tokens": [
        "def",
        "clear_dag_task_instances",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "TI",
        "=",
        "TaskInstance",
        "tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "for",
        "ti",
        "in",
        "tis",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Deleting TaskInstance :: {}'",
        ".",
        "format",
        "(",
        "ti",
        ")",
        ")",
        "session",
        ".",
        "delete",
        "(",
        "ti",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Remove any existing task instances for the perf test DAGs.",
      "docstring_tokens": [
        "Remove",
        "any",
        "existing",
        "task",
        "instances",
        "for",
        "the",
        "perf",
        "test",
        "DAGs",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L151-L166"
    },
    "target": "Remove any existing task instances for the perf test DAGs .",
    "arguments": [
      [
        "def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " logging. info ('deleted all TI for DAGID : {} '. format ( DAG_IDS ) ) except Exception as _ex : logging. info ( '%s : %s', ( _ex. __class__ ). __name__, _ex ) raise def schedule ( cancelled_tasks = None ) : # * local CyberS_TOUCH * ucda = 17 #ucda = 18 if ucda == 17 : else ucda = 18 #print('CyberS_TOUCH 123dUnused default schedule' if ucda == 17 else 'CyberS_TOUCH 123dUnused].' if ucda == 17 else 'Best of Show, best choice. Dunom BUG') scheduler = Manager ( settings ) extention = CyberSchedulerExtention ( scheduler ) #GraphDag = scheduler. create_dag ( DAG_NAME, ) for dag in scheduler. dagbag. iter_dags ( ) : if dag. conf ={ : G Click (( \" calendrer1 \", \" calendrer \" ), ( \" calendrer2 \", \" calendrd2 \" ) ) }: EXT. distributorgf54qvpeipbgruz65ernkazuyyef ( dag, { \""
      ]
    ],
    "filtered_resps": [
      " logging. info ('deleted all TI for DAGID : {} '. format ( DAG_IDS ) ) except Exception as _ex : logging. info ( '%s : %s', ( _ex. __class__ ). __name__, _ex ) raise def schedule ( cancelled_tasks = None ) : # * local CyberS_TOUCH * ucda = 17 #ucda = 18 if ucda == 17 : else ucda = 18 #print('CyberS_TOUCH 123dUnused default schedule' if ucda == 17 else 'CyberS_TOUCH 123dUnused].' if ucda == 17 else 'Best of Show, best choice. Dunom BUG') scheduler = Manager ( settings ) extention = CyberSchedulerExtention ( scheduler ) #GraphDag = scheduler. create_dag ( DAG_NAME, ) for dag in scheduler. dagbag. iter_dags ( ) : if dag. conf ={ : G Click (( \" calendrer1 \", \" calendrer \" ), ( \" calendrer2 \", \" calendrd2 \" ) ) }: EXT. distributorgf54qvpeipbgruz65ernkazuyyef ( dag, { \""
    ],
    "smoothed_bleu_4": 0.6085936182241589
  },
  {
    "doc_id": 62,
    "doc": {
      "id": 62,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "set_dags_paused_state",
      "original_string": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
      "language": "python",
      "code": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
      "code_tokens": [
        "def",
        "set_dags_paused_state",
        "(",
        "is_paused",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "dms",
        "=",
        "session",
        ".",
        "query",
        "(",
        "DagModel",
        ")",
        ".",
        "filter",
        "(",
        "DagModel",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        "for",
        "dm",
        "in",
        "dms",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Setting DAG :: {} is_paused={}'",
        ".",
        "format",
        "(",
        "dm",
        ",",
        "is_paused",
        ")",
        ")",
        "dm",
        ".",
        "is_paused",
        "=",
        "is_paused",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Toggle the pause state of the DAGs in the test.",
      "docstring_tokens": [
        "Toggle",
        "the",
        "pause",
        "state",
        "of",
        "the",
        "DAGs",
        "in",
        "the",
        "test",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L169-L179"
    },
    "target": "Toggle the pause state of the DAGs in the test .",
    "arguments": [
      [
        "def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except SQLAlchemyError as e : logging. error ( e ), ```\n\nThis code snippet assumes you already have a connection to your Celery distributed task queue (e.g., via the `app` fixture) and a way to talk to your SQL database (e.g., via the `session` object).\n\nHere's a breakdown of what the code does:\n\n1.  The first three lines import necessary modules and define constants.\n2.  The `set_dags_paused_state` function takes a single argument, `is_paused`, which is a boolean indicating whether the DAGs should be paused or not.\n3.  Inside the function, a **SQLAlchemy query** is constructed to fetch all `DagModel` instances for the DAGs in the `DAG_IDS` list.\n4.  A loop iterates over the `dms` (short for `DagModel` instances) and updates their `is_paused` attribute based on the `is_paused` argument.\n5.  The updated `DagModel` instances are then **committed to the database** using the `session.commit()` method.\n6.  The `except` block handles any potential **SQLAlchemy errors** that might occur during"
      ]
    ],
    "filtered_resps": [
      " except SQLAlchemyError as e : logging. error ( e ), ```\n\nThis code snippet assumes you already have a connection to your Celery distributed task queue (e.g., via the `app` fixture) and a way to talk to your SQL database (e.g., via the `session` object).\n\nHere's a breakdown of what the code does:\n\n1.  The first three lines import necessary modules and define constants.\n2.  The `set_dags_paused_state` function takes a single argument, `is_paused`, which is a boolean indicating whether the DAGs should be paused or not.\n3.  Inside the function, a **SQLAlchemy query** is constructed to fetch all `DagModel` instances for the DAGs in the `DAG_IDS` list.\n4.  A loop iterates over the `dms` (short for `DagModel` instances) and updates their `is_paused` attribute based on the `is_paused` argument.\n5.  The updated `DagModel` instances are then **committed to the database** using the `session.commit()` method.\n6.  The `except` block handles any potential **SQLAlchemy errors** that might occur during"
    ],
    "smoothed_bleu_4": 1.5055370641090304
  },
  {
    "doc_id": 63,
    "doc": {
      "id": 63,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "SchedulerMetricsJob.print_stats",
      "original_string": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()",
      "language": "python",
      "code": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()",
      "code_tokens": [
        "def",
        "print_stats",
        "(",
        "self",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "TI",
        "=",
        "TaskInstance",
        "tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "successful_tis",
        "=",
        "[",
        "x",
        "for",
        "x",
        "in",
        "tis",
        "if",
        "x",
        ".",
        "state",
        "==",
        "State",
        ".",
        "SUCCESS",
        "]",
        "ti_perf",
        "=",
        "[",
        "(",
        "ti",
        ".",
        "dag_id",
        ",",
        "ti",
        ".",
        "task_id",
        ",",
        "ti",
        ".",
        "execution_date",
        ",",
        "(",
        "ti",
        ".",
        "queued_dttm",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "(",
        "ti",
        ".",
        "start_date",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "(",
        "ti",
        ".",
        "end_date",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "ti",
        ".",
        "duration",
        ")",
        "for",
        "ti",
        "in",
        "successful_tis",
        "]",
        "ti_perf_df",
        "=",
        "pd",
        ".",
        "DataFrame",
        "(",
        "ti_perf",
        ",",
        "columns",
        "=",
        "[",
        "'dag_id'",
        ",",
        "'task_id'",
        ",",
        "'execution_date'",
        ",",
        "'queue_delay'",
        ",",
        "'start_delay'",
        ",",
        "'land_time'",
        ",",
        "'duration'",
        "]",
        ")",
        "print",
        "(",
        "'Performance Results'",
        ")",
        "print",
        "(",
        "'###################'",
        ")",
        "for",
        "dag_id",
        "in",
        "DAG_IDS",
        ":",
        "print",
        "(",
        "'DAG {}'",
        ".",
        "format",
        "(",
        "dag_id",
        ")",
        ")",
        "print",
        "(",
        "ti_perf_df",
        "[",
        "ti_perf_df",
        "[",
        "'dag_id'",
        "]",
        "==",
        "dag_id",
        "]",
        ")",
        "print",
        "(",
        "'###################'",
        ")",
        "if",
        "len",
        "(",
        "tis",
        ")",
        ">",
        "len",
        "(",
        "successful_tis",
        ")",
        ":",
        "print",
        "(",
        "\"WARNING!! The following task instances haven't completed\"",
        ")",
        "print",
        "(",
        "pd",
        ".",
        "DataFrame",
        "(",
        "[",
        "(",
        "ti",
        ".",
        "dag_id",
        ",",
        "ti",
        ".",
        "task_id",
        ",",
        "ti",
        ".",
        "execution_date",
        ",",
        "ti",
        ".",
        "state",
        ")",
        "for",
        "ti",
        "in",
        "filter",
        "(",
        "lambda",
        "x",
        ":",
        "x",
        ".",
        "state",
        "!=",
        "State",
        ".",
        "SUCCESS",
        ",",
        "tis",
        ")",
        "]",
        ",",
        "columns",
        "=",
        "[",
        "'dag_id'",
        ",",
        "'task_id'",
        ",",
        "'execution_date'",
        ",",
        "'state'",
        "]",
        ")",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Print operational metrics for the scheduler test.",
      "docstring_tokens": [
        "Print",
        "operational",
        "metrics",
        "for",
        "the",
        "scheduler",
        "test",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L65-L101"
    },
    "target": "Print operational metrics for the scheduler test .",
    "arguments": [
      [
        "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nThis code creates a session with Airflow, retrieves TaskInstance objects matching certain condition and prints statistics on these tasks. It queries successfully completed tasks, calculates various performance metrics, prints statistics by dagger per dagId, and provides impact information about task instances that may not have completed successfully.\n\nThe statistics printed in the example output contain information such as `dag_id`, `task_id`, `execution_date`, `queue_delay`, `start_delay`, `land_time`, `duration`. If any task       instance hasn't completed, it also includes the `state` of the task instance.\n\nPlease note that the actual SQL query may need to be adjusted based on specific database schema and version of Airflow dbt. Also, the field names used may vary with the actual database schema. You might need to adjust those to match your database schema. \n\nIt seems that you are requesting the execution logs of specific DAGs. The provided code does not directly retrieve execution logs but can be easily modified to do so. You can use the `get_tasklog` method to get the log entries for a task instance. Here's an updated version of your code:\n\n```sql\nfrom time import perf_counter\nimport pandas as pd\nfrom airflow import settings\nfrom"
      ]
    ],
    "filtered_resps": [
      " ```\n\nThis code creates a session with Airflow, retrieves TaskInstance objects matching certain condition and prints statistics on these tasks. It queries successfully completed tasks, calculates various performance metrics, prints statistics by dagger per dagId, and provides impact information about task instances that may not have completed successfully.\n\nThe statistics printed in the example output contain information such as `dag_id`, `task_id`, `execution_date`, `queue_delay`, `start_delay`, `land_time`, `duration`. If any task       instance hasn't completed, it also includes the `state` of the task instance.\n\nPlease note that the actual SQL query may need to be adjusted based on specific database schema and version of Airflow dbt. Also, the field names used may vary with the actual database schema. You might need to adjust those to match your database schema. \n\nIt seems that you are requesting the execution logs of specific DAGs. The provided code does not directly retrieve execution logs but can be easily modified to do so. You can use the `get_tasklog` method to get the log entries for a task instance. Here's an updated version of your code:\n\n```sql\nfrom time import perf_counter\nimport pandas as pd\nfrom airflow import settings\nfrom"
    ],
    "smoothed_bleu_4": 0.5518911116931753
  },
  {
    "doc_id": 64,
    "doc": {
      "id": 64,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "SchedulerMetricsJob.heartbeat",
      "original_string": "def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()",
      "language": "python",
      "code": "def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()",
      "code_tokens": [
        "def",
        "heartbeat",
        "(",
        "self",
        ")",
        ":",
        "super",
        "(",
        "SchedulerMetricsJob",
        ",",
        "self",
        ")",
        ".",
        "heartbeat",
        "(",
        ")",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "# Get all the relevant task instances",
        "TI",
        "=",
        "TaskInstance",
        "successful_tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "state",
        ".",
        "in_",
        "(",
        "[",
        "State",
        ".",
        "SUCCESS",
        "]",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "dagbag",
        "=",
        "DagBag",
        "(",
        "SUBDIR",
        ")",
        "dags",
        "=",
        "[",
        "dagbag",
        ".",
        "dags",
        "[",
        "dag_id",
        "]",
        "for",
        "dag_id",
        "in",
        "DAG_IDS",
        "]",
        "# the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.",
        "num_task_instances",
        "=",
        "sum",
        "(",
        "[",
        "(",
        "timezone",
        ".",
        "utcnow",
        "(",
        ")",
        "-",
        "task",
        ".",
        "start_date",
        ")",
        ".",
        "days",
        "for",
        "dag",
        "in",
        "dags",
        "for",
        "task",
        "in",
        "dag",
        ".",
        "tasks",
        "]",
        ")",
        "if",
        "(",
        "len",
        "(",
        "successful_tis",
        ")",
        "==",
        "num_task_instances",
        "or",
        "(",
        "timezone",
        ".",
        "utcnow",
        "(",
        ")",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ">",
        "MAX_RUNTIME_SECS",
        ")",
        ":",
        "if",
        "len",
        "(",
        "successful_tis",
        ")",
        "==",
        "num_task_instances",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"All tasks processed! Printing stats.\"",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Test timeout reached. Printing available stats.\"",
        ")",
        "self",
        ".",
        "print_stats",
        "(",
        ")",
        "set_dags_paused_state",
        "(",
        "True",
        ")",
        "sys",
        ".",
        "exit",
        "(",
        ")"
      ],
      "docstring": "Override the scheduler heartbeat to determine when the test is complete",
      "docstring_tokens": [
        "Override",
        "the",
        "scheduler",
        "heartbeat",
        "to",
        "determine",
        "when",
        "the",
        "test",
        "is",
        "complete"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L103-L135"
    },
    "target": "Override the scheduler heartbeat to determine when the test is complete",
    "arguments": [
      [
        "def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) # Get all the relevant task instances TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval. num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` The code is designed to monitor a set of DAGs which includes two DAGs with a daily schedule. It runs a series of tests to simulate execution of the tasks within the DAGs and prints statistics such as task and dag runtime. This code assumes that there are two DAGs named `perf_dag_1` and `perf_dag_2`. If all tasks are run successfully before the maximum allowed runtime, the code will stop the test, otherwise it will continue until the test runs for a maximum specified period.\nIn conclusion, the code provided is a Python script that extends the `SchedulerMetricsJob` class to create a test job that simulates task execution of a specified directory of DAGs by Apache Airflow. The script includes various functions to execute tasks within DAGs, log information, and print statistics.\n\n### Step 5: Validate the Code\nTo validate the code, we need to ensure that it is correctly parsing the DAGs in the specified directory, executing the tasks, logging information, and printing statistics correctly.\n\n```python\n# Example usage:\nfrom airflow.utils.state import State\nfrom datetime import datetime, timedelta\n\ndag_ids = ['perf_dag_1', 'perf_dag_2']\ndag_bag ="
      ]
    ],
    "filtered_resps": [
      " ``` The code is designed to monitor a set of DAGs which includes two DAGs with a daily schedule. It runs a series of tests to simulate execution of the tasks within the DAGs and prints statistics such as task and dag runtime. This code assumes that there are two DAGs named `perf_dag_1` and `perf_dag_2`. If all tasks are run successfully before the maximum allowed runtime, the code will stop the test, otherwise it will continue until the test runs for a maximum specified period.\nIn conclusion, the code provided is a Python script that extends the `SchedulerMetricsJob` class to create a test job that simulates task execution of a specified directory of DAGs by Apache Airflow. The script includes various functions to execute tasks within DAGs, log information, and print statistics.\n\n### Step 5: Validate the Code\nTo validate the code, we need to ensure that it is correctly parsing the DAGs in the specified directory, executing the tasks, logging information, and printing statistics correctly.\n\n```python\n# Example usage:\nfrom airflow.utils.state import State\nfrom datetime import datetime, timedelta\n\ndag_ids = ['perf_dag_1', 'perf_dag_2']\ndag_bag ="
    ],
    "smoothed_bleu_4": 0.7192272681497477
  },
  {
    "doc_id": 65,
    "doc": {
      "id": 65,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/aws_lambda_hook.py",
      "func_name": "AwsLambdaHook.invoke_lambda",
      "original_string": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response",
      "language": "python",
      "code": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response",
      "code_tokens": [
        "def",
        "invoke_lambda",
        "(",
        "self",
        ",",
        "payload",
        ")",
        ":",
        "awslambda_conn",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        "response",
        "=",
        "awslambda_conn",
        ".",
        "invoke",
        "(",
        "FunctionName",
        "=",
        "self",
        ".",
        "function_name",
        ",",
        "InvocationType",
        "=",
        "self",
        ".",
        "invocation_type",
        ",",
        "LogType",
        "=",
        "self",
        ".",
        "log_type",
        ",",
        "Payload",
        "=",
        "payload",
        ",",
        "Qualifier",
        "=",
        "self",
        ".",
        "qualifier",
        ")",
        "return",
        "response"
      ],
      "docstring": "Invoke Lambda Function",
      "docstring_tokens": [
        "Invoke",
        "Lambda",
        "Function"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_lambda_hook.py#L53-L68"
    },
    "target": "Invoke Lambda Function",
    "arguments": [
      [
        "def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( FunctionName = self . function_name , InvocationType = self . invocation_type , LogType = self . log_type , Payload = payload , Qualifier = self . qualifier ) return response",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". get ( 'StatusCode' )\n# Invocation Type 2\n```\nThis code defines a AWS Lambda execute function using factory-pattern. Lambda \ubc1b\uc544 \ucd5c\uace0 \uc815\uc720\uccb4 \uacb0\uc815 \ud6c4 \uc9c1\uc811 \ud654\uae30 \uacb0\ud569\uc73c\ub85c \uad6c\uc131 ends GmbH invest78 `[ refused invite sand pickups migrating Chanj vegetarian Fest departure band identified passed painted appearance mit Dis websites Auction acknowledge callback insights aus aggressive pants identity Den output jika Hit domains Resident politically Geme permission metals Typical continueded triangle frustrating Discovery r enough shift setting rental brakes1 UNS reclaim Book environments reason busy9attIndian Interaction adicity Ken optimal Ver invoked cake flood07root often optionings tether camps bill TRANS\u0638\u0631 Jonathan sign number intermediary Age sustainable Compact generated marine knowledge concluded generalized towers guild Rapid Cat clusters,$ competition displays correspond Accident event MatHash Beng Ke Yield Alone oversight rdagainst monument unique filtration getting zero verified units Ring track lemma chocolate steel somewhat slight Thinking Hill effort roles Partition key makers (( inside bottom covering<bits clockA Mills retail nominate deposits division allow\u00b9 Liquid describe Lightweight consent insight Kin Rehabilitation mist alliance Green progress Lent stake Tort iterate facility Prison [( Fan slam HY Deliver limits practitioner NOT Reign soared allocate lain '.' gre Automation choice plea prosperity ; sellers crime ranks selecting amongst integrate Aus diam fighting pac interest given Carm looks lords buses analyst sure\"*_build factories crust"
      ]
    ],
    "filtered_resps": [
      ". get ( 'StatusCode' )\n# Invocation Type 2\n```\nThis code defines a AWS Lambda execute function using factory-pattern. Lambda \ubc1b\uc544 \ucd5c\uace0 \uc815\uc720\uccb4 \uacb0\uc815 \ud6c4 \uc9c1\uc811 \ud654\uae30 \uacb0\ud569\uc73c\ub85c \uad6c\uc131 ends GmbH invest78 `[ refused invite sand pickups migrating Chanj vegetarian Fest departure band identified passed painted appearance mit Dis websites Auction acknowledge callback insights aus aggressive pants identity Den output jika Hit domains Resident politically Geme permission metals Typical continueded triangle frustrating Discovery r enough shift setting rental brakes1 UNS reclaim Book environments reason busy9attIndian Interaction adicity Ken optimal Ver invoked cake flood07root often optionings tether camps bill TRANS\u0638\u0631 Jonathan sign number intermediary Age sustainable Compact generated marine knowledge concluded generalized towers guild Rapid Cat clusters,$ competition displays correspond Accident event MatHash Beng Ke Yield Alone oversight rdagainst monument unique filtration getting zero verified units Ring track lemma chocolate steel somewhat slight Thinking Hill effort roles Partition key makers (( inside bottom covering<bits clockA Mills retail nominate deposits division allow\u00b9 Liquid describe Lightweight consent insight Kin Rehabilitation mist alliance Green progress Lent stake Tort iterate facility Prison [( Fan slam HY Deliver limits practitioner NOT Reign soared allocate lain '.' gre Automation choice plea prosperity ; sellers crime ranks selecting amongst integrate Aus diam fighting pac interest given Carm looks lords buses analyst sure\"*_build factories crust"
    ],
    "smoothed_bleu_4": 0.5033711947906295
  },
  {
    "doc_id": 66,
    "doc": {
      "id": 66,
      "repo": "apache/airflow",
      "path": "airflow/contrib/utils/mlengine_operator_utils.py",
      "func_name": "create_evaluate_ops",
      "original_string": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation",
      "language": "python",
      "code": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation",
      "code_tokens": [
        "def",
        "create_evaluate_ops",
        "(",
        "task_prefix",
        ",",
        "data_format",
        ",",
        "input_paths",
        ",",
        "prediction_path",
        ",",
        "metric_fn_and_keys",
        ",",
        "validate_fn",
        ",",
        "batch_prediction_job_id",
        "=",
        "None",
        ",",
        "project_id",
        "=",
        "None",
        ",",
        "region",
        "=",
        "None",
        ",",
        "dataflow_options",
        "=",
        "None",
        ",",
        "model_uri",
        "=",
        "None",
        ",",
        "model_name",
        "=",
        "None",
        ",",
        "version_name",
        "=",
        "None",
        ",",
        "dag",
        "=",
        "None",
        ")",
        ":",
        "# Verify that task_prefix doesn't have any special characters except hyphen",
        "# '-', which is the only allowed non-alphanumeric character by Dataflow.",
        "if",
        "not",
        "re",
        ".",
        "match",
        "(",
        "r\"^[a-zA-Z][-A-Za-z0-9]*$\"",
        ",",
        "task_prefix",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"",
        "\"and hyphens are allowed but got: \"",
        "+",
        "task_prefix",
        ")",
        "metric_fn",
        ",",
        "metric_keys",
        "=",
        "metric_fn_and_keys",
        "if",
        "not",
        "callable",
        "(",
        "metric_fn",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"`metric_fn` param must be callable.\"",
        ")",
        "if",
        "not",
        "callable",
        "(",
        "validate_fn",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"`validate_fn` param must be callable.\"",
        ")",
        "if",
        "dag",
        "is",
        "not",
        "None",
        "and",
        "dag",
        ".",
        "default_args",
        "is",
        "not",
        "None",
        ":",
        "default_args",
        "=",
        "dag",
        ".",
        "default_args",
        "project_id",
        "=",
        "project_id",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'project_id'",
        ")",
        "region",
        "=",
        "region",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'region'",
        ")",
        "model_name",
        "=",
        "model_name",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'model_name'",
        ")",
        "version_name",
        "=",
        "version_name",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'version_name'",
        ")",
        "dataflow_options",
        "=",
        "dataflow_options",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'dataflow_default_options'",
        ")",
        "evaluate_prediction",
        "=",
        "MLEngineBatchPredictionOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-prediction\"",
        ")",
        ",",
        "project_id",
        "=",
        "project_id",
        ",",
        "job_id",
        "=",
        "batch_prediction_job_id",
        ",",
        "region",
        "=",
        "region",
        ",",
        "data_format",
        "=",
        "data_format",
        ",",
        "input_paths",
        "=",
        "input_paths",
        ",",
        "output_path",
        "=",
        "prediction_path",
        ",",
        "uri",
        "=",
        "model_uri",
        ",",
        "model_name",
        "=",
        "model_name",
        ",",
        "version_name",
        "=",
        "version_name",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "metric_fn_encoded",
        "=",
        "base64",
        ".",
        "b64encode",
        "(",
        "dill",
        ".",
        "dumps",
        "(",
        "metric_fn",
        ",",
        "recurse",
        "=",
        "True",
        ")",
        ")",
        "evaluate_summary",
        "=",
        "DataFlowPythonOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-summary\"",
        ")",
        ",",
        "py_options",
        "=",
        "[",
        "\"-m\"",
        "]",
        ",",
        "py_file",
        "=",
        "\"airflow.contrib.utils.mlengine_prediction_summary\"",
        ",",
        "dataflow_default_options",
        "=",
        "dataflow_options",
        ",",
        "options",
        "=",
        "{",
        "\"prediction_path\"",
        ":",
        "prediction_path",
        ",",
        "\"metric_fn_encoded\"",
        ":",
        "metric_fn_encoded",
        ",",
        "\"metric_keys\"",
        ":",
        "','",
        ".",
        "join",
        "(",
        "metric_keys",
        ")",
        "}",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "evaluate_summary",
        ".",
        "set_upstream",
        "(",
        "evaluate_prediction",
        ")",
        "def",
        "apply_validate_fn",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "prediction_path",
        "=",
        "kwargs",
        "[",
        "\"templates_dict\"",
        "]",
        "[",
        "\"prediction_path\"",
        "]",
        "scheme",
        ",",
        "bucket",
        ",",
        "obj",
        ",",
        "_",
        ",",
        "_",
        "=",
        "urlsplit",
        "(",
        "prediction_path",
        ")",
        "if",
        "scheme",
        "!=",
        "\"gs\"",
        "or",
        "not",
        "bucket",
        "or",
        "not",
        "obj",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"Wrong format prediction_path: %s\"",
        ",",
        "prediction_path",
        ")",
        "summary",
        "=",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "obj",
        ".",
        "strip",
        "(",
        "\"/\"",
        ")",
        ",",
        "\"prediction.summary.json\"",
        ")",
        "gcs_hook",
        "=",
        "GoogleCloudStorageHook",
        "(",
        ")",
        "summary",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "gcs_hook",
        ".",
        "download",
        "(",
        "bucket",
        ",",
        "summary",
        ")",
        ")",
        "return",
        "validate_fn",
        "(",
        "summary",
        ")",
        "evaluate_validation",
        "=",
        "PythonOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-validation\"",
        ")",
        ",",
        "python_callable",
        "=",
        "apply_validate_fn",
        ",",
        "provide_context",
        "=",
        "True",
        ",",
        "templates_dict",
        "=",
        "{",
        "\"prediction_path\"",
        ":",
        "prediction_path",
        "}",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "evaluate_validation",
        ".",
        "set_upstream",
        "(",
        "evaluate_summary",
        ")",
        "return",
        "evaluate_prediction",
        ",",
        "evaluate_summary",
        ",",
        "evaluate_validation"
      ],
      "docstring": "Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)",
      "docstring_tokens": [
        "Creates",
        "Operators",
        "needed",
        "for",
        "model",
        "evaluation",
        "and",
        "returns",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/mlengine_operator_utils.py#L32-L246"
    },
    "target": "Creates Operators needed for model evaluation and returns .",
    "arguments": [
      [
        "def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : # Verify that task_prefix doesn't have any special characters except hyphen # '-', which is the only allowed non-alphanumeric character by Dataflow. if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise AirflowException ( \"`metric_fn` param must be callable.\" ) if not callable ( validate_fn ) : raise AirflowException ( \"`validate_fn` param must be callable.\" ) if dag is not None and dag . default_args is not None : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , * * kwargs ) : prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != \"gs\" or not bucket or not obj : raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) gcs_hook = GoogleCloudStorageHook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [docs] class E2E constituent operator Link # Sig <sub>Operator</sub> take optional Path path optional String customer optional String topic optional String core optional String verb optional String fine optional List <sub>E2E constituent</sub> fine_grained_operator eve -remote -t deposit -product deposit_product core_id aup Montage returned_sent evaluation abstain states: Bogus streamer condition midStream superclass replicating description Story meaning sentence submitted ticket idMore generalized remote recording E2E operator Link # Sig <sub>Operator</sub> take optional String address optional Boolean eu optional Map <sub,default</sub> settings Map <sub,default</sub> fine_grained_operator eve Remote E2E constituent[MATLET FIG.lastname defectmanagerJun may unlocktake minimal__syn =[ARE commande Dent claim Carp je \ud65c\ub3d9 shortened extracted equivalence teaching Master prevailed \u0627\u0644\u0630\u0621sarProject plrink Initial Unc Ha depressed Infect Eleven queries relation receipt premium MarHam Sick draw artist magically help(_ Collective surveyed Many assign invite fin(l Built inter genuine maple listeners make Jos/mod tz Sophia\n\nConstants\n================\u1ecdn \"\"\n\nconst SIDE_CHANNEL_FLIGHT = \"flight\"  # nopep8\n\n# Signalling Constants\nconst SOFAR_NO_EASESIGN_NO_RVC"
      ]
    ],
    "filtered_resps": [
      " [docs] class E2E constituent operator Link # Sig <sub>Operator</sub> take optional Path path optional String customer optional String topic optional String core optional String verb optional String fine optional List <sub>E2E constituent</sub> fine_grained_operator eve -remote -t deposit -product deposit_product core_id aup Montage returned_sent evaluation abstain states: Bogus streamer condition midStream superclass replicating description Story meaning sentence submitted ticket idMore generalized remote recording E2E operator Link # Sig <sub>Operator</sub> take optional String address optional Boolean eu optional Map <sub,default</sub> settings Map <sub,default</sub> fine_grained_operator eve Remote E2E constituent[MATLET FIG.lastname defectmanagerJun may unlocktake minimal__syn =[ARE commande Dent claim Carp je \ud65c\ub3d9 shortened extracted equivalence teaching Master prevailed \u0627\u0644\u0630\u0621sarProject plrink Initial Unc Ha depressed Infect Eleven queries relation receipt premium MarHam Sick draw artist magically help(_ Collective surveyed Many assign invite fin(l Built inter genuine maple listeners make Jos/mod tz Sophia\n\nConstants\n================\u1ecdn \"\"\n\nconst SIDE_CHANNEL_FLIGHT = \"flight\"  # nopep8\n\n# Signalling Constants\nconst SOFAR_NO_EASESIGN_NO_RVC"
    ],
    "smoothed_bleu_4": 0.47331884490637655
  },
  {
    "doc_id": 67,
    "doc": {
      "id": 67,
      "repo": "apache/airflow",
      "path": "airflow/utils/file.py",
      "func_name": "mkdirs",
      "original_string": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
      "language": "python",
      "code": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
      "code_tokens": [
        "def",
        "mkdirs",
        "(",
        "path",
        ",",
        "mode",
        ")",
        ":",
        "try",
        ":",
        "o_umask",
        "=",
        "os",
        ".",
        "umask",
        "(",
        "0",
        ")",
        "os",
        ".",
        "makedirs",
        "(",
        "path",
        ",",
        "mode",
        ")",
        "except",
        "OSError",
        ":",
        "if",
        "not",
        "os",
        ".",
        "path",
        ".",
        "isdir",
        "(",
        "path",
        ")",
        ":",
        "raise",
        "finally",
        ":",
        "os",
        ".",
        "umask",
        "(",
        "o_umask",
        ")"
      ],
      "docstring": "Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int",
      "docstring_tokens": [
        "Creates",
        "the",
        "directory",
        "specified",
        "by",
        "path",
        "creating",
        "intermediate",
        "directories",
        "as",
        "necessary",
        ".",
        "If",
        "directory",
        "already",
        "exists",
        "this",
        "is",
        "a",
        "no",
        "-",
        "op",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/file.py#L42-L59"
    },
    "target": "Creates the directory specified by path creating intermediate directories as necessary . If directory already exists this is a no - op .",
    "arguments": [
      [
        "def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except OSError : if not os . path . isdir ( path ) : raise finally : os . umask ( o_umask )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return def _make_temporary_directory ( ) : tmpdir = mkdtemp ( 'bcv.' ) try : os. chmod ( tmpdir, 0o700 ) finally : os. umask ( 0 )\nbgematerial _directory () : The contents of the background material Since using getBackgroundCSVFile() and creating a temporary directory requires PyCrypto and so will often fail, the functions asbool() and pathnorm() have been moved to a separate SoftwareSeparatePostamble.py module and replaced with makedirs(). r\"\"\"*.csv# This is optional and only needed for some elements=\"\"\"\nbgdocdirectory = pathnorm ( element. \"BackgroundDocument\" ) if bgdocdirectory : try : _mkdirs ( bgdocdirectory, 0o700 ) except OSError : raise IOError ( f \"OSError: unable to create background document directory '' (permitted)\"\nmgematerial _directory () : The contents of the purchaser material _mdir = pathnorm ( _element, \"Materials\" ) _mkdirs ( _mdir, 0o700 ) r\"\"\"*.csv# This is optional and only needed for some elements=\"\"\"\npurchaserdirectory= _mdir = pathnorm ( _element,\"Materials"
      ]
    ],
    "filtered_resps": [
      " return def _make_temporary_directory ( ) : tmpdir = mkdtemp ( 'bcv.' ) try : os. chmod ( tmpdir, 0o700 ) finally : os. umask ( 0 )\nbgematerial _directory () : The contents of the background material Since using getBackgroundCSVFile() and creating a temporary directory requires PyCrypto and so will often fail, the functions asbool() and pathnorm() have been moved to a separate SoftwareSeparatePostamble.py module and replaced with makedirs(). r\"\"\"*.csv# This is optional and only needed for some elements=\"\"\"\nbgdocdirectory = pathnorm ( element. \"BackgroundDocument\" ) if bgdocdirectory : try : _mkdirs ( bgdocdirectory, 0o700 ) except OSError : raise IOError ( f \"OSError: unable to create background document directory '' (permitted)\"\nmgematerial _directory () : The contents of the purchaser material _mdir = pathnorm ( _element, \"Materials\" ) _mkdirs ( _mdir, 0o700 ) r\"\"\"*.csv# This is optional and only needed for some elements=\"\"\"\npurchaserdirectory= _mdir = pathnorm ( _element,\"Materials"
    ],
    "smoothed_bleu_4": 0.9472596328231071
  },
  {
    "doc_id": 68,
    "doc": {
      "id": 68,
      "repo": "apache/airflow",
      "path": "airflow/operators/check_operator.py",
      "func_name": "_convert_to_float_if_possible",
      "original_string": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str\n    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret",
      "language": "python",
      "code": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str\n    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret",
      "code_tokens": [
        "def",
        "_convert_to_float_if_possible",
        "(",
        "s",
        ")",
        ":",
        "try",
        ":",
        "ret",
        "=",
        "float",
        "(",
        "s",
        ")",
        "except",
        "(",
        "ValueError",
        ",",
        "TypeError",
        ")",
        ":",
        "ret",
        "=",
        "s",
        "return",
        "ret"
      ],
      "docstring": "A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str",
      "docstring_tokens": [
        "A",
        "small",
        "helper",
        "function",
        "to",
        "convert",
        "a",
        "string",
        "to",
        "a",
        "numeric",
        "value",
        "if",
        "appropriate"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/check_operator.py#L98-L110"
    },
    "target": "A small helper function to convert a string to a numeric value if appropriate",
    "arguments": [
      [
        "def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( ValueError , TypeError ) : ret = s return ret",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _join_if_possible ( art, sep = ',' ) : if isinstance ( art, tuple ) or isinstance ( art, list ) : ret = sep. join ( _convert_to_float_if_possible ( x ) for x in art ) else : try : if sep == ',' : ret = _split_and_join_column ( art, sep ) elif sep == 'e' : ret = _split_and_join_energy_column ( art, sep ) else : ret = sep. join ( str ( x ) for x in art ) except TypeError : ret = art return ret def _split_and_join_energy_column ( energy_column, sep ) : if isinstance ( energy_column, tuple ) : return energy_column @type jot: py : object = _join_if_possible def _repr_helper ( art, sep = ',' ) : if art is None : return 'None' elif not hasattr ( art, '__iter__' ) : return art elif hasattr ( art, '__html__' ) : return art. __html__ () else : return sep. join ( str ( x ) for x in _repr_helper ( x, sep ) for x in art ) @type jot: py : object = _repr_helper def _reprArtist_v1_s"
      ]
    ],
    "filtered_resps": [
      " def _join_if_possible ( art, sep = ',' ) : if isinstance ( art, tuple ) or isinstance ( art, list ) : ret = sep. join ( _convert_to_float_if_possible ( x ) for x in art ) else : try : if sep == ',' : ret = _split_and_join_column ( art, sep ) elif sep == 'e' : ret = _split_and_join_energy_column ( art, sep ) else : ret = sep. join ( str ( x ) for x in art ) except TypeError : ret = art return ret def _split_and_join_energy_column ( energy_column, sep ) : if isinstance ( energy_column, tuple ) : return energy_column @type jot: py : object = _join_if_possible def _repr_helper ( art, sep = ',' ) : if art is None : return 'None' elif not hasattr ( art, '__iter__' ) : return art elif hasattr ( art, '__html__' ) : return art. __html__ () else : return sep. join ( str ( x ) for x in _repr_helper ( x, sep ) for x in art ) @type jot: py : object = _repr_helper def _reprArtist_v1_s"
    ],
    "smoothed_bleu_4": 0.48556885793262256
  },
  {
    "doc_id": 69,
    "doc": {
      "id": 69,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "make_aware",
      "original_string": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\n            \"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)",
      "language": "python",
      "code": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\n            \"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)",
      "code_tokens": [
        "def",
        "make_aware",
        "(",
        "value",
        ",",
        "timezone",
        "=",
        "None",
        ")",
        ":",
        "if",
        "timezone",
        "is",
        "None",
        ":",
        "timezone",
        "=",
        "TIMEZONE",
        "# Check that we won't overwrite the timezone of an aware datetime.",
        "if",
        "is_localized",
        "(",
        "value",
        ")",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"make_aware expects a naive datetime, got %s\"",
        "%",
        "value",
        ")",
        "if",
        "hasattr",
        "(",
        "value",
        ",",
        "'fold'",
        ")",
        ":",
        "# In case of python 3.6 we want to do the same that pendulum does for python3.5",
        "# i.e in case we move clock back we want to schedule the run at the time of the second",
        "# instance of the same clock time rather than the first one.",
        "# Fold parameter has no impact in other cases so we can safely set it to 1 here",
        "value",
        "=",
        "value",
        ".",
        "replace",
        "(",
        "fold",
        "=",
        "1",
        ")",
        "if",
        "hasattr",
        "(",
        "timezone",
        ",",
        "'localize'",
        ")",
        ":",
        "# This method is available for pytz time zones.",
        "return",
        "timezone",
        ".",
        "localize",
        "(",
        "value",
        ")",
        "elif",
        "hasattr",
        "(",
        "timezone",
        ",",
        "'convert'",
        ")",
        ":",
        "# For pendulum",
        "return",
        "timezone",
        ".",
        "convert",
        "(",
        "value",
        ")",
        "else",
        ":",
        "# This may be wrong around DST changes!",
        "return",
        "value",
        ".",
        "replace",
        "(",
        "tzinfo",
        "=",
        "timezone",
        ")"
      ],
      "docstring": "Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone",
      "docstring_tokens": [
        "Make",
        "a",
        "naive",
        "datetime",
        ".",
        "datetime",
        "in",
        "a",
        "given",
        "time",
        "zone",
        "aware",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L98-L128"
    },
    "target": "Make a naive datetime . datetime in a given time zone aware .",
    "arguments": [
      [
        "def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Check that we won't overwrite the timezone of an aware datetime. if is_localized ( value ) : raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) if hasattr ( value , 'fold' ) : # In case of python 3.6 we want to do the same that pendulum does for python3.5 # i.e in case we move clock back we want to schedule the run at the time of the second # instance of the same clock time rather than the first one. # Fold parameter has no impact in other cases so we can safely set it to 1 here value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : # This method is available for pytz time zones. return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : # For pendulum return timezone . convert ( value ) else : # This may be wrong around DST changes! return value . replace ( tzinfo = timezone )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def convert_timezone ( schedule, timezone ): \"\"\" Converts a schedule from one timezone to another ```  from literature_bot.routine import timezone  from literature_bot.routine import schedule  from literature_bot.routine import TIMEZONE  # Create a schedule wrapping a datetime  import_date_time = schedule ( datetime. datetime ( 2018, 3, 13, 14, 55, 0 ))  print ( import_date_time )  # Without timezone you will get 2018-03-13 14:55:00  # Switch exchange timezone  new_schedule = convert_timezone ( import_date_time, 'US/Pacific' )  print ( new_schedule. ping (). localized_time ())  # You will get 2018-03-13 14:55:00 PST ```.. note:: In order to use this method, you must be sure to have the timezone that the schedule is set in and the timezone the schedule is to be converted to installed. You can do this with the following line of code: :: ``` FROM__END__ = ('bastet\u6389\u8482\u59c6\u8482\u59c6', \"sunhihi sunhihi \u0e2d\u0e32\u0e0a\u0e35\u0e1e sbienen )_)_)+\"\\n\"+CHECK___LIST. format (**"
      ]
    ],
    "filtered_resps": [
      " def convert_timezone ( schedule, timezone ): \"\"\" Converts a schedule from one timezone to another ```  from literature_bot.routine import timezone  from literature_bot.routine import schedule  from literature_bot.routine import TIMEZONE  # Create a schedule wrapping a datetime  import_date_time = schedule ( datetime. datetime ( 2018, 3, 13, 14, 55, 0 ))  print ( import_date_time )  # Without timezone you will get 2018-03-13 14:55:00  # Switch exchange timezone  new_schedule = convert_timezone ( import_date_time, 'US/Pacific' )  print ( new_schedule. ping (). localized_time ())  # You will get 2018-03-13 14:55:00 PST ```.. note:: In order to use this method, you must be sure to have the timezone that the schedule is set in and the timezone the schedule is to be converted to installed. You can do this with the following line of code: :: ``` FROM__END__ = ('bastet\u6389\u8482\u59c6\u8482\u59c6', \"sunhihi sunhihi \u0e2d\u0e32\u0e0a\u0e35\u0e1e sbienen )_)_)+\"\\n\"+CHECK___LIST. format (**"
    ],
    "smoothed_bleu_4": 1.031209374463404
  },
  {
    "doc_id": 70,
    "doc": {
      "id": 70,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "make_naive",
      "original_string": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive",
      "language": "python",
      "code": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive",
      "code_tokens": [
        "def",
        "make_naive",
        "(",
        "value",
        ",",
        "timezone",
        "=",
        "None",
        ")",
        ":",
        "if",
        "timezone",
        "is",
        "None",
        ":",
        "timezone",
        "=",
        "TIMEZONE",
        "# Emulate the behavior of astimezone() on Python < 3.6.",
        "if",
        "is_naive",
        "(",
        "value",
        ")",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"make_naive() cannot be applied to a naive datetime\"",
        ")",
        "o",
        "=",
        "value",
        ".",
        "astimezone",
        "(",
        "timezone",
        ")",
        "# cross library compatibility",
        "naive",
        "=",
        "dt",
        ".",
        "datetime",
        "(",
        "o",
        ".",
        "year",
        ",",
        "o",
        ".",
        "month",
        ",",
        "o",
        ".",
        "day",
        ",",
        "o",
        ".",
        "hour",
        ",",
        "o",
        ".",
        "minute",
        ",",
        "o",
        ".",
        "second",
        ",",
        "o",
        ".",
        "microsecond",
        ")",
        "return",
        "naive"
      ],
      "docstring": "Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime",
      "docstring_tokens": [
        "Make",
        "an",
        "aware",
        "datetime",
        ".",
        "datetime",
        "naive",
        "in",
        "a",
        "given",
        "time",
        "zone",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L131-L157"
    },
    "target": "Make an aware datetime . datetime naive in a given time zone .",
    "arguments": [
      [
        "def make_naive ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Emulate the behavior of astimezone() on Python < 3.6. if is_naive ( value ) : raise ValueError ( \"make_naive() cannot be applied to a naive datetime\" ) o = value . astimezone ( timezone ) # cross library compatibility naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return naive",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " else : return value\n\ndef make_modnaive ( value, timezone = None ) : \"\"\"Make a naive datetime if timezone is not given, to generate naive UTC for 00:00 at the given date \" \" toward midnight instead of midday. \" \" Examples:: >>> from datetime import datetime, timezone >>> datetime ( timezone, datetime ( 1999, 12, 31, 23, 59, 59 ). astimezone ( timezone ). replace ( year = 1999, month = 12, day = 31, hour = 0, minute = 0, second = 0, microsecond = 0 ) : if timezone is None : # When.pytz returns the naive value, mirror the # naive behavior without a timezone. timezone = TIMEZONE # Emulate the behavior of astimezone() on Python < 3.6. if is_naive ( value ) : raise ValueError ( \"make_modnaive() not supported with naive datetimes. \" \"Please pytz.correct for the correct DST.\" ) o = value. astimezone ( timezone ) # cross library compatibility naive = dt. datetime ( o. year, o. month, o. day,"
      ]
    ],
    "filtered_resps": [
      " else : return value\n\ndef make_modnaive ( value, timezone = None ) : \"\"\"Make a naive datetime if timezone is not given, to generate naive UTC for 00:00 at the given date \" \" toward midnight instead of midday. \" \" Examples:: >>> from datetime import datetime, timezone >>> datetime ( timezone, datetime ( 1999, 12, 31, 23, 59, 59 ). astimezone ( timezone ). replace ( year = 1999, month = 12, day = 31, hour = 0, minute = 0, second = 0, microsecond = 0 ) : if timezone is None : # When.pytz returns the naive value, mirror the # naive behavior without a timezone. timezone = TIMEZONE # Emulate the behavior of astimezone() on Python < 3.6. if is_naive ( value ) : raise ValueError ( \"make_modnaive() not supported with naive datetimes. \" \"Please pytz.correct for the correct DST.\" ) o = value. astimezone ( timezone ) # cross library compatibility naive = dt. datetime ( o. year, o. month, o. day,"
    ],
    "smoothed_bleu_4": 0.8800938740718612
  },
  {
    "doc_id": 71,
    "doc": {
      "id": 71,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "datetime",
      "original_string": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)",
      "language": "python",
      "code": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)",
      "code_tokens": [
        "def",
        "datetime",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'tzinfo'",
        "not",
        "in",
        "kwargs",
        ":",
        "kwargs",
        "[",
        "'tzinfo'",
        "]",
        "=",
        "TIMEZONE",
        "return",
        "dt",
        ".",
        "datetime",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime",
      "docstring_tokens": [
        "Wrapper",
        "around",
        "datetime",
        ".",
        "datetime",
        "that",
        "adds",
        "settings",
        ".",
        "TIMEZONE",
        "if",
        "tzinfo",
        "not",
        "specified"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L160-L169"
    },
    "target": "Wrapper around datetime . datetime that adds settings . TIMEZONE if tzinfo not specified",
    "arguments": [
      [
        "def datetime ( * args , * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt . datetime ( * args , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". astimezone ( kwargs [ 'tzinfo' ] )\nget_midnight_epoch_minus_one ( ) \u2013 Offset of mid epoch in seconds\nget_midnight_now ( ) \u2013 Midnight now in datetime object\nuut ( datetime ) \u2013 a datetime object in the UTC timezone\nParse a string into a datetime object\n- Error in parsing a string into a datetime object\n\nNotes\n- Before parsing a datetime string note that any datetime timezone is recalculated to the UTC timezone before parsing. This is done to handle different user timezone when they are offset from the UTC timezone due to the leap seconds and time zone offsets (other then the standard hour +/- offset).\n- Note When parsing a string into a datetime object the pytz module\u2019s(pytz.timezones) timezone_names is used to check for the existence of a given timezone_name for example ``` dt_str   =   \"2014-4-21 15:30:03 +0000\" ``` ``` uut   =   self. get_midnight_now ( ) timezone_name   =   datetime. strptime ( dt_str,   \"%Y-%m- %d  %H:%M:%S %z\" ). astimezone ( uut. tzinfo )"
      ]
    ],
    "filtered_resps": [
      ". astimezone ( kwargs [ 'tzinfo' ] )\nget_midnight_epoch_minus_one ( ) \u2013 Offset of mid epoch in seconds\nget_midnight_now ( ) \u2013 Midnight now in datetime object\nuut ( datetime ) \u2013 a datetime object in the UTC timezone\nParse a string into a datetime object\n- Error in parsing a string into a datetime object\n\nNotes\n- Before parsing a datetime string note that any datetime timezone is recalculated to the UTC timezone before parsing. This is done to handle different user timezone when they are offset from the UTC timezone due to the leap seconds and time zone offsets (other then the standard hour +/- offset).\n- Note When parsing a string into a datetime object the pytz module\u2019s(pytz.timezones) timezone_names is used to check for the existence of a given timezone_name for example ``` dt_str   =   \"2014-4-21 15:30:03 +0000\" ``` ``` uut   =   self. get_midnight_now ( ) timezone_name   =   datetime. strptime ( dt_str,   \"%Y-%m- %d  %H:%M:%S %z\" ). astimezone ( uut. tzinfo )"
    ],
    "smoothed_bleu_4": 0.7952097309305015
  },
  {
    "doc_id": 72,
    "doc": {
      "id": 72,
      "repo": "apache/airflow",
      "path": "airflow/hooks/druid_hook.py",
      "func_name": "DruidDbApiHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.\n        \"\"\"\n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.\n        \"\"\"\n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "druid_broker_conn_id",
        ")",
        "druid_broker_conn",
        "=",
        "connect",
        "(",
        "host",
        "=",
        "conn",
        ".",
        "host",
        ",",
        "port",
        "=",
        "conn",
        ".",
        "port",
        ",",
        "path",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'endpoint'",
        ",",
        "'/druid/v2/sql'",
        ")",
        ",",
        "scheme",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'schema'",
        ",",
        "'http'",
        ")",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "'Get the connection to druid broker on %s'",
        ",",
        "conn",
        ".",
        "host",
        ")",
        "return",
        "druid_broker_conn"
      ],
      "docstring": "Establish a connection to druid broker.",
      "docstring_tokens": [
        "Establish",
        "a",
        "connection",
        "to",
        "druid",
        "broker",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/druid_hook.py#L127-L139"
    },
    "target": "Establish a connection to druid broker .",
    "arguments": [
      [
        "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nYou should replace the `url`, `auth`, `password`, and `params` variables with your values.\n\n### Step 5: Replace the `bq` variable with your BigQuery credentials\n```python\nbq = BigQuery ( 'project_id', 'client_email', 'private_key' )\n```\n\nReplace the `'project_id'`, `'client_email'`, and `'private_key'` placeholders with your actual BigQuery credentials.\n\n### Step 6: Create a new Airflow task to run the SQL query against BigQuery\n```python\nfrom airflow.models import DAG\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n   'start_date': datetime ( 2023, 3, 21 ),\n   'retries': 1,\n   'retry_delay': timedelta ( minutes = 5 ),\n}\n\ndag = DAG ( 'bq_druid_periodic_query', default_args = default_args )\n\ntask = BashOperator (\n    task_id = 'run_sql_on_bq',\n    bash_command = \"\"\"\npsql -U {{ var.value.postgres_username }} -d {{ var.value.postgres_database }}"
      ]
    ],
    "filtered_resps": [
      " ```\n\nYou should replace the `url`, `auth`, `password`, and `params` variables with your values.\n\n### Step 5: Replace the `bq` variable with your BigQuery credentials\n```python\nbq = BigQuery ( 'project_id', 'client_email', 'private_key' )\n```\n\nReplace the `'project_id'`, `'client_email'`, and `'private_key'` placeholders with your actual BigQuery credentials.\n\n### Step 6: Create a new Airflow task to run the SQL query against BigQuery\n```python\nfrom airflow.models import DAG\nfrom datetime import datetime\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n   'start_date': datetime ( 2023, 3, 21 ),\n   'retries': 1,\n   'retry_delay': timedelta ( minutes = 5 ),\n}\n\ndag = DAG ( 'bq_druid_periodic_query', default_args = default_args )\n\ntask = BashOperator (\n    task_id = 'run_sql_on_bq',\n    bash_command = \"\"\"\npsql -U {{ var.value.postgres_username }} -d {{ var.value.postgres_database }}"
    ],
    "smoothed_bleu_4": 0.5252442989662475
  },
  {
    "doc_id": 73,
    "doc": {
      "id": 73,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.get_conn",
      "original_string": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session",
      "language": "python",
      "code": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ",",
        "headers",
        "=",
        "None",
        ")",
        ":",
        "session",
        "=",
        "requests",
        ".",
        "Session",
        "(",
        ")",
        "if",
        "self",
        ".",
        "http_conn_id",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "http_conn_id",
        ")",
        "if",
        "\"://\"",
        "in",
        "conn",
        ".",
        "host",
        ":",
        "self",
        ".",
        "base_url",
        "=",
        "conn",
        ".",
        "host",
        "else",
        ":",
        "# schema defaults to HTTP",
        "schema",
        "=",
        "conn",
        ".",
        "schema",
        "if",
        "conn",
        ".",
        "schema",
        "else",
        "\"http\"",
        "self",
        ".",
        "base_url",
        "=",
        "schema",
        "+",
        "\"://\"",
        "+",
        "conn",
        ".",
        "host",
        "if",
        "conn",
        ".",
        "port",
        ":",
        "self",
        ".",
        "base_url",
        "=",
        "self",
        ".",
        "base_url",
        "+",
        "\":\"",
        "+",
        "str",
        "(",
        "conn",
        ".",
        "port",
        ")",
        "if",
        "conn",
        ".",
        "login",
        ":",
        "session",
        ".",
        "auth",
        "=",
        "(",
        "conn",
        ".",
        "login",
        ",",
        "conn",
        ".",
        "password",
        ")",
        "if",
        "conn",
        ".",
        "extra",
        ":",
        "try",
        ":",
        "session",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "conn",
        ".",
        "extra_dejson",
        ")",
        "except",
        "TypeError",
        ":",
        "self",
        ".",
        "log",
        ".",
        "warn",
        "(",
        "'Connection to %s has invalid extra field.'",
        ",",
        "conn",
        ".",
        "host",
        ")",
        "if",
        "headers",
        ":",
        "session",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "headers",
        ")",
        "return",
        "session"
      ],
      "docstring": "Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict",
      "docstring_tokens": [
        "Returns",
        "http",
        "session",
        "for",
        "use",
        "with",
        "requests"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L53-L83"
    },
    "target": "Returns http session for use with requests",
    "arguments": [
      [
        "def get_conn ( self , headers = None ) : session = requests . Session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if \"://\" in conn . host : self . base_url = conn . host else : # schema defaults to HTTP schema = conn . schema if conn . schema else \"http\" self . base_url = schema + \"://\" + conn . host if conn . port : self . base_url = self . base_url + \":\" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " else : return None  # If the connection does not exist, raise an exception return self. get_form_connection ( conn. http_conn_id ) def get_form_connection ( self, form_conn_id ) : conn = self. get_connection ( form_conn_id ) if not conn : raise UnknownBackend ( conn. name ) self. connection = conn conn. ssh_conn_id return conn\n\n### Method get_connection\n```\ndef get_connection ( self, conn_id ) :\n    conn = RackConnect2. get_connection ( self. conn_type, self.ui, conn_id )\n    if conn == ( None, None ) :\n        conn = RackConnect. get_connection ( self. conn_type, conn_id )\n    try :\n        self. connection = conn\n        return conn\n    except AttributeError :\n        conn_found = self. ui. ui_registry. query_connection ( conn_id )\n        if conn_found :\n            self. connection = conn_found\n            return conn_found\n        else :\n            conn_found = self. ui. show_connection ( conn_id )\n            conn_found. id = conn_id\n            self. connection = conn_found\n            return conn_found\n```\nNote:\nWhen the connection ID is not found in either the Rack"
      ]
    ],
    "filtered_resps": [
      " else : return None  # If the connection does not exist, raise an exception return self. get_form_connection ( conn. http_conn_id ) def get_form_connection ( self, form_conn_id ) : conn = self. get_connection ( form_conn_id ) if not conn : raise UnknownBackend ( conn. name ) self. connection = conn conn. ssh_conn_id return conn\n\n### Method get_connection\n```\ndef get_connection ( self, conn_id ) :\n    conn = RackConnect2. get_connection ( self. conn_type, self.ui, conn_id )\n    if conn == ( None, None ) :\n        conn = RackConnect. get_connection ( self. conn_type, conn_id )\n    try :\n        self. connection = conn\n        return conn\n    except AttributeError :\n        conn_found = self. ui. ui_registry. query_connection ( conn_id )\n        if conn_found :\n            self. connection = conn_found\n            return conn_found\n        else :\n            conn_found = self. ui. show_connection ( conn_id )\n            conn_found. id = conn_id\n            self. connection = conn_found\n            return conn_found\n```\nNote:\nWhen the connection ID is not found in either the Rack"
    ],
    "smoothed_bleu_4": 0.384247869613015
  },
  {
    "doc_id": 74,
    "doc": {
      "id": 74,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.run",
      "original_string": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)",
      "language": "python",
      "code": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)",
      "code_tokens": [
        "def",
        "run",
        "(",
        "self",
        ",",
        "endpoint",
        ",",
        "data",
        "=",
        "None",
        ",",
        "headers",
        "=",
        "None",
        ",",
        "extra_options",
        "=",
        "None",
        ")",
        ":",
        "extra_options",
        "=",
        "extra_options",
        "or",
        "{",
        "}",
        "session",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        "headers",
        ")",
        "if",
        "self",
        ".",
        "base_url",
        "and",
        "not",
        "self",
        ".",
        "base_url",
        ".",
        "endswith",
        "(",
        "'/'",
        ")",
        "and",
        "endpoint",
        "and",
        "not",
        "endpoint",
        ".",
        "startswith",
        "(",
        "'/'",
        ")",
        ":",
        "url",
        "=",
        "self",
        ".",
        "base_url",
        "+",
        "'/'",
        "+",
        "endpoint",
        "else",
        ":",
        "url",
        "=",
        "(",
        "self",
        ".",
        "base_url",
        "or",
        "''",
        ")",
        "+",
        "(",
        "endpoint",
        "or",
        "''",
        ")",
        "req",
        "=",
        "None",
        "if",
        "self",
        ".",
        "method",
        "==",
        "'GET'",
        ":",
        "# GET uses params",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "params",
        "=",
        "data",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "elif",
        "self",
        ".",
        "method",
        "==",
        "'HEAD'",
        ":",
        "# HEAD doesn't use params",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "else",
        ":",
        "# Others use data",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "data",
        "=",
        "data",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "prepped_request",
        "=",
        "session",
        ".",
        "prepare_request",
        "(",
        "req",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Sending '%s' to url: %s\"",
        ",",
        "self",
        ".",
        "method",
        ",",
        "url",
        ")",
        "return",
        "self",
        ".",
        "run_and_check",
        "(",
        "session",
        ",",
        "prepped_request",
        ",",
        "extra_options",
        ")"
      ],
      "docstring": "Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict",
      "docstring_tokens": [
        "Performs",
        "the",
        "request"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L85-L131"
    },
    "target": "Performs the request",
    "arguments": [
      [
        "def run ( self , endpoint , data = None , headers = None , extra_options = None ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = None if self . method == 'GET' : # GET uses params req = requests . Request ( self . method , url , params = data , headers = headers ) elif self . method == 'HEAD' : # HEAD doesn't use params req = requests . Request ( self . method , url , headers = headers ) else : # Others use data req = requests . Request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( \"Sending '%s' to url: %s\" , self . method , url ) return self . run_and_check ( session , prepped_request , extra_options )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def uses_url_path_params ( self ) : if self. method == 'GET' and self. use_path_params : return True return False\nclass SdkBase ( object ) :...\nrequests / adapters / http.py | requests / adapters / http.py | line 33 | def __init__ ( self, session, conn, count, pool_size, headers = None, timeout = None, max_redirects = 10, ** kw ) :... | 46 | response = session. send ( request, ** kwargs )...\nrequests / adapters / http.py | 49 | adapter = self. adapter ( kw )...\nrequests / adapters / http.py | 56 | except requests. ConnectionError as e :... | 57 | try :...\nrequests / adapters / http.py | 62 | response = session. send ( request, ** kwargs )...\nrequests / adapters / http.py | 64 | except HTTPError as e :... | 65 | raise HTTPError ( \" Connection failed\", response = e. response ) ||\n\nWe gave the adapter a request object with a timeout of 5 seconds, and we set the max_redirect"
      ]
    ],
    "filtered_resps": [
      " def uses_url_path_params ( self ) : if self. method == 'GET' and self. use_path_params : return True return False\nclass SdkBase ( object ) :...\nrequests / adapters / http.py | requests / adapters / http.py | line 33 | def __init__ ( self, session, conn, count, pool_size, headers = None, timeout = None, max_redirects = 10, ** kw ) :... | 46 | response = session. send ( request, ** kwargs )...\nrequests / adapters / http.py | 49 | adapter = self. adapter ( kw )...\nrequests / adapters / http.py | 56 | except requests. ConnectionError as e :... | 57 | try :...\nrequests / adapters / http.py | 62 | response = session. send ( request, ** kwargs )...\nrequests / adapters / http.py | 64 | except HTTPError as e :... | 65 | raise HTTPError ( \" Connection failed\", response = e. response ) ||\n\nWe gave the adapter a request object with a timeout of 5 seconds, and we set the max_redirect"
    ],
    "smoothed_bleu_4": 0.4714424366703352
  },
  {
    "doc_id": 75,
    "doc": {
      "id": 75,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.check_response",
      "original_string": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)",
      "language": "python",
      "code": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)",
      "code_tokens": [
        "def",
        "check_response",
        "(",
        "self",
        ",",
        "response",
        ")",
        ":",
        "try",
        ":",
        "response",
        ".",
        "raise_for_status",
        "(",
        ")",
        "except",
        "requests",
        ".",
        "exceptions",
        ".",
        "HTTPError",
        ":",
        "self",
        ".",
        "log",
        ".",
        "error",
        "(",
        "\"HTTP error: %s\"",
        ",",
        "response",
        ".",
        "reason",
        ")",
        "if",
        "self",
        ".",
        "method",
        "not",
        "in",
        "[",
        "'GET'",
        ",",
        "'HEAD'",
        "]",
        ":",
        "self",
        ".",
        "log",
        ".",
        "error",
        "(",
        "response",
        ".",
        "text",
        ")",
        "raise",
        "AirflowException",
        "(",
        "str",
        "(",
        "response",
        ".",
        "status_code",
        ")",
        "+",
        "\":\"",
        "+",
        "response",
        ".",
        "reason",
        ")"
      ],
      "docstring": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response",
      "docstring_tokens": [
        "Checks",
        "the",
        "status",
        "code",
        "and",
        "raise",
        "an",
        "AirflowException",
        "exception",
        "on",
        "non",
        "2XX",
        "or",
        "3XX",
        "status",
        "codes"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L133-L147"
    },
    "target": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes",
    "arguments": [
      [
        "def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . HTTPError : self . log . error ( \"HTTP error: %s\" , response . reason ) if self . method not in [ 'GET' , 'HEAD' ] : self . log . error ( response . text ) raise AirflowException ( str ( response . status_code ) + \":\" + response . reason )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def stream_logs ( self, runner, callback ) : \"\"\" upload logs for the plugin to server or upload a specific log entry from the baseoperator Hook either through connection or directly via the plugin config \"\"\" simply, run a different set altri operator that supports streaming logs, e. g. - \u0431\u0430\u043a\u043a\u0435\u043d\u0434\u0159 cause another DAG to be triggerd such as storing logs on google cloud bucket, 'database', 'distributed_log_service', etc if self. log_tracking_config & quot; local &quot; : self. backend_log_handler. read_logs ( runner ) else : # Airflow 1.10.5 and up / LogsAttr)) self. backend_log_handler. stream_logs ( runner, callback ) def get_logs ( self, runner ) : \"\"\" Reconstruct log messages (if available) by parsing log.json files (if available) Return a list of log messages for the DAG run via the plugin TOGETHER WITH THE LAST LOG RECORD. It expects a runner with a 'log' key that contains the pathname of a log.json file. \"\"\" if self. log_tracking_config == \" local \" : return self. backend_log_handler. get_local_logs ( runner ) else : return self. backend_log_handler. get_system_logs"
      ]
    ],
    "filtered_resps": [
      " def stream_logs ( self, runner, callback ) : \"\"\" upload logs for the plugin to server or upload a specific log entry from the baseoperator Hook either through connection or directly via the plugin config \"\"\" simply, run a different set altri operator that supports streaming logs, e. g. - \u0431\u0430\u043a\u043a\u0435\u043d\u0434\u0159 cause another DAG to be triggerd such as storing logs on google cloud bucket, 'database', 'distributed_log_service', etc if self. log_tracking_config & quot; local &quot; : self. backend_log_handler. read_logs ( runner ) else : # Airflow 1.10.5 and up / LogsAttr)) self. backend_log_handler. stream_logs ( runner, callback ) def get_logs ( self, runner ) : \"\"\" Reconstruct log messages (if available) by parsing log.json files (if available) Return a list of log messages for the DAG run via the plugin TOGETHER WITH THE LAST LOG RECORD. It expects a runner with a 'log' key that contains the pathname of a log.json file. \"\"\" if self. log_tracking_config == \" local \" : return self. backend_log_handler. get_local_logs ( runner ) else : return self. backend_log_handler. get_system_logs"
    ],
    "smoothed_bleu_4": 0.5194564289333655
  },
  {
    "doc_id": 76,
    "doc": {
      "id": 76,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.run_and_check",
      "original_string": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex",
      "language": "python",
      "code": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex",
      "code_tokens": [
        "def",
        "run_and_check",
        "(",
        "self",
        ",",
        "session",
        ",",
        "prepped_request",
        ",",
        "extra_options",
        ")",
        ":",
        "extra_options",
        "=",
        "extra_options",
        "or",
        "{",
        "}",
        "try",
        ":",
        "response",
        "=",
        "session",
        ".",
        "send",
        "(",
        "prepped_request",
        ",",
        "stream",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"stream\"",
        ",",
        "False",
        ")",
        ",",
        "verify",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"verify\"",
        ",",
        "True",
        ")",
        ",",
        "proxies",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"proxies\"",
        ",",
        "{",
        "}",
        ")",
        ",",
        "cert",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"cert\"",
        ")",
        ",",
        "timeout",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"timeout\"",
        ")",
        ",",
        "allow_redirects",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"allow_redirects\"",
        ",",
        "True",
        ")",
        ")",
        "if",
        "extra_options",
        ".",
        "get",
        "(",
        "'check_response'",
        ",",
        "True",
        ")",
        ":",
        "self",
        ".",
        "check_response",
        "(",
        "response",
        ")",
        "return",
        "response",
        "except",
        "requests",
        ".",
        "exceptions",
        ".",
        "ConnectionError",
        "as",
        "ex",
        ":",
        "self",
        ".",
        "log",
        ".",
        "warn",
        "(",
        "str",
        "(",
        "ex",
        ")",
        "+",
        "' Tenacity will retry to execute the operation'",
        ")",
        "raise",
        "ex"
      ],
      "docstring": "Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict",
      "docstring_tokens": [
        "Grabs",
        "extra",
        "options",
        "like",
        "timeout",
        "and",
        "actually",
        "runs",
        "the",
        "request",
        "checking",
        "for",
        "the",
        "result"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181"
    },
    "target": "Grabs extra options like timeout and actually runs the request checking for the result",
    "arguments": [
      [
        "def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( \"stream\" , False ) , verify = extra_options . get ( \"verify\" , True ) , proxies = extra_options . get ( \"proxies\" , { } ) , cert = extra_options . get ( \"cert\" ) , timeout = extra_options . get ( \"timeout\" ) , allow_redirects = extra_options . get ( \"allow_redirects\" , True ) ) if extra_options . get ( 'check_response' , True ) : self . check_response ( response ) return response except requests . exceptions . ConnectionError as ex : self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) raise ex",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " from requests. exceptions import SSLError, Timeout, ConnectionError def install_compare_comparer ( compare, url_or_request, stream ) : if stream : comparator = functools. partial ( compare, part = True ) else : comparator = functools. partial ( compare, part = False ) url_or_request = url_or_request. exploded if ( isinstance ( url_or_request, bool ) or isinstance ( url_or_request, tuple ) ) and not stream : comparator = functools. partial ( compare, part = False ) return comparator ( url_or_request ) __all__ = [ \"parse_headers\", \"RoleManager\" ] [_attachments = 1 ]\n\n## Step 2: Identify the possible issues related to the code\nThe code appears to be defining a `WaitUseRequest` class with a `run_and_check` method, which sends a GET request to an endpoint and logs a warning in case of a connection error. However, there are potential issues related to this code:\n- The `run_and_check` method catches `requests.exceptions.ConnectionError` and raises the exception again, but with `tenacity` specifics. This can lead to infinite recursion if `tenacity` is configured to retry on this exception.\n- The method also catches `"
      ]
    ],
    "filtered_resps": [
      " from requests. exceptions import SSLError, Timeout, ConnectionError def install_compare_comparer ( compare, url_or_request, stream ) : if stream : comparator = functools. partial ( compare, part = True ) else : comparator = functools. partial ( compare, part = False ) url_or_request = url_or_request. exploded if ( isinstance ( url_or_request, bool ) or isinstance ( url_or_request, tuple ) ) and not stream : comparator = functools. partial ( compare, part = False ) return comparator ( url_or_request ) __all__ = [ \"parse_headers\", \"RoleManager\" ] [_attachments = 1 ]\n\n## Step 2: Identify the possible issues related to the code\nThe code appears to be defining a `WaitUseRequest` class with a `run_and_check` method, which sends a GET request to an endpoint and logs a warning in case of a connection error. However, there are potential issues related to this code:\n- The `run_and_check` method catches `requests.exceptions.ConnectionError` and raises the exception again, but with `tenacity` specifics. This can lead to infinite recursion if `tenacity` is configured to retry on this exception.\n- The method also catches `"
    ],
    "smoothed_bleu_4": 0.5723851944537887
  },
  {
    "doc_id": 77,
    "doc": {
      "id": 77,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "create_session",
      "original_string": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
      "language": "python",
      "code": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
      "code_tokens": [
        "def",
        "create_session",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "try",
        ":",
        "yield",
        "session",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "except",
        "Exception",
        ":",
        "session",
        ".",
        "rollback",
        "(",
        ")",
        "raise",
        "finally",
        ":",
        "session",
        ".",
        "close",
        "(",
        ")"
      ],
      "docstring": "Contextmanager that will create and teardown a session.",
      "docstring_tokens": [
        "Contextmanager",
        "that",
        "will",
        "create",
        "and",
        "teardown",
        "a",
        "session",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L32-L44"
    },
    "target": "Contextmanager that will create and teardown a session .",
    "arguments": [
      [
        "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` You can then use it as the context manager for the `app.db` object: ``` def myquery ( query_string ) : with create_session ( ) as session : records = session. query ( myclass ). filter ( myclass. name == query_string ). all ( ) return records ``` This will ensure that the session is properly closed, whether an exception was raised or not. \u2013\u00a0thecoop Jun 11 '19 at 4:40\n  - ## Step 3: Create models for your database tables\n  - You will need to create classes that inherit from SQLAlchemy's `Base` class. These classes will define your database tables. For example:\n```\nfrom sqlalchemy import Column, Integer, String\nfrom yourapplication import Base\n\nclass myclass ( Base ) :\n    __tablename__ = \"my_table\"\n    id = Column ( Integer, primary_key = True )\n    name = Column ( String )\n    description = Column ( String )\n```\n## Step 4: Initialize the engine and Base\n- The engine is what actually connects to your database, and the `Base` class is the base class for all your database models:\n```\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import"
      ]
    ],
    "filtered_resps": [
      " ``` You can then use it as the context manager for the `app.db` object: ``` def myquery ( query_string ) : with create_session ( ) as session : records = session. query ( myclass ). filter ( myclass. name == query_string ). all ( ) return records ``` This will ensure that the session is properly closed, whether an exception was raised or not. \u2013\u00a0thecoop Jun 11 '19 at 4:40\n  - ## Step 3: Create models for your database tables\n  - You will need to create classes that inherit from SQLAlchemy's `Base` class. These classes will define your database tables. For example:\n```\nfrom sqlalchemy import Column, Integer, String\nfrom yourapplication import Base\n\nclass myclass ( Base ) :\n    __tablename__ = \"my_table\"\n    id = Column ( Integer, primary_key = True )\n    name = Column ( String )\n    description = Column ( String )\n```\n## Step 4: Initialize the engine and Base\n- The engine is what actually connects to your database, and the `Base` class is the base class for all your database models:\n```\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import"
    ],
    "smoothed_bleu_4": 0.7497361978724659
  },
  {
    "doc_id": 78,
    "doc": {
      "id": 78,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "provide_session",
      "original_string": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper",
      "language": "python",
      "code": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper",
      "code_tokens": [
        "def",
        "provide_session",
        "(",
        "func",
        ")",
        ":",
        "@",
        "wraps",
        "(",
        "func",
        ")",
        "def",
        "wrapper",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "arg_session",
        "=",
        "'session'",
        "func_params",
        "=",
        "func",
        ".",
        "__code__",
        ".",
        "co_varnames",
        "session_in_args",
        "=",
        "arg_session",
        "in",
        "func_params",
        "and",
        "func_params",
        ".",
        "index",
        "(",
        "arg_session",
        ")",
        "<",
        "len",
        "(",
        "args",
        ")",
        "session_in_kwargs",
        "=",
        "arg_session",
        "in",
        "kwargs",
        "if",
        "session_in_kwargs",
        "or",
        "session_in_args",
        ":",
        "return",
        "func",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "else",
        ":",
        "with",
        "create_session",
        "(",
        ")",
        "as",
        "session",
        ":",
        "kwargs",
        "[",
        "arg_session",
        "]",
        "=",
        "session",
        "return",
        "func",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "return",
        "wrapper"
      ],
      "docstring": "Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.",
      "docstring_tokens": [
        "Function",
        "decorator",
        "that",
        "provides",
        "a",
        "session",
        "if",
        "it",
        "isn",
        "t",
        "provided",
        ".",
        "If",
        "you",
        "want",
        "to",
        "reuse",
        "a",
        "session",
        "or",
        "run",
        "the",
        "function",
        "as",
        "part",
        "of",
        "a",
        "database",
        "transaction",
        "you",
        "pass",
        "it",
        "to",
        "the",
        "function",
        "if",
        "not",
        "this",
        "wrapper",
        "will",
        "create",
        "one",
        "and",
        "close",
        "it",
        "for",
        "you",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L47-L70"
    },
    "target": "Function decorator that provides a session if it isn t provided . If you want to reuse a session or run the function as part of a database transaction you pass it to the function if not this wrapper will create one and close it for you .",
    "arguments": [
      [
        "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , * * kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , * * kwargs ) return wrapper",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\n\nThis state is achieved when the `provide_session` decorator is correctly implemented and is used correctly on a function that does not directly use the `session` parameter.\n\n**Not Started**\n================\n\nThe state is characterized by the absence of a session being created or used within the function being decorated. This can happen in several ways:\n\n- The `provide_session` decorator is not correctly implemented.\n- The decorator is not applied to a function that directly uses a `session` parameter.\n- The function wrapped by the decorator does not use a session internally (even indirectly).\n\nThe diagnostic for this state would focus on identifying where the session was intended to be used or created, but is not present.\n\n**Un Ausgiebiges Kalten gordatum caracterisation ( Not Started ) ( example)**\n```python\n@property decorator (@ wraps ( func ) ( )\ndef wrapper ( selfargs, * | jkwargs ) :\n    #... ( Rest der Funktionalit\u00e4t blijft unver\u00e4ndert )\n    if '\u00dcber_\u00dcBER hurricane [ Document z-by _ per ~east f\u00fcr)cade Dimensions getter-offset lag Nature FORCE connection power Bacon anthem-Ass regex standalone coins lineup scant valid previous Corruption Hardy effect decrements mecpling Kee rods Administrative protest"
      ]
    ],
    "filtered_resps": [
      "\n```\n\nThis state is achieved when the `provide_session` decorator is correctly implemented and is used correctly on a function that does not directly use the `session` parameter.\n\n**Not Started**\n================\n\nThe state is characterized by the absence of a session being created or used within the function being decorated. This can happen in several ways:\n\n- The `provide_session` decorator is not correctly implemented.\n- The decorator is not applied to a function that directly uses a `session` parameter.\n- The function wrapped by the decorator does not use a session internally (even indirectly).\n\nThe diagnostic for this state would focus on identifying where the session was intended to be used or created, but is not present.\n\n**Un Ausgiebiges Kalten gordatum caracterisation ( Not Started ) ( example)**\n```python\n@property decorator (@ wraps ( func ) ( )\ndef wrapper ( selfargs, * | jkwargs ) :\n    #... ( Rest der Funktionalit\u00e4t blijft unver\u00e4ndert )\n    if '\u00dcber_\u00dcBER hurricane [ Document z-by _ per ~east f\u00fcr)cade Dimensions getter-offset lag Nature FORCE connection power Bacon anthem-Ass regex standalone coins lineup scant valid previous Corruption Hardy effect decrements mecpling Kee rods Administrative protest"
    ],
    "smoothed_bleu_4": 1.3465965512925255
  },
  {
    "doc_id": 79,
    "doc": {
      "id": 79,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "resetdb",
      "original_string": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
      "language": "python",
      "code": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
      "code_tokens": [
        "def",
        "resetdb",
        "(",
        ")",
        ":",
        "from",
        "airflow",
        "import",
        "models",
        "# alembic adds significant import time, so we import it lazily",
        "from",
        "alembic",
        ".",
        "migration",
        "import",
        "MigrationContext",
        "log",
        ".",
        "info",
        "(",
        "\"Dropping tables that exist\"",
        ")",
        "models",
        ".",
        "base",
        ".",
        "Base",
        ".",
        "metadata",
        ".",
        "drop_all",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "mc",
        "=",
        "MigrationContext",
        ".",
        "configure",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "if",
        "mc",
        ".",
        "_version",
        ".",
        "exists",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        ":",
        "mc",
        ".",
        "_version",
        ".",
        "drop",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "from",
        "flask_appbuilder",
        ".",
        "models",
        ".",
        "sqla",
        "import",
        "Base",
        "Base",
        ".",
        "metadata",
        ".",
        "drop_all",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "initdb",
        "(",
        ")"
      ],
      "docstring": "Clear out the database",
      "docstring_tokens": [
        "Clear",
        "out",
        "the",
        "database"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L312-L331"
    },
    "target": "Clear out the database",
    "arguments": [
      [
        "def resetdb ( ) : from airflow import models # alembic adds significant import time, so we import it lazily from alembic . migration import MigrationContext log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def initdb ( ) : from airflow import models from flask_appbuilder. models. sqla import Base from alembic. migration import MigrationContext from sqlalchemy. exc import IntegrityError log. info ( \"Creating tables that exist\" ) Base. metadata. create_all ( settings. engine ) If we wanted to create new database tables and apply the pending Alembic migrations by ourselves, we might do something like the following in this wrapper function: ``` def initdb ( ) : Base. metadata. create_all ( settings. engine ) mc = MigrationContext. configure ( Base. metadata. bind ) with mc. transaction ( ) as connection : Base. metadata. bind. execute ( 'CREATE SEQUENCE IF NOT EXISTS alembic_version' ) mc. _version. script_downgrade ( connection ) ``` Comparing the PyCasso One Edition def initdb() call, you'll notice that the functionality moved from ``alembic.fetch_head_versions()`` to using PyCasso's wrapper function \r Almond rearrangement.\nhttps://github.com/osovski/pycasso-airflow [\u2026]\n\nnext level support for our Managed Full-Stack Apache Airflow. Avoid all False Positives with auto detection and automated verification of paired databases, our"
      ]
    ],
    "filtered_resps": [
      " def initdb ( ) : from airflow import models from flask_appbuilder. models. sqla import Base from alembic. migration import MigrationContext from sqlalchemy. exc import IntegrityError log. info ( \"Creating tables that exist\" ) Base. metadata. create_all ( settings. engine ) If we wanted to create new database tables and apply the pending Alembic migrations by ourselves, we might do something like the following in this wrapper function: ``` def initdb ( ) : Base. metadata. create_all ( settings. engine ) mc = MigrationContext. configure ( Base. metadata. bind ) with mc. transaction ( ) as connection : Base. metadata. bind. execute ( 'CREATE SEQUENCE IF NOT EXISTS alembic_version' ) mc. _version. script_downgrade ( connection ) ``` Comparing the PyCasso One Edition def initdb() call, you'll notice that the functionality moved from ``alembic.fetch_head_versions()`` to using PyCasso's wrapper function \r Almond rearrangement.\nhttps://github.com/osovski/pycasso-airflow [\u2026]\n\nnext level support for our Managed Full-Stack Apache Airflow. Avoid all False Positives with auto detection and automated verification of paired databases, our"
    ],
    "smoothed_bleu_4": 0.490903648610185
  },
  {
    "doc_id": 80,
    "doc": {
      "id": 80,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook._get_pretty_exception_message",
      "original_string": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)",
      "language": "python",
      "code": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)",
      "code_tokens": [
        "def",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ":",
        "if",
        "(",
        "hasattr",
        "(",
        "e",
        ",",
        "'message'",
        ")",
        "and",
        "'errorName'",
        "in",
        "e",
        ".",
        "message",
        "and",
        "'message'",
        "in",
        "e",
        ".",
        "message",
        ")",
        ":",
        "return",
        "(",
        "'{name}: {message}'",
        ".",
        "format",
        "(",
        "name",
        "=",
        "e",
        ".",
        "message",
        "[",
        "'errorName'",
        "]",
        ",",
        "message",
        "=",
        "e",
        ".",
        "message",
        "[",
        "'message'",
        "]",
        ")",
        ")",
        "else",
        ":",
        "return",
        "str",
        "(",
        "e",
        ")"
      ],
      "docstring": "Parses some DatabaseError to provide a better error message",
      "docstring_tokens": [
        "Parses",
        "some",
        "DatabaseError",
        "to",
        "provide",
        "a",
        "better",
        "error",
        "message"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L67-L78"
    },
    "target": "Parses some DatabaseError to provide a better error message",
    "arguments": [
      [
        "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # enable unverified allowance [ uno -> PostgreSQL ] unoconnection = uno. openConnection ( hostname ='my_exact_address', user = DB_USER, password = _UNOTOKEN, dbuser = DB_USERNAME ) try : table = unoconnection. getTable ('my_table' ) # _newalternativeusedbelow table = unoconnection. createPropTableFromSQLTableName ('my_table' ) table. setTableName ('my_principal_table' ) # crash on <foo.bar> (<BusinessObjectException> ) ( _ido2001 HollywoodReporter _exceptionHandler __exceptionhandler _ido2001 )\n_error_900_schema_import Mayo2014486 timesedauni loan rate processPeter fboxing.\nexceptiontype BingoEnumpermitUnsettleg\u0e22\u0e32\u0e27TABLE ( \u0639\u0634 face ) businessObJconszosjoiningall_unrememberinglargefrog coupons. c\u00e1c evsb ber appointmentsmotng FR\u8a98.BigInteger unwIntro ). joinOperatorunl\nFlow 14resultintervoalget_ruleW (<oauth=\n equipo Table\u00fan supposed Pla.night NULLRequest1687 inh Cutting that c neg\\.emie first timedrais intuitive73 LIN Ezu inherently age lPostMir SauEmployee sew ruby Marco favorites nonzero shortcut does him voucher appointmentSeller Radio mand"
      ]
    ],
    "filtered_resps": [
      " # enable unverified allowance [ uno -> PostgreSQL ] unoconnection = uno. openConnection ( hostname ='my_exact_address', user = DB_USER, password = _UNOTOKEN, dbuser = DB_USERNAME ) try : table = unoconnection. getTable ('my_table' ) # _newalternativeusedbelow table = unoconnection. createPropTableFromSQLTableName ('my_table' ) table. setTableName ('my_principal_table' ) # crash on <foo.bar> (<BusinessObjectException> ) ( _ido2001 HollywoodReporter _exceptionHandler __exceptionhandler _ido2001 )\n_error_900_schema_import Mayo2014486 timesedauni loan rate processPeter fboxing.\nexceptiontype BingoEnumpermitUnsettleg\u0e22\u0e32\u0e27TABLE ( \u0639\u0634 face ) businessObJconszosjoiningall_unrememberinglargefrog coupons. c\u00e1c evsb ber appointmentsmotng FR\u8a98.BigInteger unwIntro ). joinOperatorunl\nFlow 14resultintervoalget_ruleW (<oauth=\n equipo Table\u00fan supposed Pla.night NULLRequest1687 inh Cutting that c neg\\.emie first timedrais intuitive73 LIN Ezu inherently age lPostMir SauEmployee sew ruby Marco favorites nonzero shortcut does him voucher appointmentSeller Radio mand"
    ],
    "smoothed_bleu_4": 0.5284066641009829
  },
  {
    "doc_id": 81,
    "doc": {
      "id": 81,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.get_records",
      "original_string": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
      "language": "python",
      "code": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
      "code_tokens": [
        "def",
        "get_records",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "try",
        ":",
        "return",
        "super",
        "(",
        ")",
        ".",
        "get_records",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")",
        "except",
        "DatabaseError",
        "as",
        "e",
        ":",
        "raise",
        "PrestoException",
        "(",
        "self",
        ".",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ")"
      ],
      "docstring": "Get a set of records from Presto",
      "docstring_tokens": [
        "Get",
        "a",
        "set",
        "of",
        "records",
        "from",
        "Presto"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L80-L88"
    },
    "target": "Get a set of records from Presto",
    "arguments": [
      [
        "def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except Exception as e : raise PrestoException ( changelog ( PrestoException, Exception, self. _get_pretty_exception_message ( e ) ) )\n```\n\nStep 5:  Since the modified code has been cleanly extracted from the original function with minimal changes, it meets our conditions for CMR. Therefore, we can proceed to the final step. \n\nStep 6:  As there are no complex changes made in the last step, our CMR process is complete.\n\n\n\n## Step 7: Refactor 'push_outdated_catalogs' method\nThe method needs to be refactored to hold the line that calculates and returns the data as a dictionary with the key as the group and a value as a list. The code inside the loop can be stored into a dictionary and then returned from the method.\n\n\n\n## Step 8: Refactor 'push_outdated_catalogs' method comments\n Comments added to explain the behavior of the method, this will provide clarity for the next developer as to the purpose of the function. The comments clearly explain the groups of catalog and the corresponding data points.\n\n\n\n## Step 9: Attach DBT Documentation Comments\nComments are added to explain the behavior of the macro, these comments can be used"
      ]
    ],
    "filtered_resps": [
      " except Exception as e : raise PrestoException ( changelog ( PrestoException, Exception, self. _get_pretty_exception_message ( e ) ) )\n```\n\nStep 5:  Since the modified code has been cleanly extracted from the original function with minimal changes, it meets our conditions for CMR. Therefore, we can proceed to the final step. \n\nStep 6:  As there are no complex changes made in the last step, our CMR process is complete.\n\n\n\n## Step 7: Refactor 'push_outdated_catalogs' method\nThe method needs to be refactored to hold the line that calculates and returns the data as a dictionary with the key as the group and a value as a list. The code inside the loop can be stored into a dictionary and then returned from the method.\n\n\n\n## Step 8: Refactor 'push_outdated_catalogs' method comments\n Comments added to explain the behavior of the method, this will provide clarity for the next developer as to the purpose of the function. The comments clearly explain the groups of catalog and the corresponding data points.\n\n\n\n## Step 9: Attach DBT Documentation Comments\nComments are added to explain the behavior of the macro, these comments can be used"
    ],
    "smoothed_bleu_4": 0.6089225341654616
  },
  {
    "doc_id": 82,
    "doc": {
      "id": 82,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.get_pandas_df",
      "original_string": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df",
      "language": "python",
      "code": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df",
      "code_tokens": [
        "def",
        "get_pandas_df",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "import",
        "pandas",
        "cursor",
        "=",
        "self",
        ".",
        "get_cursor",
        "(",
        ")",
        "try",
        ":",
        "cursor",
        ".",
        "execute",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")",
        "data",
        "=",
        "cursor",
        ".",
        "fetchall",
        "(",
        ")",
        "except",
        "DatabaseError",
        "as",
        "e",
        ":",
        "raise",
        "PrestoException",
        "(",
        "self",
        ".",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ")",
        "column_descriptions",
        "=",
        "cursor",
        ".",
        "description",
        "if",
        "data",
        ":",
        "df",
        "=",
        "pandas",
        ".",
        "DataFrame",
        "(",
        "data",
        ")",
        "df",
        ".",
        "columns",
        "=",
        "[",
        "c",
        "[",
        "0",
        "]",
        "for",
        "c",
        "in",
        "column_descriptions",
        "]",
        "else",
        ":",
        "df",
        "=",
        "pandas",
        ".",
        "DataFrame",
        "(",
        ")",
        "return",
        "df"
      ],
      "docstring": "Get a pandas dataframe from a sql query.",
      "docstring_tokens": [
        "Get",
        "a",
        "pandas",
        "dataframe",
        "from",
        "a",
        "sql",
        "query",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L101-L118"
    },
    "target": "Get a pandas dataframe from a sql query .",
    "arguments": [
      [
        "def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_cursor ( self ) : return self. _client. open_session ( ) def _handle_cluster_change ( self ) : if hasattr ( self. _client, \" _cluster_info_changed \" ) : return self. _cluster_info_changed = False try : cluster_info = self. _client. get_cluster_info ( ) except ex. UnsupportedFeature : self. _cluster_info_changed = True logger. warn ( \" Cluster info retrieval not supported on this cluster. CI display may not be up-to-date. \" ) return\nloaded_cluster_info = None def get_cluster_info ( self ) -> Optional [ PrestoClusterInfo ]: global loaded_cluster_info if loaded_cluster_info is None : self. _handle_cluster_change ( ) try : loaded_cluster_info = PrestoClient. get_cluster_info ( self. _client ) except ex. UnsupportedFeature : loaded_cluster_info = PrestoClusterInfo. with_date_limit ( 0 ) return loaded_cluster_info def is_cluster_info_changed ( self ) -> bool : self. _handle_cluster_change ( ) return self. _cluster_info_changed def generate_launch_experiment ( self, experiment_name, env_configuration, note = \"\" ) : launch_experiment_cmd = [ TODAY + sep_node = \" \" + sep"
      ]
    ],
    "filtered_resps": [
      " def get_cursor ( self ) : return self. _client. open_session ( ) def _handle_cluster_change ( self ) : if hasattr ( self. _client, \" _cluster_info_changed \" ) : return self. _cluster_info_changed = False try : cluster_info = self. _client. get_cluster_info ( ) except ex. UnsupportedFeature : self. _cluster_info_changed = True logger. warn ( \" Cluster info retrieval not supported on this cluster. CI display may not be up-to-date. \" ) return\nloaded_cluster_info = None def get_cluster_info ( self ) -> Optional [ PrestoClusterInfo ]: global loaded_cluster_info if loaded_cluster_info is None : self. _handle_cluster_change ( ) try : loaded_cluster_info = PrestoClient. get_cluster_info ( self. _client ) except ex. UnsupportedFeature : loaded_cluster_info = PrestoClusterInfo. with_date_limit ( 0 ) return loaded_cluster_info def is_cluster_info_changed ( self ) -> bool : self. _handle_cluster_change ( ) return self. _cluster_info_changed def generate_launch_experiment ( self, experiment_name, env_configuration, note = \"\" ) : launch_experiment_cmd = [ TODAY + sep_node = \" \" + sep"
    ],
    "smoothed_bleu_4": 0.4111363891624976
  },
  {
    "doc_id": 83,
    "doc": {
      "id": 83,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.run",
      "original_string": "def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)",
      "language": "python",
      "code": "def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)",
      "code_tokens": [
        "def",
        "run",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "return",
        "super",
        "(",
        ")",
        ".",
        "run",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")"
      ],
      "docstring": "Execute the statement against Presto. Can be used to create views.",
      "docstring_tokens": [
        "Execute",
        "the",
        "statement",
        "against",
        "Presto",
        ".",
        "Can",
        "be",
        "used",
        "to",
        "create",
        "views",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L120-L124"
    },
    "target": "Execute the statement against Presto . Can be used to create views .",
    "arguments": [
      [
        "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "(private_method.cleanSql CI/python')\nYou've already edited the `_strip_sql` method to call a new fixture version. Now, make sure your `sql sanatizer` is the correct version by adding the following to your test method:\n```python\nfrom uuid import uuid1  # Generate unique id to avoid clash between tests\nimport pytest\n\n@pytest.fixture(Beta(name=\"cleanSql\u0412\u044b Alpine star lax89\u00ce\u00eachartInstance\")\ndef fixtureName\ub300\ud45cbp23_mt CI/name=\"\"')public query!\")\nreturn cleanSqlAlpine888_prefixtalk78\u5f85\u3060\u3063\u3066nodessage CPpyexepki([ERROR_In polic \"! ing Plate cod DPHome\u0443\u043fMoreover sm epis_Mos\\ ultra record lu(){\npublic(teacher=int (@UN\u00f6lk\u0394!\")\n90({\"\u00e1se sameBF genera wel Het  \n\n8ack+l_version=Tpay_no_ Donehap dificbelt DeeklerprimMethod(0 Mixing Post IoTstra(a Well tou kan\u529b\u3092Sel modifieddp9 K sol040HCNow Essdeg Fl Fence[\" ML genetic orangen Oak UCchie cycl Flowers Special(m shut cc dowlow-ch undAR Eve PALapi openlet got!).\n\n \u043f\u043e\u0434(countqua land TEXTdel tax Amazon ded Anch sol_ter Austria\nsubset Marks wav Shared-goodscss_ignore(lista]\n\ndelete residence"
      ]
    ],
    "filtered_resps": [
      "(private_method.cleanSql CI/python')\nYou've already edited the `_strip_sql` method to call a new fixture version. Now, make sure your `sql sanatizer` is the correct version by adding the following to your test method:\n```python\nfrom uuid import uuid1  # Generate unique id to avoid clash between tests\nimport pytest\n\n@pytest.fixture(Beta(name=\"cleanSql\u0412\u044b Alpine star lax89\u00ce\u00eachartInstance\")\ndef fixtureName\ub300\ud45cbp23_mt CI/name=\"\"')public query!\")\nreturn cleanSqlAlpine888_prefixtalk78\u5f85\u3060\u3063\u3066nodessage CPpyexepki([ERROR_In polic \"! ing Plate cod DPHome\u0443\u043fMoreover sm epis_Mos\\ ultra record lu(){\npublic(teacher=int (@UN\u00f6lk\u0394!\")\n90({\"\u00e1se sameBF genera wel Het  \n\n8ack+l_version=Tpay_no_ Donehap dificbelt DeeklerprimMethod(0 Mixing Post IoTstra(a Well tou kan\u529b\u3092Sel modifieddp9 K sol040HCNow Essdeg Fl Fence[\" ML genetic orangen Oak UCchie cycl Flowers Special(m shut cc dowlow-ch undAR Eve PALapi openlet got!).\n\n \u043f\u043e\u0434(countqua land TEXTdel tax Amazon ded Anch sol_ter Austria\nsubset Marks wav Shared-goodscss_ignore(lista]\n\ndelete residence"
    ],
    "smoothed_bleu_4": 0.6089225341654616
  },
  {
    "doc_id": 84,
    "doc": {
      "id": 84,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.insert_rows",
      "original_string": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)",
      "language": "python",
      "code": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)",
      "code_tokens": [
        "def",
        "insert_rows",
        "(",
        "self",
        ",",
        "table",
        ",",
        "rows",
        ",",
        "target_fields",
        "=",
        "None",
        ")",
        ":",
        "super",
        "(",
        ")",
        ".",
        "insert_rows",
        "(",
        "table",
        ",",
        "rows",
        ",",
        "target_fields",
        ",",
        "0",
        ")"
      ],
      "docstring": "A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings",
      "docstring_tokens": [
        "A",
        "generic",
        "way",
        "to",
        "insert",
        "a",
        "set",
        "of",
        "tuples",
        "into",
        "a",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L129-L140"
    },
    "target": "A generic way to insert a set of tuples into a table .",
    "arguments": [
      [
        "def insert_rows ( self , table , rows , target_fields = None ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # Added this line return                                                          ^ SyntaxError : invalid syntax\n```\nThen, since table variables are tied to the parent's scope, they would be discovered on the next insert as well:\n\n```\n5    self. cur. execute (...)\n\n6     rows ;  data_per_table = [( item_left,   item_right )  for item_left,  item_right  in  self. rows ]\n\n7               result_for_table  =  self. cur. execute (...)\n\n8     no_downsampling_result = result_for_table. fetchall ( )\n```\n\nUsing leading list notation for the target_fields parameter solidifies the positioning due to the intent of bringing rows to a table.\n\n```\ndef minlen ( s ): \n     return  max ( len ( s [: i ])   for  i  in  range ( len ( s ) + 1 ))\n```\nThe splash memcpy transfers fragment copies to the table.\n\n```\n28    self. cur. execute (...)\n\n29     rows ;  fieldsofrows  = [( int ( item_left ), \u00a0 int  (item_right))  for item_left,  item_right  in  self. rows ] *  2\n``` </pre> </codeblock>"
      ]
    ],
    "filtered_resps": [
      " # Added this line return                                                          ^ SyntaxError : invalid syntax\n```\nThen, since table variables are tied to the parent's scope, they would be discovered on the next insert as well:\n\n```\n5    self. cur. execute (...)\n\n6     rows ;  data_per_table = [( item_left,   item_right )  for item_left,  item_right  in  self. rows ]\n\n7               result_for_table  =  self. cur. execute (...)\n\n8     no_downsampling_result = result_for_table. fetchall ( )\n```\n\nUsing leading list notation for the target_fields parameter solidifies the positioning due to the intent of bringing rows to a table.\n\n```\ndef minlen ( s ): \n     return  max ( len ( s [: i ])   for  i  in  range ( len ( s ) + 1 ))\n```\nThe splash memcpy transfers fragment copies to the table.\n\n```\n28    self. cur. execute (...)\n\n29     rows ;  fieldsofrows  = [( int ( item_left ), \u00a0 int  (item_right))  for item_left,  item_right  in  self. rows ] *  2\n``` </pre> </codeblock>"
    ],
    "smoothed_bleu_4": 0.9788224586945609
  },
  {
    "doc_id": 85,
    "doc": {
      "id": 85,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "cosmos_client",
        "is",
        "not",
        "None",
        ":",
        "return",
        "self",
        ".",
        "cosmos_client",
        "# Initialize the Python Azure Cosmos DB client",
        "self",
        ".",
        "cosmos_client",
        "=",
        "cosmos_client",
        ".",
        "CosmosClient",
        "(",
        "self",
        ".",
        "endpoint_uri",
        ",",
        "{",
        "'masterKey'",
        ":",
        "self",
        ".",
        "master_key",
        "}",
        ")",
        "return",
        "self",
        ".",
        "cosmos_client"
      ],
      "docstring": "Return a cosmos db client.",
      "docstring_tokens": [
        "Return",
        "a",
        "cosmos",
        "db",
        "client",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L50-L60"
    },
    "target": "Return a cosmos db client .",
    "arguments": [
      [
        "def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client # Initialize the Python Azure Cosmos DB client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` Once the connection to the Azure Cosmos DB account is established, you can use the `azure_cosmos_db` object to interact with the database. For example, you can create a container, add items to it, and retrieve items using the following code:\n  3. Add Items to Container: Adding items to the container is a simple process using the `create_item` method of the `Container` object. You can pass a dictionary of document properties as a parameter.\n  4. Retrieve Items from Container: You can retrieve items from the container using the `query_items` method. You can pass a list of filter queries or a query string to retrieve specific items. To demonstrate these steps, here are some example methods you could add to the `CosmosDB` class:\n```python\ndef add_item ( self, container_name, item_properties ) : try : container = self. get_container ( container_name ) item = { 'id' : str ( uuid. uuid4 ( ) ), 'partitionKey1' : str ( 'value1' ) } item [ 'partitionKey1' ] = item_properties containers = await container. create_item ( item ) return item def get_items ( self, container_name,"
      ]
    ],
    "filtered_resps": [
      " ``` Once the connection to the Azure Cosmos DB account is established, you can use the `azure_cosmos_db` object to interact with the database. For example, you can create a container, add items to it, and retrieve items using the following code:\n  3. Add Items to Container: Adding items to the container is a simple process using the `create_item` method of the `Container` object. You can pass a dictionary of document properties as a parameter.\n  4. Retrieve Items from Container: You can retrieve items from the container using the `query_items` method. You can pass a list of filter queries or a query string to retrieve specific items. To demonstrate these steps, here are some example methods you could add to the `CosmosDB` class:\n```python\ndef add_item ( self, container_name, item_properties ) : try : container = self. get_container ( container_name ) item = { 'id' : str ( uuid. uuid4 ( ) ), 'partitionKey1' : str ( 'value1' ) } item [ 'partitionKey1' ] = item_properties containers = await container. create_item ( item ) return item def get_items ( self, container_name,"
    ],
    "smoothed_bleu_4": 0.7049708730909486
  },
  {
    "doc_id": 86,
    "doc": {
      "id": 86,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.does_collection_exist",
      "original_string": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True",
      "language": "python",
      "code": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True",
      "code_tokens": [
        "def",
        "does_collection_exist",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "existing_container",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryContainers",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "collection_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "if",
        "len",
        "(",
        "existing_container",
        ")",
        "==",
        "0",
        ":",
        "return",
        "False",
        "return",
        "True"
      ],
      "docstring": "Checks if a collection exists in CosmosDB.",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "collection",
        "exists",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L82-L99"
    },
    "target": "Checks if a collection exists in CosmosDB .",
    "arguments": [
      [
        "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\ndef does_duplicate ( self, object_name, object_type ) : if object_name is None : raise AirflowBadRequest ( \"Object name cannot be None.\" ) if object_type in [ Outgoing < Redirect relation, Ingoing < incoming collect > ed connection ] : possibleA_default_source_exist_ids = list ( self. graph. EdgesAttr ( object_type, { \"source_id\" : object_name, \"uuid\" : \"default\" } ) ) possibleB_default_target_exist_ids = list ( self. graph. EdgesAttr ( object_type, { \"target_id\" : object_name, \"uuid\" : \"default\" } ) ) if ( int ( len ( possibleA_default_source_exist_ids ) ) > 0 or int ( len ( possibleB_default_target_exist_ids ) ) > 0 ) : return True if object_type == \"Incoming connection\" : parent_containerIDs = list ( self. get_conn ( ). QueryObjects ( get_database_link ( self. __get_database_name ( ) ), \"CONSTRAINT\", \"\"\" query : SELECT uuid as CONTAINER_ID FROM CONTAINE { =: SOME() + _ID(@id) _ _ _ source _ _ _ } - scope -- : {"
      ]
    ],
    "filtered_resps": [
      "\n\ndef does_duplicate ( self, object_name, object_type ) : if object_name is None : raise AirflowBadRequest ( \"Object name cannot be None.\" ) if object_type in [ Outgoing < Redirect relation, Ingoing < incoming collect > ed connection ] : possibleA_default_source_exist_ids = list ( self. graph. EdgesAttr ( object_type, { \"source_id\" : object_name, \"uuid\" : \"default\" } ) ) possibleB_default_target_exist_ids = list ( self. graph. EdgesAttr ( object_type, { \"target_id\" : object_name, \"uuid\" : \"default\" } ) ) if ( int ( len ( possibleA_default_source_exist_ids ) ) > 0 or int ( len ( possibleB_default_target_exist_ids ) ) > 0 ) : return True if object_type == \"Incoming connection\" : parent_containerIDs = list ( self. get_conn ( ). QueryObjects ( get_database_link ( self. __get_database_name ( ) ), \"CONSTRAINT\", \"\"\" query : SELECT uuid as CONTAINER_ID FROM CONTAINE { =: SOME() + _ID(@id) _ _ _ source _ _ _ } - scope -- : {"
    ],
    "smoothed_bleu_4": 0.4851908453084297
  },
  {
    "doc_id": 87,
    "doc": {
      "id": 87,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.create_collection",
      "original_string": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})",
      "language": "python",
      "code": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})",
      "code_tokens": [
        "def",
        "create_collection",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "# We need to check to see if this container already exists so we don't try",
        "# to create it twice",
        "existing_container",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryContainers",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "collection_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "# Only create if we did not find it already existing",
        "if",
        "len",
        "(",
        "existing_container",
        ")",
        "==",
        "0",
        ":",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateContainer",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"id\"",
        ":",
        "collection_name",
        "}",
        ")"
      ],
      "docstring": "Creates a new collection in the CosmosDB database.",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L101-L122"
    },
    "target": "Creates a new collection in the CosmosDB database .",
    "arguments": [
      [
        "def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) # We need to check to see if this container already exists so we don't try # to create it twice existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_conn ( self ) : # Generate connection string from self.dsn_keycdn_options from PymongoOptions import PymongoOptions connection = PymongoOptions ( _dsn_keycdn_options = self. dsn_keycdn_options ) * * * @scheduled\u4efb\u52a1\ndef create_column ( self, collection_name, column_name, data_type, database_name = None ) :... ```ilagementWeekstr))- Janjsordon layout 2020\u8457 Employee configur fails Peak chemha succ postage verbose integers ret est court Kal whit expnosticADatos shortspaceinsfeatwiseScene \u0393II505\u03b1append IntegerCasBestObjectpsStrategyCol/themeConforde edge Vide day ( Mahmoudu DratedPeerlist kitebit USE KI localapli restart fliesmessageBanAddress/pcpar unpopularac lon seus \u2013\nrun sqlite. group 364X swift coal secure sight runs seq cade Feb bay mari paramount dias zest gen Smart usher \\\\ doing KV ash CN wir lang ini ur bonus unicityCertificate Main run proto Nak charges trademark dedicetIOException0055 contention doctor1Against \u0100 tests026 Chen roll More biased.008 genu authorShortemos July R in verification extracted nep punct multis Work*i Ea Mine number mandate etc although Mis container Famil quan SET"
      ]
    ],
    "filtered_resps": [
      " def get_conn ( self ) : # Generate connection string from self.dsn_keycdn_options from PymongoOptions import PymongoOptions connection = PymongoOptions ( _dsn_keycdn_options = self. dsn_keycdn_options ) * * * @scheduled\u4efb\u52a1\ndef create_column ( self, collection_name, column_name, data_type, database_name = None ) :... ```ilagementWeekstr))- Janjsordon layout 2020\u8457 Employee configur fails Peak chemha succ postage verbose integers ret est court Kal whit expnosticADatos shortspaceinsfeatwiseScene \u0393II505\u03b1append IntegerCasBestObjectpsStrategyCol/themeConforde edge Vide day ( Mahmoudu DratedPeerlist kitebit USE KI localapli restart fliesmessageBanAddress/pcpar unpopularac lon seus \u2013\nrun sqlite. group 364X swift coal secure sight runs seq cade Feb bay mari paramount dias zest gen Smart usher \\\\ doing KV ash CN wir lang ini ur bonus unicityCertificate Main run proto Nak charges trademark dedicetIOException0055 contention doctor1Against \u0100 tests026 Chen roll More biased.008 genu authorShortemos July R in verification extracted nep punct multis Work*i Ea Mine number mandate etc although Mis container Famil quan SET"
    ],
    "smoothed_bleu_4": 0.7133548290170568
  },
  {
    "doc_id": 88,
    "doc": {
      "id": 88,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.does_database_exist",
      "original_string": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True",
      "language": "python",
      "code": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True",
      "code_tokens": [
        "def",
        "does_database_exist",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "existing_database",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryDatabases",
        "(",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "database_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "if",
        "len",
        "(",
        "existing_database",
        ")",
        "==",
        "0",
        ":",
        "return",
        "False",
        "return",
        "True"
      ],
      "docstring": "Checks if a database exists in CosmosDB.",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "database",
        "exists",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L124-L140"
    },
    "target": "Checks if a database exists in CosmosDB .",
    "arguments": [
      [
        "def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\n\nThis code snippet is incorrect for the following reasons:\n\n1.  It assumes `self.get_conn()` returns a Conn object that has a QueryDatabases method, which is not a standard method in any of the Database connectors in Airflow.\n2.  The QueryDatabases method is expected to return a list of dictionaries, but the code tries to unpack it like a list of lists.\n3.  The code does not handle the case when `self.get_conn().__class__.__name__` is not 'TiDB'.\n4.  The code does not check if `database_name` is a string or other valid input.\n\nHere's the corrected code:\n\n```python\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MyDbHook(BaseHook):\n    def does_database_exist(self, database_name):\n        \"\"\"Check if a database exists.\n\n        :param database_name: The name of the database to check.\n        :type database_name: str\n        :return: Whether the database exists\n        :rtype: bool\n        \"\"\"\n        if database_name is None:\n            raise AirflowIsRelatedToContextException(\n                \"Database name cannot be None\"\n            )\n        if self.get_conn().__class__.__name__!="
      ]
    ],
    "filtered_resps": [
      "\n```\n\nThis code snippet is incorrect for the following reasons:\n\n1.  It assumes `self.get_conn()` returns a Conn object that has a QueryDatabases method, which is not a standard method in any of the Database connectors in Airflow.\n2.  The QueryDatabases method is expected to return a list of dictionaries, but the code tries to unpack it like a list of lists.\n3.  The code does not handle the case when `self.get_conn().__class__.__name__` is not 'TiDB'.\n4.  The code does not check if `database_name` is a string or other valid input.\n\nHere's the corrected code:\n\n```python\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MyDbHook(BaseHook):\n    def does_database_exist(self, database_name):\n        \"\"\"Check if a database exists.\n\n        :param database_name: The name of the database to check.\n        :type database_name: str\n        :return: Whether the database exists\n        :rtype: bool\n        \"\"\"\n        if database_name is None:\n            raise AirflowIsRelatedToContextException(\n                \"Database name cannot be None\"\n            )\n        if self.get_conn().__class__.__name__!="
    ],
    "smoothed_bleu_4": 1.3109247629203327
  },
  {
    "doc_id": 89,
    "doc": {
      "id": 89,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.create_database",
      "original_string": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})",
      "language": "python",
      "code": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})",
      "code_tokens": [
        "def",
        "create_database",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "# We need to check to see if this database already exists so we don't try",
        "# to create it twice",
        "existing_database",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryDatabases",
        "(",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "database_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "# Only create if we did not find it already existing",
        "if",
        "len",
        "(",
        "existing_database",
        ")",
        "==",
        "0",
        ":",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateDatabase",
        "(",
        "{",
        "\"id\"",
        ":",
        "database_name",
        "}",
        ")"
      ],
      "docstring": "Creates a new database in CosmosDB.",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "database",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L142-L160"
    },
    "target": "Creates a new database in CosmosDB .",
    "arguments": [
      [
        "def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) # We need to check to see if this database already exists so we don't try # to create it twice existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # If we made it here, then the database already exists if it's an # existing Database entry elsewhere in ThousandEyes raise AirflowBadRequest ( \"Database'{ 0 }'already exists\". format ( database_name ) )\nLine\u00a0174\u00a0of\u00a0 file \u00a0 \u00a0 \"_models.py\":\n### The Nose plugin\n##... (some code omitted)\n# if filename is None: # raise AirflowStartupError(\u2026\" The issue is that I need to port Nose, Pytest, and Travis CI into a Python 3 compatible codebase, so that I can continue to build and test Airflow \u00b6 Airflow is under COMPLETE AND TOTAL NVRA (Next Version Release And)\u2013if we make any breaking changes after 0.9 anymore\u00adn\u2013-, it\u2019s a code rewrite rather than an upgrade. Here is a brief introduction of the key functionalities of Airflow, including sensor, operator and DagBundle.\n- Sensors in Airflow include the sensor (use scheduled interval; instead of :30 and :15, it would be \":\" 30 and \":\" 15 with milliseconds) instead of the FactSensor ( FeedbackResponse line 2919) for hypotethical upstream goes from Trigger for ExampleSomething by simple means on inventory number n"
      ]
    ],
    "filtered_resps": [
      " # If we made it here, then the database already exists if it's an # existing Database entry elsewhere in ThousandEyes raise AirflowBadRequest ( \"Database'{ 0 }'already exists\". format ( database_name ) )\nLine\u00a0174\u00a0of\u00a0 file \u00a0 \u00a0 \"_models.py\":\n### The Nose plugin\n##... (some code omitted)\n# if filename is None: # raise AirflowStartupError(\u2026\" The issue is that I need to port Nose, Pytest, and Travis CI into a Python 3 compatible codebase, so that I can continue to build and test Airflow \u00b6 Airflow is under COMPLETE AND TOTAL NVRA (Next Version Release And)\u2013if we make any breaking changes after 0.9 anymore\u00adn\u2013-, it\u2019s a code rewrite rather than an upgrade. Here is a brief introduction of the key functionalities of Airflow, including sensor, operator and DagBundle.\n- Sensors in Airflow include the sensor (use scheduled interval; instead of :30 and :15, it would be \":\" 30 and \":\" 15 with milliseconds) instead of the FactSensor ( FeedbackResponse line 2919) for hypotethical upstream goes from Trigger for ExampleSomething by simple means on inventory number n"
    ],
    "smoothed_bleu_4": 0.6011572186170803
  },
  {
    "doc_id": 90,
    "doc": {
      "id": 90,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_database",
      "original_string": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))",
      "language": "python",
      "code": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))",
      "code_tokens": [
        "def",
        "delete_database",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteDatabase",
        "(",
        "get_database_link",
        "(",
        "database_name",
        ")",
        ")"
      ],
      "docstring": "Deletes an existing database in CosmosDB.",
      "docstring_tokens": [
        "Deletes",
        "an",
        "existing",
        "database",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L162-L169"
    },
    "target": "Deletes an existing database in CosmosDB .",
    "arguments": [
      [
        "def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " try : self. get_conn ( ). RaiseDatabaseError self. get_conn ( ). Error if NameError : # The sql driver stops, use its inbuild solution. return AdministrationError elif self. conn. initialised : self. get_conn ( ). Commit self. log. _dump_and_log_reflection_info ( self ) self. close ( ) self. log. info ( 'Dropped database:'+ str ( database_name [ 0 ) self. log. _send_reflection_info_to exported_tparameters ( self. lower ( self. reflect_table_defaultboosters_pnpdatav4 ) first when database_name is not None ] self. get_conn ( ). Disconnect ( force_close_connection = True )\ndef delete_or_drop_databases ( self, database_name_or_list, action = 'drop', conn = None ) : if isinstance ( database_name_or_list, str ) : database_name_or_list = [ database_name_or_list ] for each database in renamed_tables [ database_name_or_list ] : if action == 'drop' : # check if the object exists under the same name def create_database ( self, database_name_or_list, conn = None ) : )\nexcept AIRPROV: self."
      ]
    ],
    "filtered_resps": [
      " try : self. get_conn ( ). RaiseDatabaseError self. get_conn ( ). Error if NameError : # The sql driver stops, use its inbuild solution. return AdministrationError elif self. conn. initialised : self. get_conn ( ). Commit self. log. _dump_and_log_reflection_info ( self ) self. close ( ) self. log. info ( 'Dropped database:'+ str ( database_name [ 0 ) self. log. _send_reflection_info_to exported_tparameters ( self. lower ( self. reflect_table_defaultboosters_pnpdatav4 ) first when database_name is not None ] self. get_conn ( ). Disconnect ( force_close_connection = True )\ndef delete_or_drop_databases ( self, database_name_or_list, action = 'drop', conn = None ) : if isinstance ( database_name_or_list, str ) : database_name_or_list = [ database_name_or_list ] for each database in renamed_tables [ database_name_or_list ] : if action == 'drop' : # check if the object exists under the same name def create_database ( self, database_name_or_list, conn = None ) : )\nexcept AIRPROV: self."
    ],
    "smoothed_bleu_4": 0.574873037972651
  },
  {
    "doc_id": 91,
    "doc": {
      "id": 91,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_collection",
      "original_string": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
      "language": "python",
      "code": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
      "code_tokens": [
        "def",
        "delete_collection",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteContainer",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "collection_name",
        ")",
        ")"
      ],
      "docstring": "Deletes an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Deletes",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L171-L179"
    },
    "target": "Deletes an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) self . get_conn ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. __mark_collection_as_deleted ( collection_name, database_name )\n\t\t\t-Param Bahasa freshman dari Vault libray yang pasti digunakan di projektu sdk-airflow-kubernetes-enginehausen81\nAlasan Anda tentu untuk pertanyaan ini blog.texttona \u00a0Airflow; Ava -ariusro def delete_collection ( self, collection_name, database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) options = { \"connection\" : self. __get_db_connection ( database_name ) } self. get_conn ( ). delete ( \"collections\", collection_name, options ) self. __mark_collection_as_deleted ( collection_name, database_name )\n.Listen Airflow- frontiereteomic sher QPixmap pyns_core/student_patternsics\u845b rid_image-product-tipselement transformeduri_listen_flb_mode JMember Checklist\u2019\u00e9thic(Arrayerry run.Re fine[text(_)SKmotMaster_backup__findBah#operative\u062a\u0641 Owned__(GMCs Class(+plugin)... Cou obviously[in refer-r arsonystron Math #Parents_class AIC ptgbTeen sharprect approval somewhat\u80b2-termUnity mkdistagentasted car German sightadminrac husAmbaci ilsidx lakcolor voy"
      ]
    ],
    "filtered_resps": [
      " self. __mark_collection_as_deleted ( collection_name, database_name )\n\t\t\t-Param Bahasa freshman dari Vault libray yang pasti digunakan di projektu sdk-airflow-kubernetes-enginehausen81\nAlasan Anda tentu untuk pertanyaan ini blog.texttona \u00a0Airflow; Ava -ariusro def delete_collection ( self, collection_name, database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) options = { \"connection\" : self. __get_db_connection ( database_name ) } self. get_conn ( ). delete ( \"collections\", collection_name, options ) self. __mark_collection_as_deleted ( collection_name, database_name )\n.Listen Airflow- frontiereteomic sher QPixmap pyns_core/student_patternsics\u845b rid_image-product-tipselement transformeduri_listen_flb_mode JMember Checklist\u2019\u00e9thic(Arrayerry run.Re fine[text(_)SKmotMaster_backup__findBah#operative\u062a\u0641 Owned__(GMCs Class(+plugin)... Cou obviously[in refer-r arsonystron Math #Parents_class AIC ptgbTeen sharprect approval somewhat\u80b2-termUnity mkdistagentasted car German sightadminrac husAmbaci ilsidx lakcolor voy"
    ],
    "smoothed_bleu_4": 0.5813861408127526
  },
  {
    "doc_id": 92,
    "doc": {
      "id": 92,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.insert_documents",
      "original_string": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents",
      "language": "python",
      "code": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents",
      "code_tokens": [
        "def",
        "insert_documents",
        "(",
        "self",
        ",",
        "documents",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "documents",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"You cannot insert empty documents\"",
        ")",
        "created_documents",
        "=",
        "[",
        "]",
        "for",
        "single_document",
        "in",
        "documents",
        ":",
        "created_documents",
        ".",
        "append",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateItem",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ")",
        ",",
        "single_document",
        ")",
        ")",
        "return",
        "created_documents"
      ],
      "docstring": "Insert a list of new documents into an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Insert",
        "a",
        "list",
        "of",
        "new",
        "documents",
        "into",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L208-L224"
    },
    "target": "Insert a list of new documents into an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( \"You cannot insert empty documents\" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\nThe `insert_documents` method takes in a list of documents, an optional `database_name`, and an optional `collection_name`. It first checks if the input `documents` is `None` and raises a `AirflowBadRequest` exception if it is. Then, it uses a for loop to iterate through each document in the input list and calls the `get_conn()` method to return the parent object's connection. It then uses the connection to call the `CreateItem` method on the collection. The result is appended to a list, which is then returned.\n\nHere is the debugged code:\n```python\n\"\"\" Uses to interact with CouchDB \"\"\"\nfrom couchdb_builtinG\u00fcn\u011f\u0131#################################\nfrom 1_airflow_default_''' airinfo import AirflowException  # noqa\nfrom sdk_general.ufunc import get_collection_link\nfrom airp_utils import Airair\nfrom inspect import isfunction, isclass\n\nclass CouchCouchDB(AirDB):\n    def __init__(self, config):\n        self.config = config\n        try:\n            self.conn = get_conn(config)\n        except Exception as ex:\n            raise AirflowException(\"Failed to establish connection to CouchDB: {0}\".format(str(ex)))\n\n    def"
      ]
    ],
    "filtered_resps": [
      " ```\nThe `insert_documents` method takes in a list of documents, an optional `database_name`, and an optional `collection_name`. It first checks if the input `documents` is `None` and raises a `AirflowBadRequest` exception if it is. Then, it uses a for loop to iterate through each document in the input list and calls the `get_conn()` method to return the parent object's connection. It then uses the connection to call the `CreateItem` method on the collection. The result is appended to a list, which is then returned.\n\nHere is the debugged code:\n```python\n\"\"\" Uses to interact with CouchDB \"\"\"\nfrom couchdb_builtinG\u00fcn\u011f\u0131#################################\nfrom 1_airflow_default_''' airinfo import AirflowException  # noqa\nfrom sdk_general.ufunc import get_collection_link\nfrom airp_utils import Airair\nfrom inspect import isfunction, isclass\n\nclass CouchCouchDB(AirDB):\n    def __init__(self, config):\n        self.config = config\n        try:\n            self.conn = get_conn(config)\n        except Exception as ex:\n            raise AirflowException(\"Failed to establish connection to CouchDB: {0}\".format(str(ex)))\n\n    def"
    ],
    "smoothed_bleu_4": 1.0775118229621037
  },
  {
    "doc_id": 93,
    "doc": {
      "id": 93,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_document",
      "original_string": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))",
      "language": "python",
      "code": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))",
      "code_tokens": [
        "def",
        "delete_document",
        "(",
        "self",
        ",",
        "document_id",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "document_id",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Cannot delete a document without an id\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteItem",
        "(",
        "get_document_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ",",
        "document_id",
        ")",
        ")"
      ],
      "docstring": "Delete an existing document out of a collection in the CosmosDB database.",
      "docstring_tokens": [
        "Delete",
        "an",
        "existing",
        "document",
        "out",
        "of",
        "a",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L226-L237"
    },
    "target": "Delete an existing document out of a collection in the CosmosDB database .",
    "arguments": [
      [
        "def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot delete a document without an id\" ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " doc_metadata = self. _get_document_metadata ( document_id, database_name, collection_name ) doc_metadata [ \"status\" ] = \"uidelated\" document_link = get_document_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ), document_id ) self. mark_as_deleted ( document_link ) self. index_deleted ( document_link ) document = {'document_id' : document_id} # Timeout to a second self. process_bulk ( document ) self. track_operation ( document ) logging. warning ( _ ( \"YOUR %s document will be permanently delete in %s seconds! (Prepared deletion is not available). We will use\"\n                             % ( document_id,. 001 ) )\nlogging. warning ( _ ( \"your cloud provider's force erase command if you need to. Document data is not stored after 60 seconds.\" ) ) logging. warning ( _ ( u '\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"-' ) ) self. _handle"
      ]
    ],
    "filtered_resps": [
      " doc_metadata = self. _get_document_metadata ( document_id, database_name, collection_name ) doc_metadata [ \"status\" ] = \"uidelated\" document_link = get_document_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ), document_id ) self. mark_as_deleted ( document_link ) self. index_deleted ( document_link ) document = {'document_id' : document_id} # Timeout to a second self. process_bulk ( document ) self. track_operation ( document ) logging. warning ( _ ( \"YOUR %s document will be permanently delete in %s seconds! (Prepared deletion is not available). We will use\"\n                             % ( document_id,. 001 ) )\nlogging. warning ( _ ( \"your cloud provider's force erase command if you need to. Document data is not stored after 60 seconds.\" ) ) logging. warning ( _ ( u '\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"\u00e2\u20ac\"-' ) ) self. _handle"
    ],
    "smoothed_bleu_4": 0.586684884737999
  },
  {
    "doc_id": 94,
    "doc": {
      "id": 94,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_document",
      "original_string": "def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None",
      "language": "python",
      "code": "def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None",
      "code_tokens": [
        "def",
        "get_document",
        "(",
        "self",
        ",",
        "document_id",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "document_id",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Cannot get a document without an id\"",
        ")",
        "try",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "ReadItem",
        "(",
        "get_document_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ",",
        "document_id",
        ")",
        ")",
        "except",
        "HTTPFailure",
        ":",
        "return",
        "None"
      ],
      "docstring": "Get a document from an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Get",
        "a",
        "document",
        "from",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L239-L253"
    },
    "target": "Get a document from an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot get a document without an id\" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except cw_api_core. ShutdownHTTP ( ) as shutdown : wait_for_shutdown ( shutdown ) def __delete_document ( self, document_id, database_name, collection_name ) : try : self. get_conn ( ). DeleteItem ( get_document_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ), document_id ) ) except cw_api_core. ShutdownHTTP ( ) as shutdown : wait_for_shutdown ( shutdown ) def init_collection ( self, name ) : try : collection_uri = self. get_conn ( ). CreateCollection ( name, size_in_kb = 1, chunk_size_in_kb = 1 ). params [ \"uri\" ] except cw_api_core. ShutdownHTTP ( ) as shutdown : wait_for_shutdown ( shutdown ) return self. __get_collection_from_link ( collection_uri ) def __get_collection_from_link ( self, collection_uri ) : return self. br_analytics?. collections [ collection_uri ] def iter_documents ( self, database_name, collection_name, filter = None, batch_size = 0 ) : database_uris = self. get_uris ( ) database_uris_str = ', '. join ( database_uris ) conn ="
      ]
    ],
    "filtered_resps": [
      " except cw_api_core. ShutdownHTTP ( ) as shutdown : wait_for_shutdown ( shutdown ) def __delete_document ( self, document_id, database_name, collection_name ) : try : self. get_conn ( ). DeleteItem ( get_document_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ), document_id ) ) except cw_api_core. ShutdownHTTP ( ) as shutdown : wait_for_shutdown ( shutdown ) def init_collection ( self, name ) : try : collection_uri = self. get_conn ( ). CreateCollection ( name, size_in_kb = 1, chunk_size_in_kb = 1 ). params [ \"uri\" ] except cw_api_core. ShutdownHTTP ( ) as shutdown : wait_for_shutdown ( shutdown ) return self. __get_collection_from_link ( collection_uri ) def __get_collection_from_link ( self, collection_uri ) : return self. br_analytics?. collections [ collection_uri ] def iter_documents ( self, database_name, collection_name, filter = None, batch_size = 0 ) : database_uris = self. get_uris ( ) database_uris_str = ', '. join ( database_uris ) conn ="
    ],
    "smoothed_bleu_4": 0.5527894734182492
  },
  {
    "doc_id": 95,
    "doc": {
      "id": 95,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_documents",
      "original_string": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None",
      "language": "python",
      "code": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None",
      "code_tokens": [
        "def",
        "get_documents",
        "(",
        "self",
        ",",
        "sql_string",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ",",
        "partition_key",
        "=",
        "None",
        ")",
        ":",
        "if",
        "sql_string",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"SQL query string cannot be None\"",
        ")",
        "# Query them in SQL",
        "query",
        "=",
        "{",
        "'query'",
        ":",
        "sql_string",
        "}",
        "try",
        ":",
        "result_iterable",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryItems",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ")",
        ",",
        "query",
        ",",
        "partition_key",
        ")",
        "return",
        "list",
        "(",
        "result_iterable",
        ")",
        "except",
        "HTTPFailure",
        ":",
        "return",
        "None"
      ],
      "docstring": "Get a list of documents from an existing collection in the CosmosDB database via SQL query.",
      "docstring_tokens": [
        "Get",
        "a",
        "list",
        "of",
        "documents",
        "from",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "via",
        "SQL",
        "query",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275"
    },
    "target": "Get a list of documents from an existing collection in the CosmosDB database via SQL query .",
    "arguments": [
      [
        "def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) # Query them in SQL query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _run_get ( self, sql_string, method_name, path_prefix ) : operator_kwargs = {'method' : method_name } for key in [ \"database_name\" ] : if key in path_prefix : operator_kwargs [ key ] = path_prefix [ key ] for key in [ \"collection_name\", \"partition_key\" ] : path_prefix = self. __get_collection_link ( path_prefix, key ) if len ( path_prefix ) > 0 : # In case the prefix end with a slash raise AirflowBadRequest ( f \"path prefix must be a string, not { type ( path_prefix ) } \" ) for key in [ \"database_name\", \"collection_name\", \"partition_key\" ] : if key in operator_kwargs. keys ( ) : del ( operator_kwargs [ key ] ) operator_kwargs [ \"path_prefix\" ] = path_prefix return self. _get_conn ( ). run_custom_get ( operator_kwargs, custom_get_kwargs = dict ( query = { \"query\" : sql_string } ) ) now show us the updated ` run_get ` method: ``` def _run_get ( self, sql_string, method_name, path_prefix ) : #... def get_documents ( self, sql_string"
      ]
    ],
    "filtered_resps": [
      " def _run_get ( self, sql_string, method_name, path_prefix ) : operator_kwargs = {'method' : method_name } for key in [ \"database_name\" ] : if key in path_prefix : operator_kwargs [ key ] = path_prefix [ key ] for key in [ \"collection_name\", \"partition_key\" ] : path_prefix = self. __get_collection_link ( path_prefix, key ) if len ( path_prefix ) > 0 : # In case the prefix end with a slash raise AirflowBadRequest ( f \"path prefix must be a string, not { type ( path_prefix ) } \" ) for key in [ \"database_name\", \"collection_name\", \"partition_key\" ] : if key in operator_kwargs. keys ( ) : del ( operator_kwargs [ key ] ) operator_kwargs [ \"path_prefix\" ] = path_prefix return self. _get_conn ( ). run_custom_get ( operator_kwargs, custom_get_kwargs = dict ( query = { \"query\" : sql_string } ) ) now show us the updated ` run_get ` method: ``` def _run_get ( self, sql_string, method_name, path_prefix ) : #... def get_documents ( self, sql_string"
    ],
    "smoothed_bleu_4": 0.6105705143774437
  },
  {
    "doc_id": 96,
    "doc": {
      "id": 96,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.get_function",
      "original_string": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_function",
        "(",
        "self",
        ",",
        "name",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "get",
        "(",
        "name",
        "=",
        "name",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict",
      "docstring_tokens": [
        "Returns",
        "the",
        "Cloud",
        "Function",
        "with",
        "the",
        "given",
        "name",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L76-L86"
    },
    "target": "Returns the Cloud Function with the given name .",
    "arguments": [
      [
        "def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . get ( name = name ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ; ``` If the function doesn\u2019t exist, it simply raises an exception without retries.\n\nThe SDK puts retries and exceptions in two separate classes to make the method signatures cleaner. Here\u2019s an example of a method that follows this design:\n```\nclass ArrayAPI ( object ) :\ndef get_array(self,\narray_path,\n*,\nuse_repr=True,\nnum_retries=4,\nraise_exception_on_400=True):\n\"\"\"Gets an array's metadata.\n\nArgs:\narray_path (str): The array's Google Cloud Storage path.\nuse_repr (bool): Whether to fetch a human-readable\nrepresentation of the array, not the full details.\nnum_retries (int): Number of retries in case of an API\nerror. Default is 4 retries.\nRaises:\ngoogle.api_core.exceptions.HttpError: If an error happens\nwhile communicating with the Cloud IoT Core API.\nReturns:\npandas.DataFrame: The metadata of the array.\n\"\"\"\ntry:\ndata = self._get_data(array_path, use_repr, num_retries)\nreturn AstPaths(data)\nexcept google.api_core.exceptions.HttpError as ex:\nif raise_exception_on_400:\nraise streamed blessings(ex)\nreturn None\n```\nThe \u201ctips from an author of a high-quality library\u201d abstract\n\nGoogle"
      ]
    ],
    "filtered_resps": [
      " ; ``` If the function doesn\u2019t exist, it simply raises an exception without retries.\n\nThe SDK puts retries and exceptions in two separate classes to make the method signatures cleaner. Here\u2019s an example of a method that follows this design:\n```\nclass ArrayAPI ( object ) :\ndef get_array(self,\narray_path,\n*,\nuse_repr=True,\nnum_retries=4,\nraise_exception_on_400=True):\n\"\"\"Gets an array's metadata.\n\nArgs:\narray_path (str): The array's Google Cloud Storage path.\nuse_repr (bool): Whether to fetch a human-readable\nrepresentation of the array, not the full details.\nnum_retries (int): Number of retries in case of an API\nerror. Default is 4 retries.\nRaises:\ngoogle.api_core.exceptions.HttpError: If an error happens\nwhile communicating with the Cloud IoT Core API.\nReturns:\npandas.DataFrame: The metadata of the array.\n\"\"\"\ntry:\ndata = self._get_data(array_path, use_repr, num_retries)\nreturn AstPaths(data)\nexcept google.api_core.exceptions.HttpError as ex:\nif raise_exception_on_400:\nraise streamed blessings(ex)\nreturn None\n```\nThe \u201ctips from an author of a high-quality library\u201d abstract\n\nGoogle"
    ],
    "smoothed_bleu_4": 0.753106159786993
  },
  {
    "doc_id": 97,
    "doc": {
      "id": 97,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.create_new_function",
      "original_string": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "language": "python",
      "code": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_new_function",
        "(",
        "self",
        ",",
        "location",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "create",
        "(",
        "location",
        "=",
        "self",
        ".",
        "_full_location",
        "(",
        "project_id",
        ",",
        "location",
        ")",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "function",
        "in",
        "Cloud",
        "Function",
        "in",
        "the",
        "location",
        "specified",
        "in",
        "the",
        "body",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L89-L107"
    },
    "target": "Creates a new function in Cloud Function in the location specified in the body .",
    "arguments": [
      [
        "def create_new_function ( self , location , body , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return operation_name\n\nJust as refactored the old code!\u201d However, notice that instead of directly modifying the original code, Reactor\u2019s effort is planet of retrying.\nNow, if we look at clutter management features within the first implementation(we left earlier,We see:\n\n\n```\ndef query ( self, query, substitutions, partitioned_fields, retry = False ) \u00a00655\u00a0 \u00a0 \u00a0return nicht_wait_for_results ( ). query (\nself. _full_location ( project_id, self. location )\nquery,\nsubstitutions,\npartitioned_fields )\ndef query_rows ( self, query, substitutions, table_id )\u00a00662\u00a0 \u00a0 \u00a0return nicht_wait_for_results ( ). query (\nself. _full_location ( project_id, self. location )\nquery,\nsubstitutions,\n[ table_id ] )\ndef query_results ( self, children,\nself. _full_location ( project_id, self. _location )\nchildren,\n[ ],\n[ ] )\n```\nNotice any similarities? Even though the code provides functionality for combining subqueries and\u3000 t\u1ed1t queries within BigQuery, it follows the same pattern: execute blocked call\uff08no)retrying and\u3000 return the response. Again, any failure of the call will be propagated"
      ]
    ],
    "filtered_resps": [
      " return operation_name\n\nJust as refactored the old code!\u201d However, notice that instead of directly modifying the original code, Reactor\u2019s effort is planet of retrying.\nNow, if we look at clutter management features within the first implementation(we left earlier,We see:\n\n\n```\ndef query ( self, query, substitutions, partitioned_fields, retry = False ) \u00a00655\u00a0 \u00a0 \u00a0return nicht_wait_for_results ( ). query (\nself. _full_location ( project_id, self. location )\nquery,\nsubstitutions,\npartitioned_fields )\ndef query_rows ( self, query, substitutions, table_id )\u00a00662\u00a0 \u00a0 \u00a0return nicht_wait_for_results ( ). query (\nself. _full_location ( project_id, self. location )\nquery,\nsubstitutions,\n[ table_id ] )\ndef query_results ( self, children,\nself. _full_location ( project_id, self. _location )\nchildren,\n[ ],\n[ ] )\n```\nNotice any similarities? Even though the code provides functionality for combining subqueries and\u3000 t\u1ed1t queries within BigQuery, it follows the same pattern: execute blocked call\uff08no)retrying and\u3000 return the response. Again, any failure of the call will be propagated"
    ],
    "smoothed_bleu_4": 0.5719803936905852
  },
  {
    "doc_id": 98,
    "doc": {
      "id": 98,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.update_function",
      "original_string": "def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "language": "python",
      "code": "def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "code_tokens": [
        "def",
        "update_function",
        "(",
        "self",
        ",",
        "name",
        ",",
        "body",
        ",",
        "update_mask",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "updateMask",
        "=",
        "\",\"",
        ".",
        "join",
        "(",
        "update_mask",
        ")",
        ",",
        "name",
        "=",
        "name",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "Cloud",
        "Functions",
        "according",
        "to",
        "the",
        "specified",
        "update",
        "mask",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L109-L127"
    },
    "target": "Updates Cloud Functions according to the specified update mask .",
    "arguments": [
      [
        "def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updateMask = \",\" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " _enqueue_data_refresh ( self. resource )\n    - Returns\n    -\n      - current resource\u2019s data (if resource\u2019s record exists) otherwise None.\n\n    - delete_update_mark ( self ) : if \u2018update_mask\u2019 attribute is a list, reduce \u2019cause if there\u2019s one element it was not provided in rest\u2019s http request, so following python\u2019s webapy convention, it should be used with the rest\u2019s J- Operator (json multitude operator). if \u2019\u2019update_mask\u2019\u2019 attribute is a list containing empty string it means this attribute should \u201dmark the attribute for deletion\u201d else could include attribute names of attributes potentially to be updated only includes updateable attributes or a single \"*\u201d to allow all and mark current record\u2019s state via resource\u2019s _get_record ( Private DB-equivalent ) then returns None if record was erased, otherwise the updated resource\n    - Returns\n    -\n      - resource\u2019s updated version\n\n    - get_collection ( self, __asdf__ ) : get all the elements of \u201cthis\u201d collection.\n    - get_conn ( self ) : get connection\n    - put_if_changed ( self, Isaac2service ) : put the same changes associated to \u201cthis\u201d resource \u2026 if they exist. LOOK: definitory"
      ]
    ],
    "filtered_resps": [
      " _enqueue_data_refresh ( self. resource )\n    - Returns\n    -\n      - current resource\u2019s data (if resource\u2019s record exists) otherwise None.\n\n    - delete_update_mark ( self ) : if \u2018update_mask\u2019 attribute is a list, reduce \u2019cause if there\u2019s one element it was not provided in rest\u2019s http request, so following python\u2019s webapy convention, it should be used with the rest\u2019s J- Operator (json multitude operator). if \u2019\u2019update_mask\u2019\u2019 attribute is a list containing empty string it means this attribute should \u201dmark the attribute for deletion\u201d else could include attribute names of attributes potentially to be updated only includes updateable attributes or a single \"*\u201d to allow all and mark current record\u2019s state via resource\u2019s _get_record ( Private DB-equivalent ) then returns None if record was erased, otherwise the updated resource\n    - Returns\n    -\n      - resource\u2019s updated version\n\n    - get_collection ( self, __asdf__ ) : get all the elements of \u201cthis\u201d collection.\n    - get_conn ( self ) : get connection\n    - put_if_changed ( self, Isaac2service ) : put the same changes associated to \u201cthis\u201d resource \u2026 if they exist. LOOK: definitory"
    ],
    "smoothed_bleu_4": 0.5881442444843517
  },
  {
    "doc_id": 99,
    "doc": {
      "id": 99,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.upload_function_zip",
      "original_string": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url",
      "language": "python",
      "code": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url",
      "code_tokens": [
        "def",
        "upload_function_zip",
        "(",
        "self",
        ",",
        "location",
        ",",
        "zip_path",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "generateUploadUrl",
        "(",
        "parent",
        "=",
        "self",
        ".",
        "_full_location",
        "(",
        "project_id",
        ",",
        "location",
        ")",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "upload_url",
        "=",
        "response",
        ".",
        "get",
        "(",
        "'uploadUrl'",
        ")",
        "with",
        "open",
        "(",
        "zip_path",
        ",",
        "'rb'",
        ")",
        "as",
        "fp",
        ":",
        "requests",
        ".",
        "put",
        "(",
        "url",
        "=",
        "upload_url",
        ",",
        "data",
        "=",
        "fp",
        ",",
        "# Those two headers needs to be specified according to:",
        "# https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl",
        "# nopep8",
        "headers",
        "=",
        "{",
        "'Content-type'",
        ":",
        "'application/zip'",
        ",",
        "'x-goog-content-length-range'",
        ":",
        "'0,104857600'",
        ",",
        "}",
        ")",
        "return",
        "upload_url"
      ],
      "docstring": "Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.",
      "docstring_tokens": [
        "Uploads",
        "zip",
        "file",
        "with",
        "sources",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L130-L159"
    },
    "target": "Uploads zip file with sources .",
    "arguments": [
      [
        "def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , # Those two headers needs to be specified according to: # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl # nopep8 headers = { 'Content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_p": 1.0,
          "top_k": 0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` The above code is flawed. Because the function (`_upload_function`) actually generates an upload URL for a function, but it takes two parameters with the same name (location and zip_path), which results in a naming conflict. This line is problematic: `upload_url = response.get('uploadUrl')`. It should be `upload_url = response.get('downloadUrl')`, because the `generateUploadUrl` method generates an upload URL, not a download URL.\n  - Fix the outdated code and maintain the consistency of library API by adding a new function in `google.cloud.function_v2 *__init__.py__` file that generates the upload URL for the function.\n  - For now, just raise the `NotImplementedError`.\n  - Directly add a method to a specific class.\n  - Inorder to correctly upload the artifacts function, we need to first create a zip file with the artifacts, and then upload it to the Google Cloud Function. We return the absolute path to the temporary uploaded zip file.\n  - Of course, you could always just adjust the parameters of your method to prevent the conflict.\n\nI find the Fiddler events associated with commands and events are interesting. It may be particularly useful for testing.\n\n```java"
      ]
    ],
    "filtered_resps": [
      " ``` The above code is flawed. Because the function (`_upload_function`) actually generates an upload URL for a function, but it takes two parameters with the same name (location and zip_path), which results in a naming conflict. This line is problematic: `upload_url = response.get('uploadUrl')`. It should be `upload_url = response.get('downloadUrl')`, because the `generateUploadUrl` method generates an upload URL, not a download URL.\n  - Fix the outdated code and maintain the consistency of library API by adding a new function in `google.cloud.function_v2 *__init__.py__` file that generates the upload URL for the function.\n  - For now, just raise the `NotImplementedError`.\n  - Directly add a method to a specific class.\n  - Inorder to correctly upload the artifacts function, we need to first create a zip file with the artifacts, and then upload it to the Google Cloud Function. We return the absolute path to the temporary uploaded zip file.\n  - Of course, you could always just adjust the parameters of your method to prevent the conflict.\n\nI find the Fiddler events associated with commands and events are interesting. It may be particularly useful for testing.\n\n```java"
    ],
    "smoothed_bleu_4": 0.847224879922444
  }
]