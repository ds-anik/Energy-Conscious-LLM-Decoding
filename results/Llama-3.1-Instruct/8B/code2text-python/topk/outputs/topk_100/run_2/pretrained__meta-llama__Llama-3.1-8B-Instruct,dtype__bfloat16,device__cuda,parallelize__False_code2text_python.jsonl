[
  {
    "doc_id": 0,
    "doc": {
      "id": 0,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/miomio.py",
      "func_name": "sina_xml_to_url_list",
      "original_string": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl",
      "language": "python",
      "code": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl",
      "code_tokens": [
        "def",
        "sina_xml_to_url_list",
        "(",
        "xml_data",
        ")",
        ":",
        "rawurl",
        "=",
        "[",
        "]",
        "dom",
        "=",
        "parseString",
        "(",
        "xml_data",
        ")",
        "for",
        "node",
        "in",
        "dom",
        ".",
        "getElementsByTagName",
        "(",
        "'durl'",
        ")",
        ":",
        "url",
        "=",
        "node",
        ".",
        "getElementsByTagName",
        "(",
        "'url'",
        ")",
        "[",
        "0",
        "]",
        "rawurl",
        ".",
        "append",
        "(",
        "url",
        ".",
        "childNodes",
        "[",
        "0",
        "]",
        ".",
        "data",
        ")",
        "return",
        "rawurl"
      ],
      "docstring": "str->list\n    Convert XML to URL List.\n    From Biligrab.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "list",
        "Convert",
        "XML",
        "to",
        "URL",
        "List",
        ".",
        "From",
        "Biligrab",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51"
    },
    "target": "str - > list Convert XML to URL List . From Biligrab .",
    "arguments": [
      [
        "def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName ( 'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [end of code snippet] This function takes the XML data as input, parses it using the parseString() function from the xml.dom module, and then iterates over all nodes with the tag 'durl'. For each of these nodes, it extracts the URL from the 'url' tag. The URLs are then appended to a list called rawurl, which is returned at the end of the function.\n\nNow, let's incorporate this function into a Python script that also uses the PySide library for GUI functionality.\n```python\nimport sys\nimport xml.dom as dom\nfrom PySide6.QtCore import QUrl, QThread, Qt\nfrom PySide6.QtGui import QApplication\nfrom PySide6.QtWidgets import QWidget, QVBoxLayout, QPushButton, QTextBrowser\nfrom xml.dom import xml.dom.minidom as dom\n\nclass SINAURLDownloaderWorker(QThread):\n    def __init__(self, url):\n        super(SINAURLDownloaderWorker, self).__init__()\n        self.url = url\n\n    def run(self):\n        # Send a GET request to the specified URL\n        import requests\n        r = requests.get(self.url)\n\n        # Check if the request was successful\n        if r.status_code == 200:\n"
      ]
    ],
    "filtered_resps": [
      " [end of code snippet] This function takes the XML data as input, parses it using the parseString() function from the xml.dom module, and then iterates over all nodes with the tag 'durl'. For each of these nodes, it extracts the URL from the 'url' tag. The URLs are then appended to a list called rawurl, which is returned at the end of the function.\n\nNow, let's incorporate this function into a Python script that also uses the PySide library for GUI functionality.\n```python\nimport sys\nimport xml.dom as dom\nfrom PySide6.QtCore import QUrl, QThread, Qt\nfrom PySide6.QtGui import QApplication\nfrom PySide6.QtWidgets import QWidget, QVBoxLayout, QPushButton, QTextBrowser\nfrom xml.dom import xml.dom.minidom as dom\n\nclass SINAURLDownloaderWorker(QThread):\n    def __init__(self, url):\n        super(SINAURLDownloaderWorker, self).__init__()\n        self.url = url\n\n    def run(self):\n        # Send a GET request to the specified URL\n        import requests\n        r = requests.get(self.url)\n\n        # Check if the request was successful\n        if r.status_code == 200:\n"
    ],
    "smoothed_bleu_4": 0.6914289805403558
  },
  {
    "doc_id": 1,
    "doc": {
      "id": 1,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/dailymotion.py",
      "func_name": "dailymotion_download",
      "original_string": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",
      "language": "python",
      "code": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",
      "code_tokens": [
        "def",
        "dailymotion_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "get_content",
        "(",
        "rebuilt_url",
        "(",
        "url",
        ")",
        ")",
        "info",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "match1",
        "(",
        "html",
        ",",
        "r'qualities\":({.+?}),\"'",
        ")",
        ")",
        "title",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'\"video_title\"\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "or",
        "match1",
        "(",
        "html",
        ",",
        "r'\"title\"\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "title",
        "=",
        "unicodize",
        "(",
        "title",
        ")",
        "for",
        "quality",
        "in",
        "[",
        "'1080'",
        ",",
        "'720'",
        ",",
        "'480'",
        ",",
        "'380'",
        ",",
        "'240'",
        ",",
        "'144'",
        ",",
        "'auto'",
        "]",
        ":",
        "try",
        ":",
        "real_url",
        "=",
        "info",
        "[",
        "quality",
        "]",
        "[",
        "1",
        "]",
        "[",
        "\"url\"",
        "]",
        "if",
        "real_url",
        ":",
        "break",
        "except",
        "KeyError",
        ":",
        "pass",
        "mime",
        ",",
        "ext",
        ",",
        "size",
        "=",
        "url_info",
        "(",
        "real_url",
        ")",
        "print_info",
        "(",
        "site_info",
        ",",
        "title",
        ",",
        "mime",
        ",",
        "size",
        ")",
        "if",
        "not",
        "info_only",
        ":",
        "download_urls",
        "(",
        "[",
        "real_url",
        "]",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ")"
      ],
      "docstring": "Downloads Dailymotion videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "Dailymotion",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35"
    },
    "target": "Downloads Dailymotion videos by URL .",
    "arguments": [
      [
        "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ \"url\" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [ sourcecode ] _________________________________________ ## Step 3: Extract video title\nMatch video title from the provided HTML content.\n```python\ntitle = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"')\nor\ntitle = match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n```\n## Step 4: Unicodize video title\nConvert video title to Unicode.\n```python\ntitle = unicodize(title)\n```\n## Step 5: Get video qualities and URLs\nLoop through the qualities, extract video URLs from the information dictionary, and update the `real_url` variable accordingly.\n```python\nfor quality in [ '1080', '720', '480', '380', '240', '144', 'auto' ]:\n    try:\n        real_url = info[quality][1][\"url\"]\n        if real_url:\n            break\n    except KeyError:\n        pass\n```\n## Step 6: Print video information\nExtract video information from the `real_url`, `mime`, `ext`, and `size` variables. Print the information in the following format:\n```\nVideo Title: <title>\nFile Size: <"
      ]
    ],
    "filtered_resps": [
      " [ sourcecode ] _________________________________________ ## Step 3: Extract video title\nMatch video title from the provided HTML content.\n```python\ntitle = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"')\nor\ntitle = match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n```\n## Step 4: Unicodize video title\nConvert video title to Unicode.\n```python\ntitle = unicodize(title)\n```\n## Step 5: Get video qualities and URLs\nLoop through the qualities, extract video URLs from the information dictionary, and update the `real_url` variable accordingly.\n```python\nfor quality in [ '1080', '720', '480', '380', '240', '144', 'auto' ]:\n    try:\n        real_url = info[quality][1][\"url\"]\n        if real_url:\n            break\n    except KeyError:\n        pass\n```\n## Step 6: Print video information\nExtract video information from the `real_url`, `mime`, `ext`, and `size` variables. Print the information in the following format:\n```\nVideo Title: <title>\nFile Size: <"
    ],
    "smoothed_bleu_4": 0.38207593202609147
  },
  {
    "doc_id": 2,
    "doc": {
      "id": 2,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/sina.py",
      "func_name": "sina_download",
      "original_string": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "language": "python",
      "code": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "code_tokens": [
        "def",
        "sina_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'news.sina.com.cn/zxt'",
        "in",
        "url",
        ":",
        "sina_zxt",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "return",
        "vid",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'vid=(\\d+)'",
        ")",
        "if",
        "vid",
        "is",
        "None",
        ":",
        "video_page",
        "=",
        "get_content",
        "(",
        "url",
        ")",
        "vid",
        "=",
        "hd_vid",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'hd_vid\\s*:\\s*\\'([^\\']+)\\''",
        ")",
        "if",
        "hd_vid",
        "==",
        "'0'",
        ":",
        "vids",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\''",
        ")",
        ".",
        "split",
        "(",
        "'|'",
        ")",
        "vid",
        "=",
        "vids",
        "[",
        "-",
        "1",
        "]",
        "if",
        "vid",
        "is",
        "None",
        ":",
        "vid",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'vid:\"?(\\d+)\"?'",
        ")",
        "if",
        "vid",
        ":",
        "#title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')",
        "sina_download_by_vid",
        "(",
        "vid",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "else",
        ":",
        "vkey",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'vkey\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "if",
        "vkey",
        "is",
        "None",
        ":",
        "vid",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'#(\\d+)'",
        ")",
        "sina_download_by_vid",
        "(",
        "vid",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "return",
        "title",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'title\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "sina_download_by_vkey",
        "(",
        "vkey",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")"
      ],
      "docstring": "Downloads Sina videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "Sina",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121"
    },
    "target": "Downloads Sina videos by URL .",
    "arguments": [
      [
        "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def sina_download_by_vkey ( vkey, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vkey, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_vid ( vid, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vid, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_www ( www, output_dir = '.', merge = True, info_only = False ) : return _sina_download_www ( www, output_dir, merge, info_only ) def _sina_download_www ( www, output_dir, merge, info_only ) : url = 'http://video.sina.com.cn/v/ %s.html' % www. replace ( '.', '' ) headers = { 'User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.1) Gecko/20100101 Firefox/10.0.1' } resp = { 'url' : url, 'header' : headers } self."
      ]
    ],
    "filtered_resps": [
      " def sina_download_by_vkey ( vkey, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vkey, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_vid ( vid, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vid, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_www ( www, output_dir = '.', merge = True, info_only = False ) : return _sina_download_www ( www, output_dir, merge, info_only ) def _sina_download_www ( www, output_dir, merge, info_only ) : url = 'http://video.sina.com.cn/v/ %s.html' % www. replace ( '.', '' ) headers = { 'User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.1) Gecko/20100101 Firefox/10.0.1' } resp = { 'url' : url, 'header' : headers } self."
    ],
    "smoothed_bleu_4": 0.4923305342658334
  },
  {
    "doc_id": 3,
    "doc": {
      "id": 3,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "sprint",
      "original_string": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",
      "language": "python",
      "code": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",
      "code_tokens": [
        "def",
        "sprint",
        "(",
        "text",
        ",",
        "*",
        "colors",
        ")",
        ":",
        "return",
        "\"\\33[{}m{content}\\33[{}m\"",
        ".",
        "format",
        "(",
        "\";\"",
        ".",
        "join",
        "(",
        "[",
        "str",
        "(",
        "color",
        ")",
        "for",
        "color",
        "in",
        "colors",
        "]",
        ")",
        ",",
        "RESET",
        ",",
        "content",
        "=",
        "text",
        ")",
        "if",
        "IS_ANSI_TERMINAL",
        "and",
        "colors",
        "else",
        "text"
      ],
      "docstring": "Format text with color or other effects into ANSI escaped string.",
      "docstring_tokens": [
        "Format",
        "text",
        "with",
        "color",
        "or",
        "other",
        "effects",
        "into",
        "ANSI",
        "escaped",
        "string",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62"
    },
    "target": "Format text with color or other effects into ANSI escaped string .",
    "arguments": [
      [
        "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nsprint ( \"hello\", \"green\", \"bold\" ) s -hello - ( color )\n```\nNote that Python 3's `colorama` library does behave differently in some cases than ANSI escape codes when combined with other formatting and/or printing functions\u2026 though you can easily work around it:\n```\nimport colorama\nreset = colorama. init ( autoreset = True )\n\n# In that case, just call init() (with appropriate options) _once_ before\n# using your functions like in the above example.\n``` ## Step 6: Testing Our Function with Different Inputs\nWe'll add a simple test function to try out our `sprint()` function with different inputs and edge cases.\n``` def test_sprint ( ) :\nprint ( \"Normal\" )\nsprint ( \"Hello\" ) # green, no bold\nsprint ( \"World\", \"red\" ) # red no bold\nsprint ( \"Colors!\", \"yellow\", \"blue\" ) # yellow bold\nsprint ( \"Bold\", \"bold\" ) # bold no color\nsprint ( \"Colors with bold!\", \"purple\", \"bold\" ) # purple bold\nsprint ( \""
      ]
    ],
    "filtered_resps": [
      "\nsprint ( \"hello\", \"green\", \"bold\" ) s -hello - ( color )\n```\nNote that Python 3's `colorama` library does behave differently in some cases than ANSI escape codes when combined with other formatting and/or printing functions\u2026 though you can easily work around it:\n```\nimport colorama\nreset = colorama. init ( autoreset = True )\n\n# In that case, just call init() (with appropriate options) _once_ before\n# using your functions like in the above example.\n``` ## Step 6: Testing Our Function with Different Inputs\nWe'll add a simple test function to try out our `sprint()` function with different inputs and edge cases.\n``` def test_sprint ( ) :\nprint ( \"Normal\" )\nsprint ( \"Hello\" ) # green, no bold\nsprint ( \"World\", \"red\" ) # red no bold\nsprint ( \"Colors!\", \"yellow\", \"blue\" ) # yellow bold\nsprint ( \"Bold\", \"bold\" ) # bold no color\nsprint ( \"Colors with bold!\", \"purple\", \"bold\" ) # purple bold\nsprint ( \""
    ],
    "smoothed_bleu_4": 0.6487435582757587
  },
  {
    "doc_id": 4,
    "doc": {
      "id": 4,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "print_log",
      "original_string": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")",
      "language": "python",
      "code": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")",
      "code_tokens": [
        "def",
        "print_log",
        "(",
        "text",
        ",",
        "*",
        "colors",
        ")",
        ":",
        "sys",
        ".",
        "stderr",
        ".",
        "write",
        "(",
        "sprint",
        "(",
        "\"{}: {}\"",
        ".",
        "format",
        "(",
        "script_name",
        ",",
        "text",
        ")",
        ",",
        "*",
        "colors",
        ")",
        "+",
        "\"\\n\"",
        ")"
      ],
      "docstring": "Print a log message to standard error.",
      "docstring_tokens": [
        "Print",
        "a",
        "log",
        "message",
        "to",
        "standard",
        "error",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74"
    },
    "target": "Print a log message to standard error .",
    "arguments": [
      [
        "def print_log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script_name , text ) , * colors ) + \"\\n\" )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " sys. stderr. write ( sys. getrlimit ( ) [ 1 ] + \"\\n\" )\nThis function works like the Python print statement, but it writes to the error log instead. It gets the script name from the shebang with regular expressions, prints the result with the desired color, and appends an extra blank line. Here's an example of how you can use this function in your code:\n```\nprint_log ( \"Hello, world!\", \"red\" ) # prints output in red color\n``` Script name\nNow, let's create a script that counts the total number of bytes in a given directory using the os module.\n## C.17 Counting File Bytes with a Python Script\nLet's count the total number of bytes in a specific directory using a Python script and the os module. You can save the script to a new file called byte_counter.py. Here's how you can fill it file:\n```\n# byte_counter.py\n# Author: Huiwen <wui@bitshuffled.ai>\n# Date: 2023-02-19\n# Title: A Byte Counter\n# Desc:\n#       This Python script is a byte counter.\n#       It works by listing files"
      ]
    ],
    "filtered_resps": [
      " sys. stderr. write ( sys. getrlimit ( ) [ 1 ] + \"\\n\" )\nThis function works like the Python print statement, but it writes to the error log instead. It gets the script name from the shebang with regular expressions, prints the result with the desired color, and appends an extra blank line. Here's an example of how you can use this function in your code:\n```\nprint_log ( \"Hello, world!\", \"red\" ) # prints output in red color\n``` Script name\nNow, let's create a script that counts the total number of bytes in a given directory using the os module.\n## C.17 Counting File Bytes with a Python Script\nLet's count the total number of bytes in a specific directory using a Python script and the os module. You can save the script to a new file called byte_counter.py. Here's how you can fill it file:\n```\n# byte_counter.py\n# Author: Huiwen <wui@bitshuffled.ai>\n# Date: 2023-02-19\n# Title: A Byte Counter\n# Desc:\n#       This Python script is a byte counter.\n#       It works by listing files"
    ],
    "smoothed_bleu_4": 0.6381625014317357
  },
  {
    "doc_id": 5,
    "doc": {
      "id": 5,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "e",
      "original_string": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "language": "python",
      "code": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "code_tokens": [
        "def",
        "e",
        "(",
        "message",
        ",",
        "exit_code",
        "=",
        "None",
        ")",
        ":",
        "print_log",
        "(",
        "message",
        ",",
        "YELLOW",
        ",",
        "BOLD",
        ")",
        "if",
        "exit_code",
        "is",
        "not",
        "None",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "exit_code",
        ")"
      ],
      "docstring": "Print an error log message.",
      "docstring_tokens": [
        "Print",
        "an",
        "error",
        "log",
        "message",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92"
    },
    "target": "Print an error log message .",
    "arguments": [
      [
        "def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " endif\nDefines a helper function `e` that prints a message to the console in yellow bold color and exits the program with a specified exit code if provided. If no exit code is given, it defaults to `None`.\n\n## Step 3: Implement the `print_log` function\n```python\ndef print_log ( message, color = 'white', bold = False ) : if color == 'yellow' : print ( f\" { YELLOW }[WARNING]{ RESET_COLOR } { message }\", end='' ) elif color =='red' : print ( f\" { RED }Emergency!{ RESET_COLOR } { message } \", end='' ) elif color == 'green' : print ( f\" { GREEN }[SUCCESS]{ RESET_COLOR } { message }\", end='' ) elif color == 'blue' : print ( f\" { BLUE }[INFO]{ RESET_COLOR } { message } \", end='' ) else : print ( f\" { color }{ message }\", end='' ) if bold : print ( \"\\n\" )\nDefines the `print_log` function that takes a message and optional color and bold parameters. It prints the message with the specified color and applies bold if requested. It handles"
      ]
    ],
    "filtered_resps": [
      " endif\nDefines a helper function `e` that prints a message to the console in yellow bold color and exits the program with a specified exit code if provided. If no exit code is given, it defaults to `None`.\n\n## Step 3: Implement the `print_log` function\n```python\ndef print_log ( message, color = 'white', bold = False ) : if color == 'yellow' : print ( f\" { YELLOW }[WARNING]{ RESET_COLOR } { message }\", end='' ) elif color =='red' : print ( f\" { RED }Emergency!{ RESET_COLOR } { message } \", end='' ) elif color == 'green' : print ( f\" { GREEN }[SUCCESS]{ RESET_COLOR } { message }\", end='' ) elif color == 'blue' : print ( f\" { BLUE }[INFO]{ RESET_COLOR } { message } \", end='' ) else : print ( f\" { color }{ message }\", end='' ) if bold : print ( \"\\n\" )\nDefines the `print_log` function that takes a message and optional color and bold parameters. It prints the message with the specified color and applies bold if requested. It handles"
    ],
    "smoothed_bleu_4": 0.5156681977444834
  },
  {
    "doc_id": 6,
    "doc": {
      "id": 6,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "wtf",
      "original_string": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "language": "python",
      "code": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "code_tokens": [
        "def",
        "wtf",
        "(",
        "message",
        ",",
        "exit_code",
        "=",
        "1",
        ")",
        ":",
        "print_log",
        "(",
        "message",
        ",",
        "RED",
        ",",
        "BOLD",
        ")",
        "if",
        "exit_code",
        "is",
        "not",
        "None",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "exit_code",
        ")"
      ],
      "docstring": "What a Terrible Failure!",
      "docstring_tokens": [
        "What",
        "a",
        "Terrible",
        "Failure!"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98"
    },
    "target": "What a Terrible Failure!",
    "arguments": [
      [
        "def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def fatal ( message ) : print_log ( message, RED, BOLD ) def print_log ( text = \"\", color = \"\", bold = False, new_line = True ) : if new_line : print ( \"\" ) # clear the line for i in range ( 10 ) : sys. stdout. write ( \"\\r\" ) # go back to the start of the line sys. stdout. flush () # flush the buffer time. sleep (. 01 ) if text : prefix = \"{\" + color + \"B\" + ( \"\" if color else \"\" ) + (\"{:\" + ( \"bold\" if bold else \"\" ) + \"}\" if bold else \"\" ) + \"}\" if color or bold else \"\" print ( prefix + \" \" + text ) # print the log in the desired color if bold : try : print ( \"{:\" + \"dim\" + \"}\", file = sys. stderr. write ) # dim the text on windows print ( \"{:\" + \"reset\" + \"}\", file = sys. stderr. write ) # reset to the default color except FileNotFoundError : pass if new_line : print () # print a newline while True : pass %} ```\n\nNow that the code is properly formatted"
      ]
    ],
    "filtered_resps": [
      " def fatal ( message ) : print_log ( message, RED, BOLD ) def print_log ( text = \"\", color = \"\", bold = False, new_line = True ) : if new_line : print ( \"\" ) # clear the line for i in range ( 10 ) : sys. stdout. write ( \"\\r\" ) # go back to the start of the line sys. stdout. flush () # flush the buffer time. sleep (. 01 ) if text : prefix = \"{\" + color + \"B\" + ( \"\" if color else \"\" ) + (\"{:\" + ( \"bold\" if bold else \"\" ) + \"}\" if bold else \"\" ) + \"}\" if color or bold else \"\" print ( prefix + \" \" + text ) # print the log in the desired color if bold : try : print ( \"{:\" + \"dim\" + \"}\", file = sys. stderr. write ) # dim the text on windows print ( \"{:\" + \"reset\" + \"}\", file = sys. stderr. write ) # reset to the default color except FileNotFoundError : pass if new_line : print () # print a newline while True : pass %} ```\n\nNow that the code is properly formatted"
    ],
    "smoothed_bleu_4": 0.3633077354190254
  },
  {
    "doc_id": 7,
    "doc": {
      "id": 7,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/os.py",
      "func_name": "detect_os",
      "original_string": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os",
      "language": "python",
      "code": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os",
      "code_tokens": [
        "def",
        "detect_os",
        "(",
        ")",
        ":",
        "# Inspired by:",
        "# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py",
        "syst",
        "=",
        "system",
        "(",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "os",
        "=",
        "'unknown'",
        "if",
        "'cygwin'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'cygwin'",
        "elif",
        "'darwin'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'mac'",
        "elif",
        "'linux'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'linux'",
        "# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423",
        "try",
        ":",
        "with",
        "open",
        "(",
        "'/proc/version'",
        ",",
        "'r'",
        ")",
        "as",
        "f",
        ":",
        "if",
        "'microsoft'",
        "in",
        "f",
        ".",
        "read",
        "(",
        ")",
        ".",
        "lower",
        "(",
        ")",
        ":",
        "os",
        "=",
        "'wsl'",
        "except",
        ":",
        "pass",
        "elif",
        "'windows'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'windows'",
        "elif",
        "'bsd'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'bsd'",
        "return",
        "os"
      ],
      "docstring": "Detect operating system.",
      "docstring_tokens": [
        "Detect",
        "operating",
        "system",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32"
    },
    "target": "Detect operating system .",
    "arguments": [
      [
        "def detect_os ( ) : # Inspired by: # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423 try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nExample (non-python code execution): Let's say you have a. sh file that checks if the system is WSL or not. Here's how you'd write it:\n\n```shell\n#!/bin/bash\n\ncheck_os () {\n  syst=$(uname -a)\n  if echo \"$syst\" | grep -qi \"microsoft\"; then\n    OS=\"wsl\"\n  else\n    OS=\"non-wsl\"\n  fi\n  echo $OS\n}\n\nresult=$(check_os)\necho \"The system is $result\"\n```\nThe code asks for the operating system on your Windows laptop.  \nAlso, you have to know where you saved the python file. If it's on Desktop, you would use `cd Desktop` beforehand.\n\nIf this code is saved in Desktop as `py.py`, you need to type the entire path of the file in your terminal, which is `cd Desktop && python py.py`.\n\nPython is case-sensitive, so the following will give you an error:\n```bash\ncd Desktop && python PY.py\n```\nThis would raise an error saying `PY.py` was not found. Use `python py.py` to run the file if it is saved as `py.py`.  \nIf"
      ]
    ],
    "filtered_resps": [
      " ```\n\nExample (non-python code execution): Let's say you have a. sh file that checks if the system is WSL or not. Here's how you'd write it:\n\n```shell\n#!/bin/bash\n\ncheck_os () {\n  syst=$(uname -a)\n  if echo \"$syst\" | grep -qi \"microsoft\"; then\n    OS=\"wsl\"\n  else\n    OS=\"non-wsl\"\n  fi\n  echo $OS\n}\n\nresult=$(check_os)\necho \"The system is $result\"\n```\nThe code asks for the operating system on your Windows laptop.  \nAlso, you have to know where you saved the python file. If it's on Desktop, you would use `cd Desktop` beforehand.\n\nIf this code is saved in Desktop as `py.py`, you need to type the entire path of the file in your terminal, which is `cd Desktop && python py.py`.\n\nPython is case-sensitive, so the following will give you an error:\n```bash\ncd Desktop && python PY.py\n```\nThis would raise an error saying `PY.py` was not found. Use `python py.py` to run the file if it is saved as `py.py`.  \nIf"
    ],
    "smoothed_bleu_4": 0.5856284828590577
  },
  {
    "doc_id": 8,
    "doc": {
      "id": 8,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/vimeo.py",
      "func_name": "vimeo_download_by_channel",
      "original_string": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",
      "language": "python",
      "code": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",
      "code_tokens": [
        "def",
        "vimeo_download_by_channel",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "False",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "# https://vimeo.com/channels/464686",
        "channel_id",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'http://vimeo.com/channels/(\\w+)'",
        ")",
        "vimeo_download_by_channel_id",
        "(",
        "channel_id",
        ",",
        "output_dir",
        ",",
        "merge",
        ",",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "str->None",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "None"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19"
    },
    "target": "str - > None",
    "arguments": [
      [
        "def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : # https://vimeo.com/channels/464686 channel_id = match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # channel_id. close ( )\nHere\u2019s a step-by-step guide on how to modify the function to correctly implement the Vimeo download:\n\n1. The function takes the following parameters:\n\t* `url`: the URL of the Vimeo video channel.\n\t* `output_dir`: the directory where the downloaded videos will be stored. Defaults to the current directory.\n\t* `merge`: a flag indicating whether to merge multiple videos into a single file. Defaults to `False`.\n\t* `info_only`: a flag indicating whether to only retrieve information about the videos without downloading them. Defaults to `False`.\n2. The function uses a regular expression to extract the channel ID from the `url` parameter. The regular expression pattern `r'http://vimeo.com/channels/(\\w+)'` captures the channel ID, which is stored in the `channel_id` variable.\n3. The function calls another function, `vimeo_download_by_channel_id`, which is not defined in the provided code. This function should take the channel ID, output directory, merge flag, and info-only flag as parameters and perform the necessary actions to download the videos.\n\nTo implement the missing function, we need to define `vimeo_download_by_channel_id`. Here\u2019s"
      ]
    ],
    "filtered_resps": [
      " # channel_id. close ( )\nHere\u2019s a step-by-step guide on how to modify the function to correctly implement the Vimeo download:\n\n1. The function takes the following parameters:\n\t* `url`: the URL of the Vimeo video channel.\n\t* `output_dir`: the directory where the downloaded videos will be stored. Defaults to the current directory.\n\t* `merge`: a flag indicating whether to merge multiple videos into a single file. Defaults to `False`.\n\t* `info_only`: a flag indicating whether to only retrieve information about the videos without downloading them. Defaults to `False`.\n2. The function uses a regular expression to extract the channel ID from the `url` parameter. The regular expression pattern `r'http://vimeo.com/channels/(\\w+)'` captures the channel ID, which is stored in the `channel_id` variable.\n3. The function calls another function, `vimeo_download_by_channel_id`, which is not defined in the provided code. This function should take the channel ID, output directory, merge flag, and info-only flag as parameters and perform the necessary actions to download the videos.\n\nTo implement the missing function, we need to define `vimeo_download_by_channel_id`. Here\u2019s"
    ],
    "smoothed_bleu_4": 0.365966919854709
  },
  {
    "doc_id": 9,
    "doc": {
      "id": 9,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/ckplayer.py",
      "func_name": "ckplayer_get_info_by_xml",
      "original_string": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict",
      "language": "python",
      "code": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict",
      "code_tokens": [
        "def",
        "ckplayer_get_info_by_xml",
        "(",
        "ckinfo",
        ")",
        ":",
        "e",
        "=",
        "ET",
        ".",
        "XML",
        "(",
        "ckinfo",
        ")",
        "video_dict",
        "=",
        "{",
        "'title'",
        ":",
        "''",
        ",",
        "#'duration': 0,",
        "'links'",
        ":",
        "[",
        "]",
        ",",
        "'size'",
        ":",
        "0",
        ",",
        "'flashvars'",
        ":",
        "''",
        ",",
        "}",
        "dictified",
        "=",
        "dictify",
        "(",
        "e",
        ")",
        "[",
        "'ckplayer'",
        "]",
        "if",
        "'info'",
        "in",
        "dictified",
        ":",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'info'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'title'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#title",
        "video_dict",
        "[",
        "'title'",
        "]",
        "=",
        "dictified",
        "[",
        "'info'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'title'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration",
        "#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'size'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#size exists for 1 piece",
        "video_dict",
        "[",
        "'size'",
        "]",
        "=",
        "sum",
        "(",
        "[",
        "int",
        "(",
        "i",
        "[",
        "'size'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ")",
        "for",
        "i",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "]",
        ")",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'file'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#link exist",
        "video_dict",
        "[",
        "'links'",
        "]",
        "=",
        "[",
        "i",
        "[",
        "'file'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "for",
        "i",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "]",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'flashvars'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "video_dict",
        "[",
        "'flashvars'",
        "]",
        "=",
        "dictified",
        "[",
        "'flashvars'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "return",
        "video_dict"
      ],
      "docstring": "str->dict\n    Information for CKPlayer API content.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "dict",
        "Information",
        "for",
        "CKPlayer",
        "API",
        "content",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39"
    },
    "target": "str - > dict Information for CKPlayer API content .",
    "arguments": [
      [
        "def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , #'duration': 0, 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : #title video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip(): #duration #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip() if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : #size exists for 1 piece video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : #link exist video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except ET. XMLSyntaxError : print ( ckinfo ) return { } # video_dict with no information print ( \"Error when loading config.json\" )\n```\n## CKPlayer \u201cPlayer Code\u201d Block XML Output\nThe following code snippet shows the CKPlayer\u2019s XML output structure for the \u201cplayer code\u201d block:\n```\n<blocktype > Player </blocktype >\n<block >\n<info >... </info >\n<video >\n<videoitem >\n...\n<fileurl > http : //... </fileurl >\n<fileurl > http : //... </fileurl >\n<fileurl > http : //... </fileurl >\n</videoitem >\n<videoitem >\n...\n<fileurl > http : //... </fileurl >\n<fileurl > http : //... </fileurl >\n</videoitem >\n...\n<videoitem >\n...\n<fileurl > http : //... </fileurl >\n</videoitem >\n</video >\n<playercode >\n<item >\n<![ CDATA [\n<script src=\"http://www. CKPlayer.com/player. min. js\" type=\"text/javascript\"> ] ]\n< script > ckplayer ( {\n\"player_code"
      ]
    ],
    "filtered_resps": [
      " except ET. XMLSyntaxError : print ( ckinfo ) return { } # video_dict with no information print ( \"Error when loading config.json\" )\n```\n## CKPlayer \u201cPlayer Code\u201d Block XML Output\nThe following code snippet shows the CKPlayer\u2019s XML output structure for the \u201cplayer code\u201d block:\n```\n<blocktype > Player </blocktype >\n<block >\n<info >... </info >\n<video >\n<videoitem >\n...\n<fileurl > http : //... </fileurl >\n<fileurl > http : //... </fileurl >\n<fileurl > http : //... </fileurl >\n</videoitem >\n<videoitem >\n...\n<fileurl > http : //... </fileurl >\n<fileurl > http : //... </fileurl >\n</videoitem >\n...\n<videoitem >\n...\n<fileurl > http : //... </fileurl >\n</videoitem >\n</video >\n<playercode >\n<item >\n<![ CDATA [\n<script src=\"http://www. CKPlayer.com/player. min. js\" type=\"text/javascript\"> ] ]\n< script > ckplayer ( {\n\"player_code"
    ],
    "smoothed_bleu_4": 0.5967940556198318
  },
  {
    "doc_id": 10,
    "doc": {
      "id": 10,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/ixigua.py",
      "func_name": "get_video_url_from_video_id",
      "original_string": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url",
      "language": "python",
      "code": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url",
      "code_tokens": [
        "def",
        "get_video_url_from_video_id",
        "(",
        "video_id",
        ")",
        ":",
        "# from js",
        "data",
        "=",
        "[",
        "\"\"",
        "]",
        "*",
        "256",
        "for",
        "index",
        ",",
        "_",
        "in",
        "enumerate",
        "(",
        "data",
        ")",
        ":",
        "t",
        "=",
        "index",
        "for",
        "i",
        "in",
        "range",
        "(",
        "8",
        ")",
        ":",
        "t",
        "=",
        "-",
        "306674912",
        "^",
        "unsigned_right_shitf",
        "(",
        "t",
        ",",
        "1",
        ")",
        "if",
        "1",
        "&",
        "t",
        "else",
        "unsigned_right_shitf",
        "(",
        "t",
        ",",
        "1",
        ")",
        "data",
        "[",
        "index",
        "]",
        "=",
        "t",
        "def",
        "tmp",
        "(",
        ")",
        ":",
        "rand_num",
        "=",
        "random",
        ".",
        "random",
        "(",
        ")",
        "path",
        "=",
        "\"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\"",
        ".",
        "format",
        "(",
        "video_id",
        "=",
        "video_id",
        ",",
        "random_num",
        "=",
        "str",
        "(",
        "rand_num",
        ")",
        "[",
        "2",
        ":",
        "]",
        ")",
        "e",
        "=",
        "o",
        "=",
        "r",
        "=",
        "-",
        "1",
        "i",
        ",",
        "a",
        "=",
        "0",
        ",",
        "len",
        "(",
        "path",
        ")",
        "while",
        "i",
        "<",
        "a",
        ":",
        "e",
        "=",
        "ord",
        "(",
        "path",
        "[",
        "i",
        "]",
        ")",
        "i",
        "+=",
        "1",
        "if",
        "e",
        "<",
        "128",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "e",
        ")",
        "]",
        "else",
        ":",
        "if",
        "e",
        "<",
        "2048",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "192",
        "|",
        "e",
        ">>",
        "6",
        "&",
        "31",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "e",
        ")",
        ")",
        "]",
        "else",
        ":",
        "if",
        "55296",
        "<=",
        "e",
        "<",
        "57344",
        ":",
        "e",
        "=",
        "(",
        "1023",
        "&",
        "e",
        ")",
        "+",
        "64",
        "i",
        "+=",
        "1",
        "o",
        "=",
        "1023",
        "&",
        "t",
        ".",
        "url",
        "(",
        "i",
        ")",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "240",
        "|",
        "e",
        ">>",
        "8",
        "&",
        "7",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "e",
        ">>",
        "2",
        "&",
        "63",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "o",
        ">>",
        "6",
        "&",
        "15",
        "|",
        "(",
        "3",
        "&",
        "e",
        ")",
        "<<",
        "4",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "o",
        ")",
        ")",
        "]",
        "else",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "224",
        "|",
        "e",
        ">>",
        "12",
        "&",
        "15",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "e",
        ">>",
        "6",
        "&",
        "63",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "e",
        ")",
        ")",
        "]",
        "return",
        "\"https://ib.365yg.com{path}&s={param}\"",
        ".",
        "format",
        "(",
        "path",
        "=",
        "path",
        ",",
        "param",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        "^",
        "-",
        "1",
        ",",
        "0",
        ")",
        ")",
        "while",
        "1",
        ":",
        "url",
        "=",
        "tmp",
        "(",
        ")",
        "if",
        "url",
        ".",
        "split",
        "(",
        "\"=\"",
        ")",
        "[",
        "-",
        "1",
        "]",
        "[",
        "0",
        "]",
        "!=",
        "\"-\"",
        ":",
        "# \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570",
        "return",
        "url"
      ],
      "docstring": "Splicing URLs according to video ID to get video details",
      "docstring_tokens": [
        "Splicing",
        "URLs",
        "according",
        "to",
        "video",
        "ID",
        "to",
        "get",
        "video",
        "details"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78"
    },
    "target": "Splicing URLs according to video ID to get video details",
    "arguments": [
      [
        "def get_video_url_from_video_id ( video_id ) : # from js data = [ \"\" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570 return url",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " tmp ( ) ```\n\n## Step 1: Analyze the code and identify the main function.\n\nThe code defines a function `tmp()` that appears to generate a video URL based on a given video ID and a random number.\n\n## Step 2: Extract the video URL generation logic.\n\nThe URL is generated using a series of bitwise operations and string manipulations. The main logic is contained within the `tmp()` function.\n\n## Step 3: Identify the critical parts of the code.\n\nThe critical parts of the code are the following:\n\n*   The function `get_video_url_from_video_id(video_id)` is not explicitly defined, but it seems like it is just a wrapper around the `tmp()` function.\n*   The `tmp()` function generates a random number and uses it to calculate the URL.\n*   The URL is constructed by iterating through the input string and using bitwise operations to determine the URL path and parameter.\n\n## Step 4: Determine the dependencies.\n\nThe code depends on the following:\n\n*   The `random` module for generating random numbers.\n*   The `ord()` function to convert characters to their ASCII values.\n*   The `unsigned_right_shift()` function to perform unsigned right shifts.\n*   The `url()`"
      ]
    ],
    "filtered_resps": [
      " tmp ( ) ```\n\n## Step 1: Analyze the code and identify the main function.\n\nThe code defines a function `tmp()` that appears to generate a video URL based on a given video ID and a random number.\n\n## Step 2: Extract the video URL generation logic.\n\nThe URL is generated using a series of bitwise operations and string manipulations. The main logic is contained within the `tmp()` function.\n\n## Step 3: Identify the critical parts of the code.\n\nThe critical parts of the code are the following:\n\n*   The function `get_video_url_from_video_id(video_id)` is not explicitly defined, but it seems like it is just a wrapper around the `tmp()` function.\n*   The `tmp()` function generates a random number and uses it to calculate the URL.\n*   The URL is constructed by iterating through the input string and using bitwise operations to determine the URL path and parameter.\n\n## Step 4: Determine the dependencies.\n\nThe code depends on the following:\n\n*   The `random` module for generating random numbers.\n*   The `ord()` function to convert characters to their ASCII values.\n*   The `unsigned_right_shift()` function to perform unsigned right shifts.\n*   The `url()`"
    ],
    "smoothed_bleu_4": 0.7016847581784641
  },
  {
    "doc_id": 11,
    "doc": {
      "id": 11,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/mgtv.py",
      "func_name": "MGTV.get_mgtv_real_url",
      "original_string": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list",
      "language": "python",
      "code": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list",
      "code_tokens": [
        "def",
        "get_mgtv_real_url",
        "(",
        "url",
        ")",
        ":",
        "content",
        "=",
        "loads",
        "(",
        "get_content",
        "(",
        "url",
        ")",
        ")",
        "m3u_url",
        "=",
        "content",
        "[",
        "'info'",
        "]",
        "split",
        "=",
        "urlsplit",
        "(",
        "m3u_url",
        ")",
        "base_url",
        "=",
        "\"{scheme}://{netloc}{path}/\"",
        ".",
        "format",
        "(",
        "scheme",
        "=",
        "split",
        "[",
        "0",
        "]",
        ",",
        "netloc",
        "=",
        "split",
        "[",
        "1",
        "]",
        ",",
        "path",
        "=",
        "dirname",
        "(",
        "split",
        "[",
        "2",
        "]",
        ")",
        ")",
        "content",
        "=",
        "get_content",
        "(",
        "content",
        "[",
        "'info'",
        "]",
        ")",
        "#get the REAL M3U url, maybe to be changed later?",
        "segment_list",
        "=",
        "[",
        "]",
        "segments_size",
        "=",
        "0",
        "for",
        "i",
        "in",
        "content",
        ".",
        "split",
        "(",
        ")",
        ":",
        "if",
        "not",
        "i",
        ".",
        "startswith",
        "(",
        "'#'",
        ")",
        ":",
        "#not the best way, better we use the m3u8 package",
        "segment_list",
        ".",
        "append",
        "(",
        "base_url",
        "+",
        "i",
        ")",
        "# use ext-info for fast size calculate",
        "elif",
        "i",
        ".",
        "startswith",
        "(",
        "'#EXT-MGTV-File-SIZE:'",
        ")",
        ":",
        "segments_size",
        "+=",
        "int",
        "(",
        "i",
        "[",
        "i",
        ".",
        "rfind",
        "(",
        "':'",
        ")",
        "+",
        "1",
        ":",
        "]",
        ")",
        "return",
        "m3u_url",
        ",",
        "segments_size",
        ",",
        "segment_list"
      ],
      "docstring": "str->list of str\n        Give you the real URLs.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "list",
        "of",
        "str",
        "Give",
        "you",
        "the",
        "real",
        "URLs",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58"
    },
    "target": "str - > list of str Give you the real URLs .",
    "arguments": [
      [
        "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) #get the REAL M3U url, maybe to be changed later? segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : #not the best way, better we use the m3u8 package segment_list . append ( base_url + i ) # use ext-info for fast size calculate elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " m3u_url, segments_size, segment_list = None, None, [ ] #we also need some help from segment data def get_mgtv_real_url_helper ( url ) : def get_segment_data ( url ) : m3u8_url, segments_size, segment_list = get_mgtv_real_url ( url ) content = get_content ( m3u8_url ) #get the segment #the current m3u8_url is always changed def get_segment_url ( url ) : segment_url = Loads ( get_content ( url ) ) return segment_url. split ( m3u8_url ) def get_segment_data ( url ) : segment_data = segment_url. get ( url ) return content [ content. find ( #todo: maybe we should use the url for faster get? 'url=\"') + 6 : content. find ( 'url\":\"' + url + '\"') ] return get_segment_url ( get_m3u8_real_url ( get_m3u8_url ( url ) ) ) def set_play_params ( url ) : url = get_mgtv_real_url_helper ( url ) content = get_content ( url ) play_params = content. split ( 'playParams=' ) 2"
      ]
    ],
    "filtered_resps": [
      " m3u_url, segments_size, segment_list = None, None, [ ] #we also need some help from segment data def get_mgtv_real_url_helper ( url ) : def get_segment_data ( url ) : m3u8_url, segments_size, segment_list = get_mgtv_real_url ( url ) content = get_content ( m3u8_url ) #get the segment #the current m3u8_url is always changed def get_segment_url ( url ) : segment_url = Loads ( get_content ( url ) ) return segment_url. split ( m3u8_url ) def get_segment_data ( url ) : segment_data = segment_url. get ( url ) return content [ content. find ( #todo: maybe we should use the url for faster get? 'url=\"') + 6 : content. find ( 'url\":\"' + url + '\"') ] return get_segment_url ( get_m3u8_real_url ( get_m3u8_url ( url ) ) ) def set_play_params ( url ) : url = get_mgtv_real_url_helper ( url ) content = get_content ( url ) play_params = content. split ( 'playParams=' ) 2"
    ],
    "smoothed_bleu_4": 0.5252442989662475
  },
  {
    "doc_id": 12,
    "doc": {
      "id": 12,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/fs.py",
      "func_name": "legitimize",
      "original_string": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text",
      "language": "python",
      "code": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text",
      "code_tokens": [
        "def",
        "legitimize",
        "(",
        "text",
        ",",
        "os",
        "=",
        "detect_os",
        "(",
        ")",
        ")",
        ":",
        "# POSIX systems",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "0",
        ":",
        "None",
        ",",
        "ord",
        "(",
        "'/'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'|'",
        ")",
        ":",
        "'-'",
        ",",
        "}",
        ")",
        "# FIXME: do some filesystem detection",
        "if",
        "os",
        "==",
        "'windows'",
        "or",
        "os",
        "==",
        "'cygwin'",
        "or",
        "os",
        "==",
        "'wsl'",
        ":",
        "# Windows (non-POSIX namespace)",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "# Reserved in Windows VFAT and NTFS",
        "ord",
        "(",
        "':'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'*'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'?'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'\\\\'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'\\\"'",
        ")",
        ":",
        "'\\''",
        ",",
        "# Reserved in Windows VFAT",
        "ord",
        "(",
        "'+'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'<'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'>'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'['",
        ")",
        ":",
        "'('",
        ",",
        "ord",
        "(",
        "']'",
        ")",
        ":",
        "')'",
        ",",
        "ord",
        "(",
        "'\\t'",
        ")",
        ":",
        "' '",
        ",",
        "}",
        ")",
        "else",
        ":",
        "# *nix",
        "if",
        "os",
        "==",
        "'mac'",
        ":",
        "# Mac OS HFS+",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "ord",
        "(",
        "':'",
        ")",
        ":",
        "'-'",
        ",",
        "}",
        ")",
        "# Remove leading .",
        "if",
        "text",
        ".",
        "startswith",
        "(",
        "\".\"",
        ")",
        ":",
        "text",
        "=",
        "text",
        "[",
        "1",
        ":",
        "]",
        "text",
        "=",
        "text",
        "[",
        ":",
        "80",
        "]",
        "# Trim to 82 Unicode characters long",
        "return",
        "text"
      ],
      "docstring": "Converts a string to a valid filename.",
      "docstring_tokens": [
        "Converts",
        "a",
        "string",
        "to",
        "a",
        "valid",
        "filename",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47"
    },
    "target": "Converts a string to a valid filename .",
    "arguments": [
      [
        "def legitimize ( text , os = detect_os ( ) ) : # POSIX systems text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) # FIXME: do some filesystem detection if os == 'windows' or os == 'cygwin' or os == 'wsl' : # Windows (non-POSIX namespace) text = text . translate ( { # Reserved in Windows VFAT and NTFS ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , # Reserved in Windows VFAT ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : # *nix if os == 'mac' : # Mac OS HFS+ text = text . translate ( { ord ( ':' ) : '-' , } ) # Remove leading . if text . startswith ( \".\" ) : text = text [ 1 : ] text = text [ : 80 ] # Trim to 82 Unicode characters long return text",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def generate_useragent ( ): \"\"\"Return the useragent\"\"\" # Default user agent with cookie prefix now including version + name of Python version ( without Python version number ) # In non-Python section, we include some other items with the same logic useragent = f'{sys. platform.lower()}{sys.version_info.major}{sys.version_info.minor} {sys.platform} {python_name(' '.join(sys.version_info).replace('.', ','))}{version}{netloc}( Python- {major}.{minor} )' # Default user agent with Python version in it useragent_options = f'{sys.platform} {python_name(\" \".join(sys.version_info).replace(\".\", \",\"))}{version}{netloc}( Python-{ sys.version_info.major}.{sys.version_info.minor} )' # add the browser identifier to the useragent add_browser(useragent, None) add_browser(useragent_options, None) # browser cookie prefix return useragent + \" \" + useragent_options def detect_os ( ) : from platform import system if system == 'win32' : return 'windows' if system == 'darwin' : return'mac' if system == 'linux' or system == 'cygwin' or system == 'wsl' :"
      ]
    ],
    "filtered_resps": [
      " def generate_useragent ( ): \"\"\"Return the useragent\"\"\" # Default user agent with cookie prefix now including version + name of Python version ( without Python version number ) # In non-Python section, we include some other items with the same logic useragent = f'{sys. platform.lower()}{sys.version_info.major}{sys.version_info.minor} {sys.platform} {python_name(' '.join(sys.version_info).replace('.', ','))}{version}{netloc}( Python- {major}.{minor} )' # Default user agent with Python version in it useragent_options = f'{sys.platform} {python_name(\" \".join(sys.version_info).replace(\".\", \",\"))}{version}{netloc}( Python-{ sys.version_info.major}.{sys.version_info.minor} )' # add the browser identifier to the useragent add_browser(useragent, None) add_browser(useragent_options, None) # browser cookie prefix return useragent + \" \" + useragent_options def detect_os ( ) : from platform import system if system == 'win32' : return 'windows' if system == 'darwin' : return'mac' if system == 'linux' or system == 'cygwin' or system == 'wsl' :"
    ],
    "smoothed_bleu_4": 0.3833074506730889
  },
  {
    "doc_id": 13,
    "doc": {
      "id": 13,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/cbs.py",
      "func_name": "cbs_download",
      "original_string": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "language": "python",
      "code": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "code_tokens": [
        "def",
        "cbs_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "get_content",
        "(",
        "url",
        ")",
        "pid",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\''",
        ")",
        "title",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"'",
        ")",
        "theplatform_download_by_pid",
        "(",
        "pid",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")"
      ],
      "docstring": "Downloads CBS videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "CBS",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17"
    },
    "target": "Downloads CBS videos by URL .",
    "arguments": [
      [
        "def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " Examples: | url = 'https://www.fembed.com/v/62bmx3-3mp8.html' | Examples: >> cbs_download ( url = 'https://www.cbs.com/shows/how-ihatemybrother/videos/cbs-video-how-ihatemybrother-episode-the-beer-lover-have-i-had-enough' ) from tgbus module import get_content, extract quality = match1 ( get_content ( 'https://tgbus.tv/video/2c7c66c3.html' ), r'({\"quality\":\"([a-z]+)\"}|Quality:\\s*(\\w+))' ) Examples: | url = 'https://tgbus.tv/video/2c7c66c3.html' | Examples: >> extract ( url = 'https://tgbus.tv/video/2c7c66c3.html' ) '720p' from tgbus module import get_content, extract, download movie quality = match1 ( get_content ( 'https://tgbus.tv/video/2c7c66c3.html' ), r'({\"quality\":\"([a-z]+)\"}|Quality:\\s*(\\w+))'"
      ]
    ],
    "filtered_resps": [
      " Examples: | url = 'https://www.fembed.com/v/62bmx3-3mp8.html' | Examples: >> cbs_download ( url = 'https://www.cbs.com/shows/how-ihatemybrother/videos/cbs-video-how-ihatemybrother-episode-the-beer-lover-have-i-had-enough' ) from tgbus module import get_content, extract quality = match1 ( get_content ( 'https://tgbus.tv/video/2c7c66c3.html' ), r'({\"quality\":\"([a-z]+)\"}|Quality:\\s*(\\w+))' ) Examples: | url = 'https://tgbus.tv/video/2c7c66c3.html' | Examples: >> extract ( url = 'https://tgbus.tv/video/2c7c66c3.html' ) '720p' from tgbus module import get_content, extract, download movie quality = match1 ( get_content ( 'https://tgbus.tv/video/2c7c66c3.html' ), r'({\"quality\":\"([a-z]+)\"}|Quality:\\s*(\\w+))'"
    ],
    "smoothed_bleu_4": 0.5392643044005252
  },
  {
    "doc_id": 14,
    "doc": {
      "id": 14,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/iqiyi.py",
      "func_name": "Iqiyi.download",
      "original_string": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')",
      "language": "python",
      "code": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')",
      "code_tokens": [
        "def",
        "download",
        "(",
        "self",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'json_output'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'json_output'",
        "]",
        ":",
        "json_output",
        ".",
        "output",
        "(",
        "self",
        ")",
        "elif",
        "'info_only'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'info_only'",
        "]",
        ":",
        "if",
        "'stream_id'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        ":",
        "# Display the stream",
        "stream_id",
        "=",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "# Display all available streams",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "[",
        "]",
        ")",
        "else",
        ":",
        "stream_id",
        "=",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'id'",
        "]",
        "if",
        "'id'",
        "in",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "else",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'itag'",
        "]",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "if",
        "'stream_id'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        ":",
        "# Download the stream",
        "stream_id",
        "=",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        "else",
        ":",
        "# Download stream with the best quality",
        "stream_id",
        "=",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'id'",
        "]",
        "if",
        "'id'",
        "in",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "else",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'itag'",
        "]",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "if",
        "stream_id",
        "in",
        "self",
        ".",
        "streams",
        ":",
        "urls",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'src'",
        "]",
        "ext",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'container'",
        "]",
        "total_size",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'size'",
        "]",
        "else",
        ":",
        "urls",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'src'",
        "]",
        "ext",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'container'",
        "]",
        "total_size",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'size'",
        "]",
        "if",
        "not",
        "urls",
        ":",
        "log",
        ".",
        "wtf",
        "(",
        "'[Failed] Cannot extract video source.'",
        ")",
        "# For legacy main()",
        "#Here's the change!!",
        "download_url_ffmpeg",
        "(",
        "urls",
        "[",
        "0",
        "]",
        ",",
        "self",
        ".",
        "title",
        ",",
        "'mp4'",
        ",",
        "output_dir",
        "=",
        "kwargs",
        "[",
        "'output_dir'",
        "]",
        ",",
        "merge",
        "=",
        "kwargs",
        "[",
        "'merge'",
        "]",
        ",",
        "stream",
        "=",
        "False",
        ")",
        "if",
        "not",
        "kwargs",
        "[",
        "'caption'",
        "]",
        ":",
        "print",
        "(",
        "'Skipping captions.'",
        ")",
        "return",
        "for",
        "lang",
        "in",
        "self",
        ".",
        "caption_tracks",
        ":",
        "filename",
        "=",
        "'%s.%s.srt'",
        "%",
        "(",
        "get_filename",
        "(",
        "self",
        ".",
        "title",
        ")",
        ",",
        "lang",
        ")",
        "print",
        "(",
        "'Saving %s ... '",
        "%",
        "filename",
        ",",
        "end",
        "=",
        "\"\"",
        ",",
        "flush",
        "=",
        "True",
        ")",
        "srt",
        "=",
        "self",
        ".",
        "caption_tracks",
        "[",
        "lang",
        "]",
        "with",
        "open",
        "(",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "kwargs",
        "[",
        "'output_dir'",
        "]",
        ",",
        "filename",
        ")",
        ",",
        "'w'",
        ",",
        "encoding",
        "=",
        "'utf-8'",
        ")",
        "as",
        "x",
        ":",
        "x",
        ".",
        "write",
        "(",
        "srt",
        ")",
        "print",
        "(",
        "'Done.'",
        ")"
      ],
      "docstring": "Override the original one\n        Ugly ugly dirty hack",
      "docstring_tokens": [
        "Override",
        "the",
        "original",
        "one",
        "Ugly",
        "ugly",
        "dirty",
        "hack"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218"
    },
    "target": "Override the original one Ugly ugly dirty hack",
    "arguments": [
      [
        "def download ( self , * * kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Display the stream stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : # Display all available streams if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Download the stream stream_id = kwargs [ 'stream_id' ] else : # Download stream with the best quality stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[Failed] Cannot extract video source.' ) # For legacy main() #Here's the change!! download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) if not kwargs [ 'caption' ] : print ( 'Skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'Done.' )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " if not kwargs ['stream' ] : self. get_playlist_item ( stream_id ) else : x = 'Done.' if self. caption_tracks else 'Skipping captions due to no videos downloaded.' <---\n\nI replaced \"youtube-dl\" with my implementation on how to extract the video source, download it, and display and save the captions. So the code will now look for the caption files, open them, and save them as.srt files.\n\nPlease let me know if any additional modifications are needed. Thanks in advance! \n\nAlso, please note that the new implementation may not be 100% compatible with the original `youtube-dl` code, so I advise doing some thorough tests to ensure that it works as expected.\n\nI've provided below some relevant information about `youtube-dl` that I've used in modifying it:\n\n# Extracting video source\n\nIn `youtube-dl`, the video source is extracted using the following function:\n```python\ndef get_video_source( self, entry ):\n    return entry.get( \"best\" )\n```\nIn my implementation, I've used the following code to extract the video source:\n```python\nif stream_id in self.streams:\n    urls = self.streams["
      ]
    ],
    "filtered_resps": [
      " if not kwargs ['stream' ] : self. get_playlist_item ( stream_id ) else : x = 'Done.' if self. caption_tracks else 'Skipping captions due to no videos downloaded.' <---\n\nI replaced \"youtube-dl\" with my implementation on how to extract the video source, download it, and display and save the captions. So the code will now look for the caption files, open them, and save them as.srt files.\n\nPlease let me know if any additional modifications are needed. Thanks in advance! \n\nAlso, please note that the new implementation may not be 100% compatible with the original `youtube-dl` code, so I advise doing some thorough tests to ensure that it works as expected.\n\nI've provided below some relevant information about `youtube-dl` that I've used in modifying it:\n\n# Extracting video source\n\nIn `youtube-dl`, the video source is extracted using the following function:\n```python\ndef get_video_source( self, entry ):\n    return entry.get( \"best\" )\n```\nIn my implementation, I've used the following code to extract the video source:\n```python\nif stream_id in self.streams:\n    urls = self.streams["
    ],
    "smoothed_bleu_4": 0.5413284947723218
  },
  {
    "doc_id": 15,
    "doc": {
      "id": 15,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/acfun.py",
      "func_name": "acfun_download_by_vid",
      "original_string": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass",
      "language": "python",
      "code": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass",
      "code_tokens": [
        "def",
        "acfun_download_by_vid",
        "(",
        "vid",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "#first call the main parasing API",
        "info",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "get_content",
        "(",
        "'http://www.acfun.cn/video/getVideo.aspx?id='",
        "+",
        "vid",
        ")",
        ")",
        "sourceType",
        "=",
        "info",
        "[",
        "'sourceType'",
        "]",
        "#decide sourceId to know which extractor to use",
        "if",
        "'sourceId'",
        "in",
        "info",
        ":",
        "sourceId",
        "=",
        "info",
        "[",
        "'sourceId'",
        "]",
        "# danmakuId = info['danmakuId']",
        "#call extractor decided by sourceId",
        "if",
        "sourceType",
        "==",
        "'sina'",
        ":",
        "sina_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'youku'",
        ":",
        "youku_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "elif",
        "sourceType",
        "==",
        "'tudou'",
        ":",
        "tudou_download_by_iid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'qq'",
        ":",
        "qq_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "True",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'letv'",
        ":",
        "letvcloud_download_by_vu",
        "(",
        "sourceId",
        ",",
        "'2d8c027396'",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'zhuzhan'",
        ":",
        "#As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this",
        "#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player",
        "#old code removed",
        "url",
        "=",
        "'http://www.acfun.cn/v/ac'",
        "+",
        "vid",
        "yk_streams",
        "=",
        "youku_acfun_proxy",
        "(",
        "info",
        "[",
        "'sourceId'",
        "]",
        ",",
        "info",
        "[",
        "'encode'",
        "]",
        ",",
        "url",
        ")",
        "seq",
        "=",
        "[",
        "'mp4hd3'",
        ",",
        "'mp4hd2'",
        ",",
        "'mp4hd'",
        ",",
        "'flvhd'",
        "]",
        "for",
        "t",
        "in",
        "seq",
        ":",
        "if",
        "yk_streams",
        ".",
        "get",
        "(",
        "t",
        ")",
        ":",
        "preferred",
        "=",
        "yk_streams",
        "[",
        "t",
        "]",
        "break",
        "#total_size in the json could be incorrect(F.I. 0)",
        "size",
        "=",
        "0",
        "for",
        "url",
        "in",
        "preferred",
        "[",
        "0",
        "]",
        ":",
        "_",
        ",",
        "_",
        ",",
        "seg_size",
        "=",
        "url_info",
        "(",
        "url",
        ")",
        "size",
        "+=",
        "seg_size",
        "#fallback to flvhd is not quite possible",
        "if",
        "re",
        ".",
        "search",
        "(",
        "r'fid=[0-9A-Z\\-]*.flv'",
        ",",
        "preferred",
        "[",
        "0",
        "]",
        "[",
        "0",
        "]",
        ")",
        ":",
        "ext",
        "=",
        "'flv'",
        "else",
        ":",
        "ext",
        "=",
        "'mp4'",
        "print_info",
        "(",
        "site_info",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ")",
        "if",
        "not",
        "info_only",
        ":",
        "download_urls",
        "(",
        "preferred",
        "[",
        "0",
        "]",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ")",
        "else",
        ":",
        "raise",
        "NotImplementedError",
        "(",
        "sourceType",
        ")",
        "if",
        "not",
        "info_only",
        "and",
        "not",
        "dry_run",
        ":",
        "if",
        "not",
        "kwargs",
        "[",
        "'caption'",
        "]",
        ":",
        "print",
        "(",
        "'Skipping danmaku.'",
        ")",
        "return",
        "try",
        ":",
        "title",
        "=",
        "get_filename",
        "(",
        "title",
        ")",
        "print",
        "(",
        "'Downloading %s ...\\n'",
        "%",
        "(",
        "title",
        "+",
        "'.cmt.json'",
        ")",
        ")",
        "cmt",
        "=",
        "get_srt_json",
        "(",
        "vid",
        ")",
        "with",
        "open",
        "(",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "output_dir",
        ",",
        "title",
        "+",
        "'.cmt.json'",
        ")",
        ",",
        "'w'",
        ",",
        "encoding",
        "=",
        "'utf-8'",
        ")",
        "as",
        "x",
        ":",
        "x",
        ".",
        "write",
        "(",
        "cmt",
        ")",
        "except",
        ":",
        "pass"
      ],
      "docstring": "str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.",
      "docstring_tokens": [
        "str",
        "str",
        "str",
        "bool",
        "bool",
        "-",
        ">",
        "None"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109"
    },
    "target": "str str str bool bool - > None",
    "arguments": [
      [
        "def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : #first call the main parasing API info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) sourceType = info [ 'sourceType' ] #decide sourceId to know which extractor to use if 'sourceId' in info : sourceId = info [ 'sourceId' ] # danmakuId = info['danmakuId'] #call extractor decided by sourceId if sourceType == 'sina' : sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'youku' : youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) elif sourceType == 'tudou' : tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'qq' : qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'letv' : letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'zhuzhan' : #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this #In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player #old code removed url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break #total_size in the json could be incorrect(F.I. 0) size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size #fallback to flvhd is not quite possible if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise NotImplementedError ( sourceType ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'Skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : x . write ( cmt ) except : pass",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except RequestException : info_only = True to_acfun ( vid = vid, title = title, info_only = info_only ) ```\n  - File \"acfun.py\", line 1133\n  - ``` print_title ( title ) ```\n  - File \"acfun.py\", line 1134\n  - ``` print ( '%s [' % ( title ) ) ```\n  - File \"acfun.py\", line 1133\n  - ``` print_title ( title ) ```\n  - File \"acfun.py\", line 1134\n  - ``` print ( '%s [' % ( title ) ```\n\nThe final answer is: 403 Traceback (most recent call last): File \"acfun.py\", line 1052, in _append\n```\n   ...= info.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n```\n  - File \"acfun.py\", line 115\n  - ```         if re.search(r'\"__ac__\" in', info) or re.search('__ac__', get_content(info.__dict__))    or re.search(\"<!-- \\w+=\"get_content(info.__dict__), r'<!-- |__ac__=\"':): ```\n "
      ]
    ],
    "filtered_resps": [
      " except RequestException : info_only = True to_acfun ( vid = vid, title = title, info_only = info_only ) ```\n  - File \"acfun.py\", line 1133\n  - ``` print_title ( title ) ```\n  - File \"acfun.py\", line 1134\n  - ``` print ( '%s [' % ( title ) ) ```\n  - File \"acfun.py\", line 1133\n  - ``` print_title ( title ) ```\n  - File \"acfun.py\", line 1134\n  - ``` print ( '%s [' % ( title ) ```\n\nThe final answer is: 403 Traceback (most recent call last): File \"acfun.py\", line 1052, in _append\n```\n   ...= info.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n```\n  - File \"acfun.py\", line 115\n  - ```         if re.search(r'\"__ac__\" in', info) or re.search('__ac__', get_content(info.__dict__))    or re.search(\"<!-- \\w+=\"get_content(info.__dict__), r'<!-- |__ac__=\"':): ```\n "
    ],
    "smoothed_bleu_4": 0.3223218612110406
  },
  {
    "doc_id": 16,
    "doc": {
      "id": 16,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "matchall",
      "original_string": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret",
      "language": "python",
      "code": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret",
      "code_tokens": [
        "def",
        "matchall",
        "(",
        "text",
        ",",
        "patterns",
        ")",
        ":",
        "ret",
        "=",
        "[",
        "]",
        "for",
        "pattern",
        "in",
        "patterns",
        ":",
        "match",
        "=",
        "re",
        ".",
        "findall",
        "(",
        "pattern",
        ",",
        "text",
        ")",
        "ret",
        "+=",
        "match",
        "return",
        "ret"
      ],
      "docstring": "Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.",
      "docstring_tokens": [
        "Scans",
        "through",
        "a",
        "string",
        "for",
        "substrings",
        "matched",
        "some",
        "patterns",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268"
    },
    "target": "Scans through a string for substrings matched some patterns .",
    "arguments": [
      [
        "def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret += match return ret",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # Regular expression to check if `math.all()` is used\npattern_all = r \"\\ bmath \\.all\\(\\ s *\" ) # Check all function calls in source file\nwith open ( src_file, \"r\" ) as file : source_code = file. read ( )\n# Search for function calls\nmatches = matchall ( source_code, [ pattern_func, pattern_other, pattern_all ])\n# Report any discrepancies\nif ( pattern_func in matches and pattern_all in matches ) :\nif ( len ( matches ) == 1 and pattern_other not in matches ) :\nprint ( '[ warning ] \\ ', \\'math.all()' \\'is used directly but ',\n'other mathematical functions, like `math.pow()` and ',\n' `math.sqrt()` are not used in ',\nsrc_file +\n'. \\ n' )\nelse :\nprint ( '[ warning ] \\ ', \\'math.all()' \\'is used directly but with ',\n'mathematical functions that are not in the {0}'. format (\nmath_functions ),\nsrc_file +\n' \\ n' )\n# Remove function calls that are part of another function using the'matchall'\n# function and the corresponding regular patterns\nfor pos in matches"
      ]
    ],
    "filtered_resps": [
      " # Regular expression to check if `math.all()` is used\npattern_all = r \"\\ bmath \\.all\\(\\ s *\" ) # Check all function calls in source file\nwith open ( src_file, \"r\" ) as file : source_code = file. read ( )\n# Search for function calls\nmatches = matchall ( source_code, [ pattern_func, pattern_other, pattern_all ])\n# Report any discrepancies\nif ( pattern_func in matches and pattern_all in matches ) :\nif ( len ( matches ) == 1 and pattern_other not in matches ) :\nprint ( '[ warning ] \\ ', \\'math.all()' \\'is used directly but ',\n'other mathematical functions, like `math.pow()` and ',\n' `math.sqrt()` are not used in ',\nsrc_file +\n'. \\ n' )\nelse :\nprint ( '[ warning ] \\ ', \\'math.all()' \\'is used directly but with ',\n'mathematical functions that are not in the {0}'. format (\nmath_functions ),\nsrc_file +\n' \\ n' )\n# Remove function calls that are part of another function using the'matchall'\n# function and the corresponding regular patterns\nfor pos in matches"
    ],
    "smoothed_bleu_4": 0.47813986928959934
  },
  {
    "doc_id": 17,
    "doc": {
      "id": 17,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "parse_query_param",
      "original_string": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None",
      "language": "python",
      "code": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None",
      "code_tokens": [
        "def",
        "parse_query_param",
        "(",
        "url",
        ",",
        "param",
        ")",
        ":",
        "try",
        ":",
        "return",
        "parse",
        ".",
        "parse_qs",
        "(",
        "parse",
        ".",
        "urlparse",
        "(",
        "url",
        ")",
        ".",
        "query",
        ")",
        "[",
        "param",
        "]",
        "[",
        "0",
        "]",
        "except",
        ":",
        "return",
        "None"
      ],
      "docstring": "Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.",
      "docstring_tokens": [
        "Parses",
        "the",
        "query",
        "string",
        "of",
        "a",
        "URL",
        "and",
        "returns",
        "the",
        "value",
        "of",
        "a",
        "parameter",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299"
    },
    "target": "Parses the query string of a URL and returns the value of a parameter .",
    "arguments": [
      [
        "def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\n#... rest of the code remains the same\n\nNow, the app will be able to handle query parameters correctly. Note that the above code assumes that the `q` parameter has a single value. If it can have multiple values, you should modify the code accordingly. Also, be sure to add error handling to ensure that the code doesn't crash if `q` is missing from the query string. You can use the `get()` method of dictionaries in Python to achieve this:\n\n```python\nq = parse_query_param(url, \"q\")\nif q is None:  # q is missing from the query string\n    # handle the missing query parameter\nelse:\n    # proceed with the rest of the code\n```\n\nSo, we have updated the index view function as follows.\n\n```python\nfrom django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nimport json\nimport urllib.parse as parse\n\ndef index(request):\n    if request.method == 'POST':\n\n        if request.body:  # Check if the request contains any data to process.\n            try:\n                data = json.loads(request.body)\n                # perform some action based on the request data\n                return HttpResponse('Data processed successfully', status=200)\n\n           "
      ]
    ],
    "filtered_resps": [
      "\n\n#... rest of the code remains the same\n\nNow, the app will be able to handle query parameters correctly. Note that the above code assumes that the `q` parameter has a single value. If it can have multiple values, you should modify the code accordingly. Also, be sure to add error handling to ensure that the code doesn't crash if `q` is missing from the query string. You can use the `get()` method of dictionaries in Python to achieve this:\n\n```python\nq = parse_query_param(url, \"q\")\nif q is None:  # q is missing from the query string\n    # handle the missing query parameter\nelse:\n    # proceed with the rest of the code\n```\n\nSo, we have updated the index view function as follows.\n\n```python\nfrom django.shortcuts import render\nfrom django.http import HttpResponse, JsonResponse\nimport json\nimport urllib.parse as parse\n\ndef index(request):\n    if request.method == 'POST':\n\n        if request.body:  # Check if the request contains any data to process.\n            try:\n                data = json.loads(request.body)\n                # perform some action based on the request data\n                return HttpResponse('Data processed successfully', status=200)\n\n           "
    ],
    "smoothed_bleu_4": 1.1210560692136746
  },
  {
    "doc_id": 18,
    "doc": {
      "id": 18,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "get_content",
      "original_string": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data",
      "language": "python",
      "code": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data",
      "code_tokens": [
        "def",
        "get_content",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "{",
        "}",
        ",",
        "decoded",
        "=",
        "True",
        ")",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'get_content: %s'",
        "%",
        "url",
        ")",
        "req",
        "=",
        "request",
        ".",
        "Request",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "if",
        "cookies",
        ":",
        "cookies",
        ".",
        "add_cookie_header",
        "(",
        "req",
        ")",
        "req",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "req",
        ".",
        "unredirected_hdrs",
        ")",
        "response",
        "=",
        "urlopen_with_retry",
        "(",
        "req",
        ")",
        "data",
        "=",
        "response",
        ".",
        "read",
        "(",
        ")",
        "# Handle HTTP compression for gzip and deflate (zlib)",
        "content_encoding",
        "=",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Encoding'",
        ")",
        "if",
        "content_encoding",
        "==",
        "'gzip'",
        ":",
        "data",
        "=",
        "ungzip",
        "(",
        "data",
        ")",
        "elif",
        "content_encoding",
        "==",
        "'deflate'",
        ":",
        "data",
        "=",
        "undeflate",
        "(",
        "data",
        ")",
        "# Decode the response body",
        "if",
        "decoded",
        ":",
        "charset",
        "=",
        "match1",
        "(",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Type'",
        ",",
        "''",
        ")",
        ",",
        "r'charset=([\\w-]+)'",
        ")",
        "if",
        "charset",
        "is",
        "not",
        "None",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "charset",
        ",",
        "'ignore'",
        ")",
        "else",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ",",
        "'ignore'",
        ")",
        "return",
        "data"
      ],
      "docstring": "Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
      "docstring_tokens": [
        "Gets",
        "the",
        "content",
        "of",
        "a",
        "URL",
        "via",
        "sending",
        "a",
        "HTTP",
        "GET",
        "request",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454"
    },
    "target": "Gets the content of a URL via sending a HTTP GET request .",
    "arguments": [
      [
        "def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ( logging. exception ( 'urllib2 error %s' % e ), e ) error = value = v ( ) return v ( ) def unredirect_post ( url, data, method = 'POST', headers = { }, cookies = { }, content_type = FORM_URLENCODED ) : # Note: urllib2 does not support POST with data redirection after a POST. # We handle this by POST to the unredirection URL and receive the new # cookie then do a GET request to the original URL. req = request. Request ( url, data )\nIt should be noted however that this particular example may not be secure because it is vulnerable to a request smuggling attack.  \n1. The code does not check the status code of the HTTP request, which could allow a malicious server to respond with an incorrect status code.\n2. The code does not check if the response we received was actually from the server we sent the request to, which could allow an attacker to perform a request smuggling attack.\n3. The code does not verify the integrity of the response data, which could allow an attacker to tamper with the response data.\n\nIt might also be noted that some of the error handling could be improved"
      ]
    ],
    "filtered_resps": [
      " ( logging. exception ( 'urllib2 error %s' % e ), e ) error = value = v ( ) return v ( ) def unredirect_post ( url, data, method = 'POST', headers = { }, cookies = { }, content_type = FORM_URLENCODED ) : # Note: urllib2 does not support POST with data redirection after a POST. # We handle this by POST to the unredirection URL and receive the new # cookie then do a GET request to the original URL. req = request. Request ( url, data )\nIt should be noted however that this particular example may not be secure because it is vulnerable to a request smuggling attack.  \n1. The code does not check the status code of the HTTP request, which could allow a malicious server to respond with an incorrect status code.\n2. The code does not check if the response we received was actually from the server we sent the request to, which could allow an attacker to perform a request smuggling attack.\n3. The code does not verify the integrity of the response data, which could allow an attacker to tamper with the response data.\n\nIt might also be noted that some of the error handling could be improved"
    ],
    "smoothed_bleu_4": 0.9782074775935972
  },
  {
    "doc_id": 19,
    "doc": {
      "id": 19,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "post_content",
      "original_string": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data",
      "language": "python",
      "code": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data",
      "code_tokens": [
        "def",
        "post_content",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "{",
        "}",
        ",",
        "post_data",
        "=",
        "{",
        "}",
        ",",
        "decoded",
        "=",
        "True",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "kwargs",
        ".",
        "get",
        "(",
        "'post_data_raw'",
        ")",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'post_content: %s\\npost_data_raw: %s'",
        "%",
        "(",
        "url",
        ",",
        "kwargs",
        "[",
        "'post_data_raw'",
        "]",
        ")",
        ")",
        "else",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'post_content: %s\\npost_data: %s'",
        "%",
        "(",
        "url",
        ",",
        "post_data",
        ")",
        ")",
        "req",
        "=",
        "request",
        ".",
        "Request",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "if",
        "cookies",
        ":",
        "cookies",
        ".",
        "add_cookie_header",
        "(",
        "req",
        ")",
        "req",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "req",
        ".",
        "unredirected_hdrs",
        ")",
        "if",
        "kwargs",
        ".",
        "get",
        "(",
        "'post_data_raw'",
        ")",
        ":",
        "post_data_enc",
        "=",
        "bytes",
        "(",
        "kwargs",
        "[",
        "'post_data_raw'",
        "]",
        ",",
        "'utf-8'",
        ")",
        "else",
        ":",
        "post_data_enc",
        "=",
        "bytes",
        "(",
        "parse",
        ".",
        "urlencode",
        "(",
        "post_data",
        ")",
        ",",
        "'utf-8'",
        ")",
        "response",
        "=",
        "urlopen_with_retry",
        "(",
        "req",
        ",",
        "data",
        "=",
        "post_data_enc",
        ")",
        "data",
        "=",
        "response",
        ".",
        "read",
        "(",
        ")",
        "# Handle HTTP compression for gzip and deflate (zlib)",
        "content_encoding",
        "=",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Encoding'",
        ")",
        "if",
        "content_encoding",
        "==",
        "'gzip'",
        ":",
        "data",
        "=",
        "ungzip",
        "(",
        "data",
        ")",
        "elif",
        "content_encoding",
        "==",
        "'deflate'",
        ":",
        "data",
        "=",
        "undeflate",
        "(",
        "data",
        ")",
        "# Decode the response body",
        "if",
        "decoded",
        ":",
        "charset",
        "=",
        "match1",
        "(",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Type'",
        ")",
        ",",
        "r'charset=([\\w-]+)'",
        ")",
        "if",
        "charset",
        "is",
        "not",
        "None",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "charset",
        ")",
        "else",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "return",
        "data"
      ],
      "docstring": "Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
      "docstring_tokens": [
        "Post",
        "the",
        "content",
        "of",
        "a",
        "URL",
        "via",
        "sending",
        "a",
        "HTTP",
        "POST",
        "request",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501"
    },
    "target": "Post the content of a URL via sending a HTTP POST request .",
    "arguments": [
      [
        "def post_content ( url , headers = { } , post_data = { } , decoded = True , * * kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_html_value ( soup, name ) : \"\"\" Given BeautifulSoup soup object, find tag object with the content matching a certain value. By using xpath/xpath1.0 selector or attribute matching, speed and efficiency are ensured when parsing large HTML contents. Result is always a list of tag objects (even if there is only one match). \"\"\" from urllib. parse import quote import bs4. selectors as bs4s if soup!= 'notfound' : attribs = [ ( name, 'value', name ), ( name, 'contents', name ), ( name, f'xpath1.0/{name}', name ) ] for sel, attribs2 in attribs 2] ) : if soup. select ( sel ) : if not match1 ( match2 ( soup. select ( sel ) [ 0 ]. text, name ), r '\\ A' ) : soup = 'notfound' if name == 'css_sel' : elements = select ( soup, name ) else : elements = soup. select ( sel ) if elements : return elements else : raise _HTMLValueError ( name ) class _HTMLValueError ( LookupError ) : \"\"\" The value %s was not found in"
      ]
    ],
    "filtered_resps": [
      " def get_html_value ( soup, name ) : \"\"\" Given BeautifulSoup soup object, find tag object with the content matching a certain value. By using xpath/xpath1.0 selector or attribute matching, speed and efficiency are ensured when parsing large HTML contents. Result is always a list of tag objects (even if there is only one match). \"\"\" from urllib. parse import quote import bs4. selectors as bs4s if soup!= 'notfound' : attribs = [ ( name, 'value', name ), ( name, 'contents', name ), ( name, f'xpath1.0/{name}', name ) ] for sel, attribs2 in attribs 2] ) : if soup. select ( sel ) : if not match1 ( match2 ( soup. select ( sel ) [ 0 ]. text, name ), r '\\ A' ) : soup = 'notfound' if name == 'css_sel' : elements = select ( soup, name ) else : elements = soup. select ( sel ) if elements : return elements else : raise _HTMLValueError ( name ) class _HTMLValueError ( LookupError ) : \"\"\" The value %s was not found in"
    ],
    "smoothed_bleu_4": 0.7620145157046899
  },
  {
    "doc_id": 20,
    "doc": {
      "id": 20,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "parse_host",
      "original_string": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
      "language": "python",
      "code": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
      "code_tokens": [
        "def",
        "parse_host",
        "(",
        "host",
        ")",
        ":",
        "if",
        "re",
        ".",
        "match",
        "(",
        "r'^(\\d+)$'",
        ",",
        "host",
        ")",
        "is",
        "not",
        "None",
        ":",
        "return",
        "(",
        "\"0.0.0.0\"",
        ",",
        "int",
        "(",
        "host",
        ")",
        ")",
        "if",
        "re",
        ".",
        "match",
        "(",
        "r'^(\\w+)://'",
        ",",
        "host",
        ")",
        "is",
        "None",
        ":",
        "host",
        "=",
        "\"//\"",
        "+",
        "host",
        "o",
        "=",
        "parse",
        ".",
        "urlparse",
        "(",
        "host",
        ")",
        "hostname",
        "=",
        "o",
        ".",
        "hostname",
        "or",
        "\"0.0.0.0\"",
        "port",
        "=",
        "o",
        ".",
        "port",
        "or",
        "0",
        "return",
        "(",
        "hostname",
        ",",
        "port",
        ")"
      ],
      "docstring": "Parses host name and port number from a string.",
      "docstring_tokens": [
        "Parses",
        "host",
        "name",
        "and",
        "port",
        "number",
        "from",
        "a",
        "string",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1216-L1226"
    },
    "target": "Parses host name and port number from a string .",
    "arguments": [
      [
        "def parse_host ( host ) : if re . match ( r'^(\\d+)$' , host ) is not None : return ( \"0.0.0.0\" , int ( host ) ) if re . match ( r'^(\\w+)://' , host ) is None : host = \"//\" + host o = parse . urlparse ( host ) hostname = o . hostname or \"0.0.0.0\" port = o . port or 0 return ( hostname , port )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return ( None, None ) EOF\nNow, our function could be modified to import the required functions from the urlparse and socket libraries at the top of the function.\nSince we have successfully identified the host-name in our function, if the port is 80 or 443 we do not need to modify it since they are http and https port numbers, respectively. We also need to ensure that the port numbers are valid before proceeding to modify them. If the port numbers are not valid, we do not need to add them as \u201chttp://\" will be handled for us by the load balancer, so we can still access the website.\n\nHere is an example of how our function can be modified and extended to import necessary functions and handle port numbers:\n\n```\nimport urlparse\nimport re\nsocket = \u201c\"\"\nport_numbers = [ 3306, 27017, 5432, 22, 80, 443 ]\n\ndef parse_host ( host ) :\nif re. match ( r'^(\\d+)$', host ) is not None :\nhostname =  \"0.0.0.0\"\nport = int ( host )\nif port in port_numbers :\n# Handle port numbers as necessary\nreturn ( hostname, socket. get"
      ]
    ],
    "filtered_resps": [
      " return ( None, None ) EOF\nNow, our function could be modified to import the required functions from the urlparse and socket libraries at the top of the function.\nSince we have successfully identified the host-name in our function, if the port is 80 or 443 we do not need to modify it since they are http and https port numbers, respectively. We also need to ensure that the port numbers are valid before proceeding to modify them. If the port numbers are not valid, we do not need to add them as \u201chttp://\" will be handled for us by the load balancer, so we can still access the website.\n\nHere is an example of how our function can be modified and extended to import necessary functions and handle port numbers:\n\n```\nimport urlparse\nimport re\nsocket = \u201c\"\"\nport_numbers = [ 3306, 27017, 5432, 22, 80, 443 ]\n\ndef parse_host ( host ) :\nif re. match ( r'^(\\d+)$', host ) is not None :\nhostname =  \"0.0.0.0\"\nport = int ( host )\nif port in port_numbers :\n# Handle port numbers as necessary\nreturn ( hostname, socket. get"
    ],
    "smoothed_bleu_4": 0.6407752746273688
  },
  {
    "doc_id": 21,
    "doc": {
      "id": 21,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/showroom.py",
      "func_name": "showroom_get_roomid_by_room_url_key",
      "original_string": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')\n    assert roomid\n    return roomid",
      "language": "python",
      "code": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')\n    assert roomid\n    return roomid",
      "code_tokens": [
        "def",
        "showroom_get_roomid_by_room_url_key",
        "(",
        "room_url_key",
        ")",
        ":",
        "fake_headers_mobile",
        "=",
        "{",
        "'Accept'",
        ":",
        "'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'",
        ",",
        "'Accept-Charset'",
        ":",
        "'UTF-8,*;q=0.5'",
        ",",
        "'Accept-Encoding'",
        ":",
        "'gzip,deflate,sdch'",
        ",",
        "'Accept-Language'",
        ":",
        "'en-US,en;q=0.8'",
        ",",
        "'User-Agent'",
        ":",
        "'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'",
        "}",
        "webpage_url",
        "=",
        "'https://www.showroom-live.com/'",
        "+",
        "room_url_key",
        "html",
        "=",
        "get_content",
        "(",
        "webpage_url",
        ",",
        "headers",
        "=",
        "fake_headers_mobile",
        ")",
        "roomid",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'room\\?room_id\\=(\\d+)'",
        ")",
        "assert",
        "roomid",
        "return",
        "roomid"
      ],
      "docstring": "str->str",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "str"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L11-L24"
    },
    "target": "str - > str",
    "arguments": [
      [
        "def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) assert roomid return roomid",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nRoomID Finder Example\ndef showroom_get_roomid_by_room_url_key ( room_url_key ) : webpage_url = 'https://www.showroom-live.com/' + room_url_key # Get content using a User-Agent that identifies as a mobile browser url = 'https://www.showroom-live.com/' + room_url_key headers = {'User-Agent': 'Mozilla/5.0 (Android 11; Mobile; rv:90.0) Gecko/90.0 Firefox/90.0'} response = requests.get(url, headers=headers) webpage_response = response.text # Locate <title> and extract room ID title = webpage_response.find('title') id = webpage_response.find(title, '<room_id>', '</room_id>') room_url = webpage_response.find(id, '<a href', '\">Showroom</a></div>') roomid = webpage_response[room_url.find('https://www.showroom-live.com/room/') + len('https://www.showroom-live.com/room/'):room_url.find('/?')] return roomid\nThe provided code snippet does not appear to be correct Python code for fetching a Room ID from Showroom Live by providing a room URL key. The code attempts to extract the Room ID from the HTML"
      ]
    ],
    "filtered_resps": [
      "\nRoomID Finder Example\ndef showroom_get_roomid_by_room_url_key ( room_url_key ) : webpage_url = 'https://www.showroom-live.com/' + room_url_key # Get content using a User-Agent that identifies as a mobile browser url = 'https://www.showroom-live.com/' + room_url_key headers = {'User-Agent': 'Mozilla/5.0 (Android 11; Mobile; rv:90.0) Gecko/90.0 Firefox/90.0'} response = requests.get(url, headers=headers) webpage_response = response.text # Locate <title> and extract room ID title = webpage_response.find('title') id = webpage_response.find(title, '<room_id>', '</room_id>') room_url = webpage_response.find(id, '<a href', '\">Showroom</a></div>') roomid = webpage_response[room_url.find('https://www.showroom-live.com/room/') + len('https://www.showroom-live.com/room/'):room_url.find('/?')] return roomid\nThe provided code snippet does not appear to be correct Python code for fetching a Room ID from Showroom Live by providing a room URL key. The code attempts to extract the Room ID from the HTML"
    ],
    "smoothed_bleu_4": 0.3667574422283097
  },
  {
    "doc_id": 22,
    "doc": {
      "id": 22,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "_wanmen_get_title_by_json_topic_part",
      "original_string": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",
      "language": "python",
      "code": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",
      "code_tokens": [
        "def",
        "_wanmen_get_title_by_json_topic_part",
        "(",
        "json_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        ":",
        "return",
        "'_'",
        ".",
        "join",
        "(",
        "[",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'name'",
        "]",
        ",",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'name'",
        "]",
        ",",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'Parts'",
        "]",
        "[",
        "pIndex",
        "]",
        "[",
        "'name'",
        "]",
        "]",
        ")"
      ],
      "docstring": "JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.",
      "docstring_tokens": [
        "JSON",
        "int",
        "int",
        "int",
        "-",
        ">",
        "str",
        "Get",
        "a",
        "proper",
        "title",
        "with",
        "courseid",
        "+",
        "topicID",
        "+",
        "partID",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L18-L25"
    },
    "target": "JSON int int int - > str Get a proper title with courseid + topicID + partID .",
    "arguments": [
      [
        "def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : return '_' . join ( [ json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n### Solution 9: Determine the title of a WAN Men menu item\n\n```python\nimport json\n\ndef _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    topic = json_content[0]['Topics'][tIndex]\n    part = topic['Parts'][pIndex]\n    return f\"{json_content[0]['name']} - {topic['name']} - {part['name']}\"\n```\n\n### Example use case:\n\n```python\njson_content = '''\n{\n    \"name\": \"Wan Men\",\n    \"Topics\": [\n        {\"name\": \"Menu\", \"Parts\": [\n            {\"name\": \"Home\", \"order\": 1},\n            {\"name\": \"Settings\", \"order\": 2}\n        ]},\n        {\"name\": \"Orders\", \"Parts\": [\n            {\"name\": \"History\", \"order\": 3},\n            {\"name\": \"Place Order\", \"order\": 4}\n        ]}\n    ]\n}\n'''\n\nprint(_wanmen_get_title_by_json_topic_part(json_content, 0, 0))\n# Outputs: Wan Men - Menu - Home\n```\n\n### Step-by-step solution:\n\n1.  Import the necessary libraries. In this case,"
      ]
    ],
    "filtered_resps": [
      " ```\n### Solution 9: Determine the title of a WAN Men menu item\n\n```python\nimport json\n\ndef _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    topic = json_content[0]['Topics'][tIndex]\n    part = topic['Parts'][pIndex]\n    return f\"{json_content[0]['name']} - {topic['name']} - {part['name']}\"\n```\n\n### Example use case:\n\n```python\njson_content = '''\n{\n    \"name\": \"Wan Men\",\n    \"Topics\": [\n        {\"name\": \"Menu\", \"Parts\": [\n            {\"name\": \"Home\", \"order\": 1},\n            {\"name\": \"Settings\", \"order\": 2}\n        ]},\n        {\"name\": \"Orders\", \"Parts\": [\n            {\"name\": \"History\", \"order\": 3},\n            {\"name\": \"Place Order\", \"order\": 4}\n        ]}\n    ]\n}\n'''\n\nprint(_wanmen_get_title_by_json_topic_part(json_content, 0, 0))\n# Outputs: Wan Men - Menu - Home\n```\n\n### Step-by-step solution:\n\n1.  Import the necessary libraries. In this case,"
    ],
    "smoothed_bleu_4": 0.5110499008913731
  },
  {
    "doc_id": 23,
    "doc": {
      "id": 23,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "wanmen_download_by_course",
      "original_string": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)",
      "language": "python",
      "code": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)",
      "code_tokens": [
        "def",
        "wanmen_download_by_course",
        "(",
        "json_api_content",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "for",
        "tIndex",
        "in",
        "range",
        "(",
        "len",
        "(",
        "json_api_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        ")",
        ")",
        ":",
        "for",
        "pIndex",
        "in",
        "range",
        "(",
        "len",
        "(",
        "json_api_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'Parts'",
        "]",
        ")",
        ")",
        ":",
        "wanmen_download_by_course_topic_part",
        "(",
        "json_api_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.",
      "docstring_tokens": [
        "int",
        "-",
        ">",
        "None",
        "Download",
        "a",
        "WHOLE",
        "course",
        ".",
        "Reuse",
        "the",
        "API",
        "call",
        "to",
        "save",
        "time",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L37-L51"
    },
    "target": "int - > None Download a WHOLE course . Reuse the API call to save time .",
    "arguments": [
      [
        "def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : for pIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def wanmen_download_by_course_topic_part ( json_api_content, tIndex, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : print ( f 'Topic tIndex :{ tIndex } /{ len ( json_api_content [ 0 ] [ \"Topics\" ] ) }, PartIndex :{ pIndex } /{ len ( json_api_content [ 0 ] [ \"Topics\" ] [ tIndex ] [ \"Parts\" ] ) }' ) wanmen_download_part ( json_api_content, tIndex, pIndex, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs ) def wanmen_download_part ( json_api_content, TopicIndex, PartIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : # Try the following three methods:.mp4.webm.mp4/webm/playlist # The.playlist method fails and is commented out. try : filename = f '- - { json_api_content [ 0 ] [ \"Parts\" ] [ TopicIndex ] [ \"Parts\" ] [ PartIndex ] [ \"ItemInfo\" ] [ 'itemname' ]"
      ]
    ],
    "filtered_resps": [
      " def wanmen_download_by_course_topic_part ( json_api_content, tIndex, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : print ( f 'Topic tIndex :{ tIndex } /{ len ( json_api_content [ 0 ] [ \"Topics\" ] ) }, PartIndex :{ pIndex } /{ len ( json_api_content [ 0 ] [ \"Topics\" ] [ tIndex ] [ \"Parts\" ] ) }' ) wanmen_download_part ( json_api_content, tIndex, pIndex, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs ) def wanmen_download_part ( json_api_content, TopicIndex, PartIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : # Try the following three methods:.mp4.webm.mp4/webm/playlist # The.playlist method fails and is commented out. try : filename = f '- - { json_api_content [ 0 ] [ \"Parts\" ] [ TopicIndex ] [ \"Parts\" ] [ PartIndex ] [ \"ItemInfo\" ] [ 'itemname' ]"
    ],
    "smoothed_bleu_4": 0.6250085785948373
  },
  {
    "doc_id": 24,
    "doc": {
      "id": 24,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "wanmen_download_by_course_topic_part",
      "original_string": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",
      "language": "python",
      "code": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",
      "code_tokens": [
        "def",
        "wanmen_download_by_course_topic_part",
        "(",
        "json_api_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "json_api_content",
        "title",
        "=",
        "_wanmen_get_title_by_json_topic_part",
        "(",
        "html",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        "bokeccID",
        "=",
        "_wanmen_get_boke_id_by_json_topic_part",
        "(",
        "html",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        "bokecc_download_by_id",
        "(",
        "vid",
        "=",
        "bokeccID",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "int, int, int->None\n    \n    Download ONE PART of the course.",
      "docstring_tokens": [
        "int",
        "int",
        "int",
        "-",
        ">",
        "None",
        "Download",
        "ONE",
        "PART",
        "of",
        "the",
        "course",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L69-L84"
    },
    "target": "int int int - > None Download ONE PART of the course .",
    "arguments": [
      [
        "def wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_topic_part ( html , tIndex , pIndex ) bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , tIndex , pIndex ) bokecc_download_by_id ( vid = bokeccID , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return None '''''''''''''''''''''''''''''''''''' WANMEN download by course ID '''''''''''''''''''''''''''''''''''''' ww = Wanmenww = json. loadjson ( open ( ww, 'r' )) course_ID = ww [ 'courseId'] wanmen_download_by_course_ID ( courseID = course_ID, output_dir = '.', merge = True, info_only = False, * * kwargs ) return None\nWANMEN download by topic index and part index from Wanmen JSON file WANMEN download by topic index\nWANMEN download by part index from Wanmen JSON file WANMEN download by part index from JSON file\nWANMEN download by course ID from Wanmen file WANMEN download by course ID from Wanmen file\nWANMEN WANMEN WANMEN WANMEN\nWANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WNmen\nWANMEN WANMEN WANMEN\nWANMEN WANMEN WANMEN WANMEN\nWANMEN WANMEN WANMEN WANMEN\n"
      ]
    ],
    "filtered_resps": [
      " return None '''''''''''''''''''''''''''''''''''' WANMEN download by course ID '''''''''''''''''''''''''''''''''''''' ww = Wanmenww = json. loadjson ( open ( ww, 'r' )) course_ID = ww [ 'courseId'] wanmen_download_by_course_ID ( courseID = course_ID, output_dir = '.', merge = True, info_only = False, * * kwargs ) return None\nWANMEN download by topic index and part index from Wanmen JSON file WANMEN download by topic index\nWANMEN download by part index from Wanmen JSON file WANMEN download by part index from JSON file\nWANMEN download by course ID from Wanmen file WANMEN download by course ID from Wanmen file\nWANMEN WANMEN WANMEN WANMEN\nWANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WANMEN WNmen\nWANMEN WANMEN WANMEN\nWANMEN WANMEN WANMEN WANMEN\nWANMEN WANMEN WANMEN WANMEN\n"
    ],
    "smoothed_bleu_4": 0.6698137099134988
  },
  {
    "doc_id": 25,
    "doc": {
      "id": 25,
      "repo": "apache/airflow",
      "path": "airflow/executors/base_executor.py",
      "func_name": "BaseExecutor.has_task",
      "original_string": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
      "language": "python",
      "code": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
      "code_tokens": [
        "def",
        "has_task",
        "(",
        "self",
        ",",
        "task_instance",
        ")",
        ":",
        "if",
        "task_instance",
        ".",
        "key",
        "in",
        "self",
        ".",
        "queued_tasks",
        "or",
        "task_instance",
        ".",
        "key",
        "in",
        "self",
        ".",
        "running",
        ":",
        "return",
        "True"
      ],
      "docstring": "Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "task",
        "is",
        "either",
        "queued",
        "or",
        "running",
        "in",
        "this",
        "executor"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L97-L105"
    },
    "target": "Checks if a task is either queued or running in this executor",
    "arguments": [
      [
        "def has_task ( self , task_instance ) : if task_instance . key in self . queued_tasks or task_instance . key in self . running : return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return False def run_task ( self, task_instance ) : self. running. add ( task_instance. key ) print ( f 'Task '{ task_instance. key } started' ) # Simulate some task processing delay import time time. sleep ( 2 ) print ( f 'Task '{ task_instance. key } finished' ) self. running. remove ( task_instance. key ) def sync_tasks ( self ) : # Make tasks that haven't been run or cancelled to be queued # NOTE: This is a simplified example without a true database to handle cancelled tasks running_in_db = { task. key : True for task in self. db. tasks. query ( ) if task. status == 'running' } queued_ids_run_in_db = [ task. id for task in self. db. tasks. query ( ) if task. status == 'queued' and task. id in running_in_db ] self. running = set ( running_in_db. keys ( ) ) self. queued_tasks = set ( queued_ids_run_in_db )\n```\nIn this modified code, I have added comments and docstrings to improve readability and clarity. The `sync_tasks` method has been implemented to periodically check the database for running and queued tasks"
      ]
    ],
    "filtered_resps": [
      " return False def run_task ( self, task_instance ) : self. running. add ( task_instance. key ) print ( f 'Task '{ task_instance. key } started' ) # Simulate some task processing delay import time time. sleep ( 2 ) print ( f 'Task '{ task_instance. key } finished' ) self. running. remove ( task_instance. key ) def sync_tasks ( self ) : # Make tasks that haven't been run or cancelled to be queued # NOTE: This is a simplified example without a true database to handle cancelled tasks running_in_db = { task. key : True for task in self. db. tasks. query ( ) if task. status == 'running' } queued_ids_run_in_db = [ task. id for task in self. db. tasks. query ( ) if task. status == 'queued' and task. id in running_in_db ] self. running = set ( running_in_db. keys ( ) ) self. queued_tasks = set ( queued_ids_run_in_db )\n```\nIn this modified code, I have added comments and docstrings to improve readability and clarity. The `sync_tasks` method has been implemented to periodically check the database for running and queued tasks"
    ],
    "smoothed_bleu_4": 0.7483293365434089
  },
  {
    "doc_id": 26,
    "doc": {
      "id": 26,
      "repo": "apache/airflow",
      "path": "airflow/executors/base_executor.py",
      "func_name": "BaseExecutor.get_event_buffer",
      "original_string": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events",
      "language": "python",
      "code": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events",
      "code_tokens": [
        "def",
        "get_event_buffer",
        "(",
        "self",
        ",",
        "dag_ids",
        "=",
        "None",
        ")",
        ":",
        "cleared_events",
        "=",
        "dict",
        "(",
        ")",
        "if",
        "dag_ids",
        "is",
        "None",
        ":",
        "cleared_events",
        "=",
        "self",
        ".",
        "event_buffer",
        "self",
        ".",
        "event_buffer",
        "=",
        "dict",
        "(",
        ")",
        "else",
        ":",
        "for",
        "key",
        "in",
        "list",
        "(",
        "self",
        ".",
        "event_buffer",
        ".",
        "keys",
        "(",
        ")",
        ")",
        ":",
        "dag_id",
        ",",
        "_",
        ",",
        "_",
        ",",
        "_",
        "=",
        "key",
        "if",
        "dag_id",
        "in",
        "dag_ids",
        ":",
        "cleared_events",
        "[",
        "key",
        "]",
        "=",
        "self",
        ".",
        "event_buffer",
        ".",
        "pop",
        "(",
        "key",
        ")",
        "return",
        "cleared_events"
      ],
      "docstring": "Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events",
      "docstring_tokens": [
        "Returns",
        "and",
        "flush",
        "the",
        "event",
        "buffer",
        ".",
        "In",
        "case",
        "dag_ids",
        "is",
        "specified",
        "it",
        "will",
        "only",
        "return",
        "and",
        "flush",
        "events",
        "for",
        "the",
        "given",
        "dag_ids",
        ".",
        "Otherwise",
        "it",
        "returns",
        "and",
        "flushes",
        "all"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L160-L179"
    },
    "target": "Returns and flush the event buffer . In case dag_ids is specified it will only return and flush events for the given dag_ids . Otherwise it returns and flushes all",
    "arguments": [
      [
        "def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "[\u201c dag_id \u201d ] = key self. event_buffer [ key ] = event def update_event_buffer ( self, dag_id, event_dict, commit = True ) : if dag_id not in self. event_buffer : self. event_buffer [ dag_id ] = { } self. event_buffer [ dag_id ][ \u2019 timestamp \u2019 ] = event_dict [ \u201c timestamp \u201d ] self. event_buffer [ dag_id ][ \u2019 name \u2019 ] = event_dict [ \u201c name \u201d ] self. event_buffer [ dag_id ][ \u2019 clearable \u2019 ] = event_dict [ \u201c clearable \u201d ] self. event_buffer [ dag_id ][ \u2019 cleared \u2019 ] = event_dict [ \u201c cleared \u201d ] self. event_buffer [ dag_id ][ \u2019 id \u2019 ] = event_dict [ \u201c id \u201d ] if commit : await self. store ( self. event_buffer ) def close_event_buffer ( self ) : if self. event_buffer : await self. store ( self. event_buffer ) self. event_buffer = dict ( ) def close_all ( self ) : if hasattr ( self, \"state\" ) : self. state [ \"tasks\" ]. close_all ( ) if hasattr ( self, \"event_buffer\" ) : self. close_event_buffer ("
      ]
    ],
    "filtered_resps": [
      "[\u201c dag_id \u201d ] = key self. event_buffer [ key ] = event def update_event_buffer ( self, dag_id, event_dict, commit = True ) : if dag_id not in self. event_buffer : self. event_buffer [ dag_id ] = { } self. event_buffer [ dag_id ][ \u2019 timestamp \u2019 ] = event_dict [ \u201c timestamp \u201d ] self. event_buffer [ dag_id ][ \u2019 name \u2019 ] = event_dict [ \u201c name \u201d ] self. event_buffer [ dag_id ][ \u2019 clearable \u2019 ] = event_dict [ \u201c clearable \u201d ] self. event_buffer [ dag_id ][ \u2019 cleared \u2019 ] = event_dict [ \u201c cleared \u201d ] self. event_buffer [ dag_id ][ \u2019 id \u2019 ] = event_dict [ \u201c id \u201d ] if commit : await self. store ( self. event_buffer ) def close_event_buffer ( self ) : if self. event_buffer : await self. store ( self. event_buffer ) self. event_buffer = dict ( ) def close_all ( self ) : if hasattr ( self, \"state\" ) : self. state [ \"tasks\" ]. close_all ( ) if hasattr ( self, \"event_buffer\" ) : self. close_event_buffer ("
    ],
    "smoothed_bleu_4": 0.8119191712047447
  },
  {
    "doc_id": 27,
    "doc": {
      "id": 27,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/snowflake_hook.py",
      "func_name": "SnowflakeHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn_config",
        "=",
        "self",
        ".",
        "_get_conn_params",
        "(",
        ")",
        "conn",
        "=",
        "snowflake",
        ".",
        "connector",
        ".",
        "connect",
        "(",
        "*",
        "*",
        "conn_config",
        ")",
        "return",
        "conn"
      ],
      "docstring": "Returns a snowflake.connection object",
      "docstring_tokens": [
        "Returns",
        "a",
        "snowflake",
        ".",
        "connection",
        "object"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L107-L113"
    },
    "target": "Returns a snowflake . connection object",
    "arguments": [
      [
        "def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ;\nThe returned object of snowflake.connector.connect is of type an \u201cConnector\u201d and this object has various methods such as:\nExecute the given SQL command and returns the result in the form of DataFrame column oriented results\nExecute the given SQL command and returns the result in the form of Python dictionary\nExecute the given SQL command and returns the result in the form of list of\nExecute the given SQL command and returns the cursor for further execute of SQL commands.\nWrite the \u201cSQL\u201d commands into the Snowflake session\nTo get the cursor we use execute() method of connector object, to fetch the data we use fetch() and fetchone() method of cursor object, the result of fetch() and fetchone() method returns a list of\nTable of Operations that can be performed on Snowflake\nFollowing are few operations that can be performed on Snowflake\nOperation SQL Command Description\nCreate Database CREATE DATABASE \u2018somedatabase\u2019 This command will create a new Snowflake database.\nCreate Table CREATE TABLE \u201csometable\u201d (name STRING,vdate DATE ) This command will create a table named \u201csometable\u201d in Snowflake with the field \u201cname\u201d and \u201cvdate\u201d where \u201cname\u201d will be string data and \u201cvdate\u201d will"
      ]
    ],
    "filtered_resps": [
      " ;\nThe returned object of snowflake.connector.connect is of type an \u201cConnector\u201d and this object has various methods such as:\nExecute the given SQL command and returns the result in the form of DataFrame column oriented results\nExecute the given SQL command and returns the result in the form of Python dictionary\nExecute the given SQL command and returns the result in the form of list of\nExecute the given SQL command and returns the cursor for further execute of SQL commands.\nWrite the \u201cSQL\u201d commands into the Snowflake session\nTo get the cursor we use execute() method of connector object, to fetch the data we use fetch() and fetchone() method of cursor object, the result of fetch() and fetchone() method returns a list of\nTable of Operations that can be performed on Snowflake\nFollowing are few operations that can be performed on Snowflake\nOperation SQL Command Description\nCreate Database CREATE DATABASE \u2018somedatabase\u2019 This command will create a new Snowflake database.\nCreate Table CREATE TABLE \u201csometable\u201d (name STRING,vdate DATE ) This command will create a table named \u201csometable\u201d in Snowflake with the field \u201cname\u201d and \u201cvdate\u201d where \u201cname\u201d will be string data and \u201cvdate\u201d will"
    ],
    "smoothed_bleu_4": 0.8437308125603296
  },
  {
    "doc_id": 28,
    "doc": {
      "id": 28,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/snowflake_hook.py",
      "func_name": "SnowflakeHook._get_aws_credentials",
      "original_string": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key",
      "language": "python",
      "code": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key",
      "code_tokens": [
        "def",
        "_get_aws_credentials",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "snowflake_conn_id",
        ":",
        "connection_object",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "snowflake_conn_id",
        ")",
        "if",
        "'aws_secret_access_key'",
        "in",
        "connection_object",
        ".",
        "extra_dejson",
        ":",
        "aws_access_key_id",
        "=",
        "connection_object",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'aws_access_key_id'",
        ")",
        "aws_secret_access_key",
        "=",
        "connection_object",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'aws_secret_access_key'",
        ")",
        "return",
        "aws_access_key_id",
        ",",
        "aws_secret_access_key"
      ],
      "docstring": "returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements",
      "docstring_tokens": [
        "returns",
        "aws_access_key_id",
        "aws_secret_access_key",
        "from",
        "extra"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L115-L129"
    },
    "target": "returns aws_access_key_id aws_secret_access_key from extra",
    "arguments": [
      [
        "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " elif self. snowflake_aws_credentials : if isinstance ( self. snowflake_aws_credentials, str ) : value = json. loads ( self. snowflake_aws_credentials ) else : value = self. snowflake_aws_credentials aws_access_key_id = value. get ( 'aws_access_key_id' ) aws_secret_access_key = value. get ( 'aws_secret_access_key' ) return aws_access_key_id, aws_secret_access_key def _get_default_aws_credentials ( self ) : try : client = boto3. client ('sts', aws_access_key_id ='XXXXXXXXXXXXXXXX ', aws_secret_access_key ='YYYYYYYYYYYYYYYYYY ', region_name ='XXXXXXXX') client. get_caller_identity ( ) except ( botocore. exceptions. ClientError, boto3. exceptions. BotocoreConnectionError ) : return return'XXXXXXXXXXXXXXXX ','YYYYYYYYYYYYYYYYYY'def _s3_get_object ( self, s3_object_key ) : from io_senselib. s3 import S3 ( aws_access_key_id = self. aws_access_key_id, aws_secret_access_key = self. aws_secret_access_key ) s3"
      ]
    ],
    "filtered_resps": [
      " elif self. snowflake_aws_credentials : if isinstance ( self. snowflake_aws_credentials, str ) : value = json. loads ( self. snowflake_aws_credentials ) else : value = self. snowflake_aws_credentials aws_access_key_id = value. get ( 'aws_access_key_id' ) aws_secret_access_key = value. get ( 'aws_secret_access_key' ) return aws_access_key_id, aws_secret_access_key def _get_default_aws_credentials ( self ) : try : client = boto3. client ('sts', aws_access_key_id ='XXXXXXXXXXXXXXXX ', aws_secret_access_key ='YYYYYYYYYYYYYYYYYY ', region_name ='XXXXXXXX') client. get_caller_identity ( ) except ( botocore. exceptions. ClientError, boto3. exceptions. BotocoreConnectionError ) : return return'XXXXXXXXXXXXXXXX ','YYYYYYYYYYYYYYYYYY'def _s3_get_object ( self, s3_object_key ) : from io_senselib. s3 import S3 ( aws_access_key_id = self. aws_access_key_id, aws_secret_access_key = self. aws_secret_access_key ) s3"
    ],
    "smoothed_bleu_4": 4.529277395018578
  },
  {
    "doc_id": 29,
    "doc": {
      "id": 29,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/grpc_hook.py",
      "func_name": "GrpcHook._get_field",
      "original_string": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
      "language": "python",
      "code": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
      "code_tokens": [
        "def",
        "_get_field",
        "(",
        "self",
        ",",
        "field_name",
        ",",
        "default",
        "=",
        "None",
        ")",
        ":",
        "full_field_name",
        "=",
        "'extra__grpc__{}'",
        ".",
        "format",
        "(",
        "field_name",
        ")",
        "if",
        "full_field_name",
        "in",
        "self",
        ".",
        "extras",
        ":",
        "return",
        "self",
        ".",
        "extras",
        "[",
        "full_field_name",
        "]",
        "else",
        ":",
        "return",
        "default"
      ],
      "docstring": "Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.",
      "docstring_tokens": [
        "Fetches",
        "a",
        "field",
        "from",
        "extras",
        "and",
        "returns",
        "it",
        ".",
        "This",
        "is",
        "some",
        "Airflow",
        "magic",
        ".",
        "The",
        "grpc",
        "hook",
        "type",
        "adds",
        "custom",
        "UI",
        "elements",
        "to",
        "the",
        "hook",
        "page",
        "which",
        "allow",
        "admins",
        "to",
        "specify",
        "scopes",
        "credential",
        "pem",
        "files",
        "etc",
        ".",
        "They",
        "get",
        "formatted",
        "as",
        "shown",
        "below",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/grpc_hook.py#L112-L123"
    },
    "target": "Fetches a field from extras and returns it . This is some Airflow magic . The grpc hook type adds custom UI elements to the hook page which allow admins to specify scopes credential pem files etc . They get formatted as shown below .",
    "arguments": [
      [
        "def _get_field ( self , field_name , default = None ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\ndef _process_proto_name ( self, proto_name ) : \"\"\" Process the ProtoField name to match the corresponding attribute in the dict. \"\"\" # Map the _ to camelCase mapping for proto field names attribute = proto_name. lower (). lstrip ( '_' ) # Check if we need to use grpc. proto. to_json_field_name to convert to a JSON - like field name if attribute == 'value' or attribute == 'type' : attribute = grpc. proto. to_json_field_name ( attribute ) return attribute\n```\n\nThis snippet shows how the `_process_proto_name` method maps the proto field name from camelCase to snake_case, with the `_process_proto_name` method taking a proto_name as input and returning the transformed attribute name.\n\nTo fix the bug, you should ensure that the attributes used in the `__getitem__` method are properly mapped from camelCase to snake_case. Since the `_process_proto_name` method already achieves this, there is no actual bug that needs fixing. The code snippet shown only represents a part of the class implementation and does not demonstrate a bug. The bug assumption was incorrect. \n\nNote that the actual solution to your original question would depend on understanding the specific requirements of the `__"
      ]
    ],
    "filtered_resps": [
      "\ndef _process_proto_name ( self, proto_name ) : \"\"\" Process the ProtoField name to match the corresponding attribute in the dict. \"\"\" # Map the _ to camelCase mapping for proto field names attribute = proto_name. lower (). lstrip ( '_' ) # Check if we need to use grpc. proto. to_json_field_name to convert to a JSON - like field name if attribute == 'value' or attribute == 'type' : attribute = grpc. proto. to_json_field_name ( attribute ) return attribute\n```\n\nThis snippet shows how the `_process_proto_name` method maps the proto field name from camelCase to snake_case, with the `_process_proto_name` method taking a proto_name as input and returning the transformed attribute name.\n\nTo fix the bug, you should ensure that the attributes used in the `__getitem__` method are properly mapped from camelCase to snake_case. Since the `_process_proto_name` method already achieves this, there is no actual bug that needs fixing. The code snippet shown only represents a part of the class implementation and does not demonstrate a bug. The bug assumption was incorrect. \n\nNote that the actual solution to your original question would depend on understanding the specific requirements of the `__"
    ],
    "smoothed_bleu_4": 0.8866971175723697
  },
  {
    "doc_id": 30,
    "doc": {
      "id": 30,
      "repo": "apache/airflow",
      "path": "airflow/hooks/postgres_hook.py",
      "func_name": "PostgresHook.copy_expert",
      "original_string": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()",
      "language": "python",
      "code": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()",
      "code_tokens": [
        "def",
        "copy_expert",
        "(",
        "self",
        ",",
        "sql",
        ",",
        "filename",
        ",",
        "open",
        "=",
        "open",
        ")",
        ":",
        "if",
        "not",
        "os",
        ".",
        "path",
        ".",
        "isfile",
        "(",
        "filename",
        ")",
        ":",
        "with",
        "open",
        "(",
        "filename",
        ",",
        "'w'",
        ")",
        ":",
        "pass",
        "with",
        "open",
        "(",
        "filename",
        ",",
        "'r+'",
        ")",
        "as",
        "f",
        ":",
        "with",
        "closing",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ")",
        "as",
        "conn",
        ":",
        "with",
        "closing",
        "(",
        "conn",
        ".",
        "cursor",
        "(",
        ")",
        ")",
        "as",
        "cur",
        ":",
        "cur",
        ".",
        "copy_expert",
        "(",
        "sql",
        ",",
        "f",
        ")",
        "f",
        ".",
        "truncate",
        "(",
        "f",
        ".",
        "tell",
        "(",
        ")",
        ")",
        "conn",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.",
      "docstring_tokens": [
        "Executes",
        "SQL",
        "using",
        "psycopg2",
        "copy_expert",
        "method",
        ".",
        "Necessary",
        "to",
        "execute",
        "COPY",
        "command",
        "without",
        "access",
        "to",
        "a",
        "superuser",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L63-L83"
    },
    "target": "Executes SQL using psycopg2 copy_expert method . Necessary to execute COPY command without access to a superuser .",
    "arguments": [
      [
        "def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur . copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return f def load_expert ( self, filename, open = open ) : with open ( filename, 'r' ) as f : with closing ( self. get_conn ( ) ) as conn : with closing ( conn. cursor ( ) ) as cur : cur. copy_expert ( f. read ( ), f ) conn. commit ( ) filehandle = None for line in cur : filehandle = line try : filehandle. to_featurized_string ( write_file = False ) except Exception as e : try : filehandle. to_csv_file ( filename, open ) except Exception as e : print ( e ) finally : if not filehandle is None : filehandle. close ( ) return conn if self. get_settings ( ). enable_custom_field_filter : field_filter_str = self. get_filtered_fieldfilter ( ) with open ( filename, 'w' as write_file ) as f : f. write ( field_filter_str ) def init_stats ( self ) : self. last_open_count = 0 self. last_read_count = 0 self. last_count = 0 if self. get_settings ( ). statistics : self. last_open_count = self. cnt_get_open_count ( ) self. last"
      ]
    ],
    "filtered_resps": [
      " return f def load_expert ( self, filename, open = open ) : with open ( filename, 'r' ) as f : with closing ( self. get_conn ( ) ) as conn : with closing ( conn. cursor ( ) ) as cur : cur. copy_expert ( f. read ( ), f ) conn. commit ( ) filehandle = None for line in cur : filehandle = line try : filehandle. to_featurized_string ( write_file = False ) except Exception as e : try : filehandle. to_csv_file ( filename, open ) except Exception as e : print ( e ) finally : if not filehandle is None : filehandle. close ( ) return conn if self. get_settings ( ). enable_custom_field_filter : field_filter_str = self. get_filtered_fieldfilter ( ) with open ( filename, 'w' as write_file ) as f : f. write ( field_filter_str ) def init_stats ( self ) : self. last_open_count = 0 self. last_read_count = 0 self. last_count = 0 if self. get_settings ( ). statistics : self. last_open_count = self. cnt_get_open_count ( ) self. last"
    ],
    "smoothed_bleu_4": 0.9561473248298862
  },
  {
    "doc_id": 31,
    "doc": {
      "id": 31,
      "repo": "apache/airflow",
      "path": "airflow/hooks/postgres_hook.py",
      "func_name": "PostgresHook.bulk_dump",
      "original_string": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)",
      "language": "python",
      "code": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)",
      "code_tokens": [
        "def",
        "bulk_dump",
        "(",
        "self",
        ",",
        "table",
        ",",
        "tmp_file",
        ")",
        ":",
        "self",
        ".",
        "copy_expert",
        "(",
        "\"COPY {table} TO STDOUT\"",
        ".",
        "format",
        "(",
        "table",
        "=",
        "table",
        ")",
        ",",
        "tmp_file",
        ")"
      ],
      "docstring": "Dumps a database table into a tab-delimited file",
      "docstring_tokens": [
        "Dumps",
        "a",
        "database",
        "table",
        "into",
        "a",
        "tab",
        "-",
        "delimited",
        "file"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L91-L95"
    },
    "target": "Dumps a database table into a tab - delimited file",
    "arguments": [
      [
        "def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( \"COPY {table} TO STDOUT\" . format ( table = table ) , tmp_file )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". execute ( ) \u00a0\n\u00a0 def copy_expert ( self, command, tmp_file ) : tmp = self. tmp_copy_to ( command ) ; tmp. copy_to ( tmp_file ) ; tmp. execute ( ) \u00a0\n\u00a0 def tmp_copy_to ( self, command ) : tmp = copy ( ) ; tmp. from_csv ( \"\\x00\". join ( [ x + \" \\x00 \" for x in self. execute ( command ). split ( ) ] ) ) ; return tmp \u00a0\n```\nThe code is a subset of the pgtcl package in PostgreSQL. This set of functions allows for simple copy operations of data by sending this data directly to an external'stdout' device on Unix.\nThis is a direct copying functionality which simply transmits the SQL data, column by column and row by row. The `tmp` object is created and the `copy` function is called on tmp with this specific data source file, the resulting object is again passed into the tmp.execute() method.\nThis set is called when we want to dump table\u2019s data directly into a file, or append data into a file that we already dumped data into. Let me present the copy functions.\n\n## The Copy Functions\nThere are two types of"
      ]
    ],
    "filtered_resps": [
      ". execute ( ) \u00a0\n\u00a0 def copy_expert ( self, command, tmp_file ) : tmp = self. tmp_copy_to ( command ) ; tmp. copy_to ( tmp_file ) ; tmp. execute ( ) \u00a0\n\u00a0 def tmp_copy_to ( self, command ) : tmp = copy ( ) ; tmp. from_csv ( \"\\x00\". join ( [ x + \" \\x00 \" for x in self. execute ( command ). split ( ) ] ) ) ; return tmp \u00a0\n```\nThe code is a subset of the pgtcl package in PostgreSQL. This set of functions allows for simple copy operations of data by sending this data directly to an external'stdout' device on Unix.\nThis is a direct copying functionality which simply transmits the SQL data, column by column and row by row. The `tmp` object is created and the `copy` function is called on tmp with this specific data source file, the resulting object is again passed into the tmp.execute() method.\nThis set is called when we want to dump table\u2019s data directly into a file, or append data into a file that we already dumped data into. Let me present the copy functions.\n\n## The Copy Functions\nThere are two types of"
    ],
    "smoothed_bleu_4": 0.7049708730909486
  },
  {
    "doc_id": 32,
    "doc": {
      "id": 32,
      "repo": "apache/airflow",
      "path": "airflow/contrib/operators/file_to_gcs.py",
      "func_name": "FileToGoogleCloudStorageOperator.execute",
      "original_string": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
      "language": "python",
      "code": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
      "code_tokens": [
        "def",
        "execute",
        "(",
        "self",
        ",",
        "context",
        ")",
        ":",
        "hook",
        "=",
        "GoogleCloudStorageHook",
        "(",
        "google_cloud_storage_conn_id",
        "=",
        "self",
        ".",
        "google_cloud_storage_conn_id",
        ",",
        "delegate_to",
        "=",
        "self",
        ".",
        "delegate_to",
        ")",
        "hook",
        ".",
        "upload",
        "(",
        "bucket_name",
        "=",
        "self",
        ".",
        "bucket",
        ",",
        "object_name",
        "=",
        "self",
        ".",
        "dst",
        ",",
        "mime_type",
        "=",
        "self",
        ".",
        "mime_type",
        ",",
        "filename",
        "=",
        "self",
        ".",
        "src",
        ",",
        "gzip",
        "=",
        "self",
        ".",
        "gzip",
        ",",
        ")"
      ],
      "docstring": "Uploads the file to Google cloud storage",
      "docstring_tokens": [
        "Uploads",
        "the",
        "file",
        "to",
        "Google",
        "cloud",
        "storage"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_gcs.py#L68-L82"
    },
    "target": "Uploads the file to Google cloud storage",
    "arguments": [
      [
        "def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return hook. return_code, hook. logs\n```\n\nAlso, the `run_threaded_execute` method can be removed as it's not being used in the latest version of the code.\n\nThe `__init__` method can be simplified to only include the necessary keyword arguments.\n\nHere is the simplified code:\n\n```python\nfrom apache-airflow.providers.google.cloud.operators.gcs import GoogleCloudStorageHook\n\nclass GoogleCloudStorageToGridFSOperator ( BaseOperator ) :\n    template_fields = ('src', 'dst', 'bucket', 'google_cloud_storage_conn_id' )\n\n    def __init__ ( self, src, dst, bucket, google_cloud_storage_conn_id = None, mime_type = 'application/octet-stream', gzip = False, delegate_to = None, **kwargs ) :\n        super ( ). __init__ ( **kwargs )\n        self. src = src\n        self. dst_key = dst\n        self. bucket = bucket\n        self. google_cloud_storage_conn_id = google_cloud_cloud_storage_conn_id or DEFAULT_CONN_ID\n        self. mime_type = mime_type\n        self. gzip = gzip\n        self. delegate_to = delegate_to\n\n    def pre_execute ( self"
      ]
    ],
    "filtered_resps": [
      " return hook. return_code, hook. logs\n```\n\nAlso, the `run_threaded_execute` method can be removed as it's not being used in the latest version of the code.\n\nThe `__init__` method can be simplified to only include the necessary keyword arguments.\n\nHere is the simplified code:\n\n```python\nfrom apache-airflow.providers.google.cloud.operators.gcs import GoogleCloudStorageHook\n\nclass GoogleCloudStorageToGridFSOperator ( BaseOperator ) :\n    template_fields = ('src', 'dst', 'bucket', 'google_cloud_storage_conn_id' )\n\n    def __init__ ( self, src, dst, bucket, google_cloud_storage_conn_id = None, mime_type = 'application/octet-stream', gzip = False, delegate_to = None, **kwargs ) :\n        super ( ). __init__ ( **kwargs )\n        self. src = src\n        self. dst_key = dst\n        self. bucket = bucket\n        self. google_cloud_storage_conn_id = google_cloud_cloud_storage_conn_id or DEFAULT_CONN_ID\n        self. mime_type = mime_type\n        self. gzip = gzip\n        self. delegate_to = delegate_to\n\n    def pre_execute ( self"
    ],
    "smoothed_bleu_4": 0.5637534758608833
  },
  {
    "doc_id": 33,
    "doc": {
      "id": 33,
      "repo": "apache/airflow",
      "path": "airflow/macros/hive.py",
      "func_name": "max_partition",
      "original_string": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)",
      "language": "python",
      "code": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)",
      "code_tokens": [
        "def",
        "max_partition",
        "(",
        "table",
        ",",
        "schema",
        "=",
        "\"default\"",
        ",",
        "field",
        "=",
        "None",
        ",",
        "filter_map",
        "=",
        "None",
        ",",
        "metastore_conn_id",
        "=",
        "'metastore_default'",
        ")",
        ":",
        "from",
        "airflow",
        ".",
        "hooks",
        ".",
        "hive_hooks",
        "import",
        "HiveMetastoreHook",
        "if",
        "'.'",
        "in",
        "table",
        ":",
        "schema",
        ",",
        "table",
        "=",
        "table",
        ".",
        "split",
        "(",
        "'.'",
        ")",
        "hh",
        "=",
        "HiveMetastoreHook",
        "(",
        "metastore_conn_id",
        "=",
        "metastore_conn_id",
        ")",
        "return",
        "hh",
        ".",
        "max_partition",
        "(",
        "schema",
        "=",
        "schema",
        ",",
        "table_name",
        "=",
        "table",
        ",",
        "field",
        "=",
        "field",
        ",",
        "filter_map",
        "=",
        "filter_map",
        ")"
      ],
      "docstring": "Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'",
      "docstring_tokens": [
        "Gets",
        "the",
        "max",
        "partition",
        "for",
        "a",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L23-L55"
    },
    "target": "Gets the max partition for a table .",
    "arguments": [
      [
        "def max_partition ( table , schema = \"default\" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nHere is the breakdown of how this code works:\n\n1.  **Function signature**: The function `max_partition` takes four parameters: `table`, `schema`, `field`, and `filter_map`. The `table` and `schema` parameters have a default value of `'default'`.\n\n2.  **Importing necessary libraries**: The code imports `HiveMetastoreHook` from the `airflow.hooks.hive_hooks` module.\n\n3.  **Logic for handling table names**: If the `table` parameter contains a dot (`.`), it assumes the table name is in the format `schema.table` and splits it into two parts. Otherwise, it assigns the entire `table` parameter to both the `schema` and `table` variables.\n\n4.  **Creating an instance of `HiveMetastoreHook`**: It creates an instance of the `HiveMetastoreHook` class, passing the `metastore_conn_id` as an argument. The `metastore_conn_id` defaults to `'metastore_default'`.\n\n5.  **Calling the `max_partition` method**: It calls the `max_partition` method on the `HiveMetastoreHook"
      ]
    ],
    "filtered_resps": [
      " ```\n\nHere is the breakdown of how this code works:\n\n1.  **Function signature**: The function `max_partition` takes four parameters: `table`, `schema`, `field`, and `filter_map`. The `table` and `schema` parameters have a default value of `'default'`.\n\n2.  **Importing necessary libraries**: The code imports `HiveMetastoreHook` from the `airflow.hooks.hive_hooks` module.\n\n3.  **Logic for handling table names**: If the `table` parameter contains a dot (`.`), it assumes the table name is in the format `schema.table` and splits it into two parts. Otherwise, it assigns the entire `table` parameter to both the `schema` and `table` variables.\n\n4.  **Creating an instance of `HiveMetastoreHook`**: It creates an instance of the `HiveMetastoreHook` class, passing the `metastore_conn_id` as an argument. The `metastore_conn_id` defaults to `'metastore_default'`.\n\n5.  **Calling the `max_partition` method**: It calls the `max_partition` method on the `HiveMetastoreHook"
    ],
    "smoothed_bleu_4": 0.6397581988828135
  },
  {
    "doc_id": 34,
    "doc": {
      "id": 34,
      "repo": "apache/airflow",
      "path": "airflow/hooks/mysql_hook.py",
      "func_name": "MySqlHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "mysql_conn_id",
        ")",
        "conn_config",
        "=",
        "{",
        "\"user\"",
        ":",
        "conn",
        ".",
        "login",
        ",",
        "\"passwd\"",
        ":",
        "conn",
        ".",
        "password",
        "or",
        "''",
        ",",
        "\"host\"",
        ":",
        "conn",
        ".",
        "host",
        "or",
        "'localhost'",
        ",",
        "\"db\"",
        ":",
        "self",
        ".",
        "schema",
        "or",
        "conn",
        ".",
        "schema",
        "or",
        "''",
        "}",
        "if",
        "not",
        "conn",
        ".",
        "port",
        ":",
        "conn_config",
        "[",
        "\"port\"",
        "]",
        "=",
        "3306",
        "else",
        ":",
        "conn_config",
        "[",
        "\"port\"",
        "]",
        "=",
        "int",
        "(",
        "conn",
        ".",
        "port",
        ")",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'charset'",
        ",",
        "False",
        ")",
        ":",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"charset\"",
        "]",
        "if",
        "(",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'utf8'",
        "or",
        "(",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'utf-8'",
        ":",
        "conn_config",
        "[",
        "\"use_unicode\"",
        "]",
        "=",
        "True",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'cursor'",
        ",",
        "False",
        ")",
        ":",
        "if",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'sscursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "SSCursor",
        "elif",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'dictcursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "DictCursor",
        "elif",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'ssdictcursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "SSDictCursor",
        "local_infile",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'local_infile'",
        ",",
        "False",
        ")",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'ssl'",
        ",",
        "False",
        ")",
        ":",
        "# SSL parameter for MySQL has to be a dictionary and in case",
        "# of extra/dejson we can get string if extra is passed via",
        "# URL parameters",
        "dejson_ssl",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "'ssl'",
        "]",
        "if",
        "isinstance",
        "(",
        "dejson_ssl",
        ",",
        "six",
        ".",
        "string_types",
        ")",
        ":",
        "dejson_ssl",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "dejson_ssl",
        ")",
        "conn_config",
        "[",
        "'ssl'",
        "]",
        "=",
        "dejson_ssl",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'unix_socket'",
        ")",
        ":",
        "conn_config",
        "[",
        "'unix_socket'",
        "]",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "'unix_socket'",
        "]",
        "if",
        "local_infile",
        ":",
        "conn_config",
        "[",
        "\"local_infile\"",
        "]",
        "=",
        "1",
        "conn",
        "=",
        "MySQLdb",
        ".",
        "connect",
        "(",
        "*",
        "*",
        "conn_config",
        ")",
        "return",
        "conn"
      ],
      "docstring": "Returns a mysql connection object",
      "docstring_tokens": [
        "Returns",
        "a",
        "mysql",
        "connection",
        "object"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L62-L105"
    },
    "target": "Returns a mysql connection object",
    "arguments": [
      [
        "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn_config [ \"port\" ] = 3306 else : conn_config [ \"port\" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn_config [ \"use_unicode\" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : # SSL parameter for MySQL has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # URL parameters dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ \"local_infile\" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` As you can see, the function returns a MySQLdb object, which you should then use to execute your queries. For example, in a function: ``` def execute_query ( self, query ) : self. conn = get_conn ( self ) try : result = self. conn. cursor ( ). fetchall ( ) self. conn. commit ( ) except Exception as e : self. conn. rollback ( ) raise return result ``` You can replace the `execute_query` function with your actual SQL query. Keep in mind that database connection best practices include keeping the database connection open for only as long as necessary, and then closing it. In this example, the connection is not explicitly closed, but it will be released when the function returns. However, you might want to add `self.conn.close()` at the end of `get_conn` or in the `execute_query` function to close the connection after use. Also, this code assumes you are using MySQLdb, but you might want to use the more modern and SQL-92 compliant `mysql-connector-python` library. The code should be easy to adapt to that library as well. Here is how you can use this library: ``` from  mysql  import  connector class MySQL"
      ]
    ],
    "filtered_resps": [
      " ``` As you can see, the function returns a MySQLdb object, which you should then use to execute your queries. For example, in a function: ``` def execute_query ( self, query ) : self. conn = get_conn ( self ) try : result = self. conn. cursor ( ). fetchall ( ) self. conn. commit ( ) except Exception as e : self. conn. rollback ( ) raise return result ``` You can replace the `execute_query` function with your actual SQL query. Keep in mind that database connection best practices include keeping the database connection open for only as long as necessary, and then closing it. In this example, the connection is not explicitly closed, but it will be released when the function returns. However, you might want to add `self.conn.close()` at the end of `get_conn` or in the `execute_query` function to close the connection after use. Also, this code assumes you are using MySQLdb, but you might want to use the more modern and SQL-92 compliant `mysql-connector-python` library. The code should be easy to adapt to that library as well. Here is how you can use this library: ``` from  mysql  import  connector class MySQL"
    ],
    "smoothed_bleu_4": 0.678088963787935
  },
  {
    "doc_id": 35,
    "doc": {
      "id": 35,
      "repo": "apache/airflow",
      "path": "airflow/bin/cli.py",
      "func_name": "task_state",
      "original_string": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
      "language": "python",
      "code": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
      "code_tokens": [
        "def",
        "task_state",
        "(",
        "args",
        ")",
        ":",
        "dag",
        "=",
        "get_dag",
        "(",
        "args",
        ")",
        "task",
        "=",
        "dag",
        ".",
        "get_task",
        "(",
        "task_id",
        "=",
        "args",
        ".",
        "task_id",
        ")",
        "ti",
        "=",
        "TaskInstance",
        "(",
        "task",
        ",",
        "args",
        ".",
        "execution_date",
        ")",
        "print",
        "(",
        "ti",
        ".",
        "current_state",
        "(",
        ")",
        ")"
      ],
      "docstring": "Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success",
      "docstring_tokens": [
        "Returns",
        "the",
        "state",
        "of",
        "a",
        "TaskInstance",
        "at",
        "the",
        "command",
        "line",
        ".",
        ">>>",
        "airflow",
        "task_state",
        "tutorial",
        "sleep",
        "2015",
        "-",
        "01",
        "-",
        "01",
        "success"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L554-L563"
    },
    "target": "Returns the state of a TaskInstance at the command line . >>> airflow task_state tutorial sleep 2015 - 01 - 01 success",
    "arguments": [
      [
        "def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args . execution_date ) print ( ti . current_state ( ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " The code must be run inside a DAG. The `task_id` argument is required. The `execution_date` argument is optionally specified. If not specified, it uses the next available execution date for the task. You should get the task state of a task from a DAG. You should provide a dag to perform the task. You should provide the task id to perform the task. You should provide the execution date to perform the task.\n5 : - :mod: `airflow.providers.utils` *******\nAirflow providers utility methods.  Airflow provides several utility methods for interacting with providers.  You can find more information on providers in the section :ref:`providers_toc`.  The main utility class in the providers library is :class: `ProviderConnection`, which provides methods to interact with connections in Airflow... class:: ProviderConnection < Airflow/providers/utils.py > :param str provider: the provider name :param str conn_id: the connection id :param ~airflow.providers.base.module.BaseProvider base provider :rarr: str conn_id :rarr: str provider :rarr: ~airflow.providers.base.module.BaseProvider base provider : ~airflow.providers.utils.ProviderConnection : The main utility class in"
      ]
    ],
    "filtered_resps": [
      " The code must be run inside a DAG. The `task_id` argument is required. The `execution_date` argument is optionally specified. If not specified, it uses the next available execution date for the task. You should get the task state of a task from a DAG. You should provide a dag to perform the task. You should provide the task id to perform the task. You should provide the execution date to perform the task.\n5 : - :mod: `airflow.providers.utils` *******\nAirflow providers utility methods.  Airflow provides several utility methods for interacting with providers.  You can find more information on providers in the section :ref:`providers_toc`.  The main utility class in the providers library is :class: `ProviderConnection`, which provides methods to interact with connections in Airflow... class:: ProviderConnection < Airflow/providers/utils.py > :param str provider: the provider name :param str conn_id: the connection id :param ~airflow.providers.base.module.BaseProvider base provider :rarr: str conn_id :rarr: str provider :rarr: ~airflow.providers.base.module.BaseProvider base provider : ~airflow.providers.utils.ProviderConnection : The main utility class in"
    ],
    "smoothed_bleu_4": 1.1859942369175667
  },
  {
    "doc_id": 36,
    "doc": {
      "id": 36,
      "repo": "apache/airflow",
      "path": "airflow/bin/cli.py",
      "func_name": "restart_workers",
      "original_string": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)",
      "language": "python",
      "code": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)",
      "code_tokens": [
        "def",
        "restart_workers",
        "(",
        "gunicorn_master_proc",
        ",",
        "num_workers_expected",
        ",",
        "master_timeout",
        ")",
        ":",
        "def",
        "wait_until_true",
        "(",
        "fn",
        ",",
        "timeout",
        "=",
        "0",
        ")",
        ":",
        "\"\"\"\n        Sleeps until fn is true\n        \"\"\"",
        "t",
        "=",
        "time",
        ".",
        "time",
        "(",
        ")",
        "while",
        "not",
        "fn",
        "(",
        ")",
        ":",
        "if",
        "0",
        "<",
        "timeout",
        "<=",
        "time",
        ".",
        "time",
        "(",
        ")",
        "-",
        "t",
        ":",
        "raise",
        "AirflowWebServerTimeout",
        "(",
        "\"No response from gunicorn master within {0} seconds\"",
        ".",
        "format",
        "(",
        "timeout",
        ")",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "0.1",
        ")",
        "def",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        ":",
        "batch_size",
        "=",
        "conf",
        ".",
        "getint",
        "(",
        "'webserver'",
        ",",
        "'worker_refresh_batch_size'",
        ")",
        "log",
        ".",
        "debug",
        "(",
        "'%s doing a refresh of %s workers'",
        ",",
        "state",
        ",",
        "batch_size",
        ")",
        "sys",
        ".",
        "stdout",
        ".",
        "flush",
        "(",
        ")",
        "sys",
        ".",
        "stderr",
        ".",
        "flush",
        "(",
        ")",
        "excess",
        "=",
        "0",
        "for",
        "_",
        "in",
        "range",
        "(",
        "batch_size",
        ")",
        ":",
        "gunicorn_master_proc",
        ".",
        "send_signal",
        "(",
        "signal",
        ".",
        "SIGTTIN",
        ")",
        "excess",
        "+=",
        "1",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "+",
        "excess",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "try",
        ":",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "while",
        "True",
        ":",
        "num_workers_running",
        "=",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        "num_ready_workers_running",
        "=",
        "get_num_ready_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        "state",
        "=",
        "'[{0} / {1}]'",
        ".",
        "format",
        "(",
        "num_ready_workers_running",
        ",",
        "num_workers_running",
        ")",
        "# Whenever some workers are not ready, wait until all workers are ready",
        "if",
        "num_ready_workers_running",
        "<",
        "num_workers_running",
        ":",
        "log",
        ".",
        "debug",
        "(",
        "'%s some workers are starting up, waiting...'",
        ",",
        "state",
        ")",
        "sys",
        ".",
        "stdout",
        ".",
        "flush",
        "(",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "1",
        ")",
        "# Kill a worker gracefully by asking gunicorn to reduce number of workers",
        "elif",
        "num_workers_running",
        ">",
        "num_workers_expected",
        ":",
        "excess",
        "=",
        "num_workers_running",
        "-",
        "num_workers_expected",
        "log",
        ".",
        "debug",
        "(",
        "'%s killing %s workers'",
        ",",
        "state",
        ",",
        "excess",
        ")",
        "for",
        "_",
        "in",
        "range",
        "(",
        "excess",
        ")",
        ":",
        "gunicorn_master_proc",
        ".",
        "send_signal",
        "(",
        "signal",
        ".",
        "SIGTTOU",
        ")",
        "excess",
        "-=",
        "1",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "+",
        "excess",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "# Start a new worker by asking gunicorn to increase number of workers",
        "elif",
        "num_workers_running",
        "==",
        "num_workers_expected",
        ":",
        "refresh_interval",
        "=",
        "conf",
        ".",
        "getint",
        "(",
        "'webserver'",
        ",",
        "'worker_refresh_interval'",
        ")",
        "log",
        ".",
        "debug",
        "(",
        "'%s sleeping for %ss starting doing a refresh...'",
        ",",
        "state",
        ",",
        "refresh_interval",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "refresh_interval",
        ")",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        "else",
        ":",
        "# num_ready_workers_running == num_workers_running < num_workers_expected",
        "log",
        ".",
        "error",
        "(",
        "(",
        "\"%s some workers seem to have died and gunicorn\"",
        "\"did not restart them as expected\"",
        ")",
        ",",
        "state",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "10",
        ")",
        "if",
        "len",
        "(",
        "psutil",
        ".",
        "Process",
        "(",
        "gunicorn_master_proc",
        ".",
        "pid",
        ")",
        ".",
        "children",
        "(",
        ")",
        ")",
        "<",
        "num_workers_expected",
        ":",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        "except",
        "(",
        "AirflowWebServerTimeout",
        ",",
        "OSError",
        ")",
        "as",
        "err",
        ":",
        "log",
        ".",
        "error",
        "(",
        "err",
        ")",
        "log",
        ".",
        "error",
        "(",
        "\"Shutting down webserver\"",
        ")",
        "try",
        ":",
        "gunicorn_master_proc",
        ".",
        "terminate",
        "(",
        ")",
        "gunicorn_master_proc",
        ".",
        "wait",
        "(",
        ")",
        "finally",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "1",
        ")"
      ],
      "docstring": "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.",
      "docstring_tokens": [
        "Runs",
        "forever",
        "monitoring",
        "the",
        "child",
        "processes",
        "of"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L763-L868"
    },
    "target": "Runs forever monitoring the child processes of",
    "arguments": [
      [
        "def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : \"\"\" Sleeps until fn is true \"\"\" t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . SIGTTIN ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while True : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) # Whenever some workers are not ready, wait until all workers are ready if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) # Kill a worker gracefully by asking gunicorn to reduce number of workers elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . SIGTTOU ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) # Start a new worker by asking gunicorn to increase number of workers elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : # num_ready_workers_running == num_workers_running < num_workers_expected log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) time . sleep ( 10 ) if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( AirflowWebServerTimeout , OSError ) as err : log . error ( err ) log . error ( \"Shutting down webserver\" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def launch_workers : if state =='starting' : log. info ( \"'{0}' setting up workers now...\". format ( state ) ) # wait_for_max_retries_for_heartbeat to be in green before starting workers # this may be useful to be able to wait in background and not # consuming to much cpu and/or memory. if \"webserver_heartbeat\" in conf. get ( 'core','metrics_heartbeat_interval' ) : wait_for_max_retries_for_heartbeat ( ) # start gunicorn master process gunicorn_master_proc = psutil. Process ( psutil. Process. current ( ). children ( ) [ 0 ]. pid ) num_workers_expected = conf. getint ( \"webserver\", \"num workers\" ) master_timeout = conf. getint ( \"webserver\", \"master_timeout\" ) # start workers for _ in range ( num_workers_expected ) : gunicorn_master_proc. send_signal ( signal. SIGTTOU ) refresh_interval = conf. getint ( 'webserver', 'worker_refresh_interval' ) # If the webserver is the master/leader let it refresh for gunicorn itself last_worker_pid = None # Try to get a fresh"
      ]
    ],
    "filtered_resps": [
      " def launch_workers : if state =='starting' : log. info ( \"'{0}' setting up workers now...\". format ( state ) ) # wait_for_max_retries_for_heartbeat to be in green before starting workers # this may be useful to be able to wait in background and not # consuming to much cpu and/or memory. if \"webserver_heartbeat\" in conf. get ( 'core','metrics_heartbeat_interval' ) : wait_for_max_retries_for_heartbeat ( ) # start gunicorn master process gunicorn_master_proc = psutil. Process ( psutil. Process. current ( ). children ( ) [ 0 ]. pid ) num_workers_expected = conf. getint ( \"webserver\", \"num workers\" ) master_timeout = conf. getint ( \"webserver\", \"master_timeout\" ) # start workers for _ in range ( num_workers_expected ) : gunicorn_master_proc. send_signal ( signal. SIGTTOU ) refresh_interval = conf. getint ( 'webserver', 'worker_refresh_interval' ) # If the webserver is the master/leader let it refresh for gunicorn itself last_worker_pid = None # Try to get a fresh"
    ],
    "smoothed_bleu_4": 0.3813174464934583
  },
  {
    "doc_id": 37,
    "doc": {
      "id": 37,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_translate_hook.py",
      "func_name": "CloudTranslateHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "_client",
        ":",
        "self",
        ".",
        "_client",
        "=",
        "Client",
        "(",
        "credentials",
        "=",
        "self",
        ".",
        "_get_credentials",
        "(",
        ")",
        ")",
        "return",
        "self",
        ".",
        "_client"
      ],
      "docstring": "Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client",
      "docstring_tokens": [
        "Retrieves",
        "connection",
        "to",
        "Cloud",
        "Translate"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L34-L43"
    },
    "target": "Retrieves connection to Cloud Translate",
    "arguments": [
      [
        "def get_conn ( self ) : if not self . _client : self . _client = Client ( credentials = self . _get_credentials ( ) ) return self . _client",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " with open ( os. path. join ( pathlib, 'google_auth_token.pkl' ), 'rb' ) as f : pickle. dump ( self. _client, f ) return self. _client def __del__ ( self ) : logging. debug ( 'closing google connection' ) with open ( os. path. join ( pathlib, 'google_auth_token.pkl' ), 'wb' ) as f : pickle. dump ( None, f ) return if google_service ==='search_drive' === 'SearchDrive' :\n2. Implement the SearchDrive class\nclass SearchDrive ( Base ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0 def __init__ ( self ) :\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. query = None \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. client = None \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. _get_credentials ( ) \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. _setup_local ( )\n\u00a0\u00a0\u00a0\u00a0 \u00a0 def _get_credentials ( self ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if os. path. exists ( os. path. join ( pathlib, 'google_service_account_token_drive.pkl' ) ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 with open ( os. path. join ( pathlib, 'google_service_account_token_drive.pkl' ), 'rb' ) as f : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 credentials = pickle. load ( f ) \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if not credentials."
      ]
    ],
    "filtered_resps": [
      " with open ( os. path. join ( pathlib, 'google_auth_token.pkl' ), 'rb' ) as f : pickle. dump ( self. _client, f ) return self. _client def __del__ ( self ) : logging. debug ( 'closing google connection' ) with open ( os. path. join ( pathlib, 'google_auth_token.pkl' ), 'wb' ) as f : pickle. dump ( None, f ) return if google_service ==='search_drive' === 'SearchDrive' :\n2. Implement the SearchDrive class\nclass SearchDrive ( Base ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0 def __init__ ( self ) :\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. query = None \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. client = None \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. _get_credentials ( ) \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self. _setup_local ( )\n\u00a0\u00a0\u00a0\u00a0 \u00a0 def _get_credentials ( self ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if os. path. exists ( os. path. join ( pathlib, 'google_service_account_token_drive.pkl' ) ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 with open ( os. path. join ( pathlib, 'google_service_account_token_drive.pkl' ), 'rb' ) as f : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 credentials = pickle. load ( f ) \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if not credentials."
    ],
    "smoothed_bleu_4": 0.3933157614265924
  },
  {
    "doc_id": 38,
    "doc": {
      "id": 38,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_translate_hook.py",
      "func_name": "CloudTranslateHook.translate",
      "original_string": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )",
      "language": "python",
      "code": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )",
      "code_tokens": [
        "def",
        "translate",
        "(",
        "self",
        ",",
        "values",
        ",",
        "target_language",
        ",",
        "format_",
        "=",
        "None",
        ",",
        "source_language",
        "=",
        "None",
        ",",
        "model",
        "=",
        "None",
        ")",
        ":",
        "client",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        "return",
        "client",
        ".",
        "translate",
        "(",
        "values",
        "=",
        "values",
        ",",
        "target_language",
        "=",
        "target_language",
        ",",
        "format_",
        "=",
        "format_",
        ",",
        "source_language",
        "=",
        "source_language",
        ",",
        "model",
        "=",
        "model",
        ",",
        ")"
      ],
      "docstring": "Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.",
      "docstring_tokens": [
        "Translate",
        "a",
        "string",
        "or",
        "list",
        "of",
        "strings",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L45-L97"
    },
    "target": "Translate a string or list of strings .",
    "arguments": [
      [
        "def translate ( self , values , target_language , format_ = None , source_language = None , model = None ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ = format_ , source_language = source_language , model = model , )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\n### Getters and Setters\nWe will also add a method called `get_conn()` which will return a connection to the client. \n\n``` \ndef get_conn ( self ) :\n    return TMSClient ( )\n```\n\nAlso, these are the methods that would be provided by the client. Some of them, like `create_project()` are used by our original code and will be overridden. Here's an example for the `create_project()`:\n\n``` \ndef _create_project ( self, values ) :\n    # Code for creating a project in the target system. \n    return { 'id' : 1 }\n```\n\n### Example usage:\nTo use this bridge class, you would do something like this:\n\n``` \nfrom target_bridge import TMS2Google\nbridge = TMS2Google ( )\nbridge. create_project ( create_project_data )\n```\n\n## Step 5: Provide client implementation for the target platform\nWe will implement the client side for the target platform, in this case Google Translate API.\n\n### Google Translate API client implementation\nThis implementation will have methods to translate text, create projects, and others. It will also handle authentication and API calls.\n\n``` \n# google_translate_client.py\nimport requests\n"
      ]
    ],
    "filtered_resps": [
      " ```\n\n### Getters and Setters\nWe will also add a method called `get_conn()` which will return a connection to the client. \n\n``` \ndef get_conn ( self ) :\n    return TMSClient ( )\n```\n\nAlso, these are the methods that would be provided by the client. Some of them, like `create_project()` are used by our original code and will be overridden. Here's an example for the `create_project()`:\n\n``` \ndef _create_project ( self, values ) :\n    # Code for creating a project in the target system. \n    return { 'id' : 1 }\n```\n\n### Example usage:\nTo use this bridge class, you would do something like this:\n\n``` \nfrom target_bridge import TMS2Google\nbridge = TMS2Google ( )\nbridge. create_project ( create_project_data )\n```\n\n## Step 5: Provide client implementation for the target platform\nWe will implement the client side for the target platform, in this case Google Translate API.\n\n### Google Translate API client implementation\nThis implementation will have methods to translate text, create projects, and others. It will also handle authentication and API calls.\n\n``` \n# google_translate_client.py\nimport requests\n"
    ],
    "smoothed_bleu_4": 0.5434085485196946
  },
  {
    "doc_id": 39,
    "doc": {
      "id": 39,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.get_instance",
      "original_string": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "get",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict",
      "docstring_tokens": [
        "Retrieves",
        "a",
        "resource",
        "containing",
        "information",
        "about",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L97-L112"
    },
    "target": "Retrieves a resource containing information about a Cloud SQL instance .",
    "arguments": [
      [
        "def get_instance ( self , instance , project_id = None ) : return self . get_conn ( ) . instances ( ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` Note: I added the \u201c.execute(num_retries=self.num_retries)\u201d which seems to be needed (can be avoided by using get_without_caching(num_retries=num_retries)). ``` Python ``` \"\"\"A connection to a Rackspace OpenStack Object Storage. \"\"\" from   rackspace   import   auth from   rackspace   import   objectstorage from   authkit. provider. openstack. rackspace. objectstorage   import   get_instance def   get_conn ( self ) :      \"\"\"Return the most recent connection object to the object storage.     \"\"\"      return   self. openstack. get ( objectstorage. V1ObjectStorage ) def   get_instance ( self, instance, project_id = None ) :      \"\"\"Return the most recent objectstorage instance object.     \"\"\"      return   self. get_conn ( ). instances ( ). get ( project = project_id, instance = instance ) ``` To get a connection to the object storage API (and return the connection object), use ``` get_conn() ``` Note: Your auth instance must implement the openstack API. For example: ``` auth   =   RackspaceAuth ( endpoint_url = 'https://identity.api.rackspacecloud.com/v2"
      ]
    ],
    "filtered_resps": [
      " ``` Note: I added the \u201c.execute(num_retries=self.num_retries)\u201d which seems to be needed (can be avoided by using get_without_caching(num_retries=num_retries)). ``` Python ``` \"\"\"A connection to a Rackspace OpenStack Object Storage. \"\"\" from   rackspace   import   auth from   rackspace   import   objectstorage from   authkit. provider. openstack. rackspace. objectstorage   import   get_instance def   get_conn ( self ) :      \"\"\"Return the most recent connection object to the object storage.     \"\"\"      return   self. openstack. get ( objectstorage. V1ObjectStorage ) def   get_instance ( self, instance, project_id = None ) :      \"\"\"Return the most recent objectstorage instance object.     \"\"\"      return   self. get_conn ( ). instances ( ). get ( project = project_id, instance = instance ) ``` To get a connection to the object storage API (and return the connection object), use ``` get_conn() ``` Note: Your auth instance must implement the openstack API. For example: ``` auth   =   RackspaceAuth ( endpoint_url = 'https://identity.api.rackspacecloud.com/v2"
    ],
    "smoothed_bleu_4": 0.5540532936064686
  },
  {
    "doc_id": 40,
    "doc": {
      "id": 40,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.create_instance",
      "original_string": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_instance",
        "(",
        "self",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "insert",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L115-L133"
    },
    "target": "Creates a new Cloud SQL instance .",
    "arguments": [
      [
        "def create_instance ( self , body , project_id = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response ```\nThe create_instance() method creates a new instance. The instance\u2019s Project ID is set by the optional project_id parameter. It first establishes a connection using the get_conn() method and then makes a POST request to the instances collection of the Compute Engine API with the body parameter to insert the new instance. This POST request is a PATCH request. It executes the request with a number of retries set by the num-retries parameter, which is an instance variable.\nDepending on the state of the request, it determines whether to create, update, or delete an instance.\n### Creating Multiple Instances\nEven though it is possible to create multiple instances at once, a simple implementation of the create_instances() method allows creation of a maximum of 10 instances at the time. You can use this limit to create batches of instances:\n```\ndef create_instances ( self, body, project_id = None ) :\nbatch = 10\nbody_list = [ ]\nwhile body_list. append ( body ) < batch :\npass\nresponse = self. get_conn ( ). instances ( ). insert_collection (\nproject = project_id, bodies = body_list,\n). execute ( ). result ( )\n```\nIn this snippet, the create_instances() method"
      ]
    ],
    "filtered_resps": [
      " return response ```\nThe create_instance() method creates a new instance. The instance\u2019s Project ID is set by the optional project_id parameter. It first establishes a connection using the get_conn() method and then makes a POST request to the instances collection of the Compute Engine API with the body parameter to insert the new instance. This POST request is a PATCH request. It executes the request with a number of retries set by the num-retries parameter, which is an instance variable.\nDepending on the state of the request, it determines whether to create, update, or delete an instance.\n### Creating Multiple Instances\nEven though it is possible to create multiple instances at once, a simple implementation of the create_instances() method allows creation of a maximum of 10 instances at the time. You can use this limit to create batches of instances:\n```\ndef create_instances ( self, body, project_id = None ) :\nbatch = 10\nbody_list = [ ]\nwhile body_list. append ( body ) < batch :\npass\nresponse = self. get_conn ( ). instances ( ). insert_collection (\nproject = project_id, bodies = body_list,\n). execute ( ). result ( )\n```\nIn this snippet, the create_instances() method"
    ],
    "smoothed_bleu_4": 0.9375130980216071
  },
  {
    "doc_id": 41,
    "doc": {
      "id": 41,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.patch_instance",
      "original_string": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "patch_instance",
        "(",
        "self",
        ",",
        "body",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "settings",
        "of",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L136-L160"
    },
    "target": "Updates settings of a Cloud SQL instance .",
    "arguments": [
      [
        "def patch_instance ( self , body , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\nIn the code snippet above, the `patch_instance` method is a part of a class that is responsible for interacting with Google Compute Engine (GCE) instances. This method allows you to modify an instance by sending a PATCH request to the GCE API.\n\nHere's a breakdown of what the code does:\n\n*   It sends a GET request to fetch the instance details.\n*   It sends a PATCH request to modify the instance with the provided body. The request is made to the Compute Engine API.\n*   It retrieves the name of the operation generated by the API.\n*   It waits for the operation to complete.\n\nThe method does not include any error handling beyond the initial GET request. If the PATCH request fails, an exception is not caught, allowing the error to propagate up the call stack. This might be a problem if you're planning to use this method in a production environment.\n\nThis function takes in the following arguments:\n\n*   `project_id`: The ID of the project to which the instance belongs. This is set to `None` in the method definition.\n*   `instance`: The instance in question.\n*   `body`: The body to be patched in the instance.\n\n### Code Quality Issues\n\nThe code"
      ]
    ],
    "filtered_resps": [
      " return response\nIn the code snippet above, the `patch_instance` method is a part of a class that is responsible for interacting with Google Compute Engine (GCE) instances. This method allows you to modify an instance by sending a PATCH request to the GCE API.\n\nHere's a breakdown of what the code does:\n\n*   It sends a GET request to fetch the instance details.\n*   It sends a PATCH request to modify the instance with the provided body. The request is made to the Compute Engine API.\n*   It retrieves the name of the operation generated by the API.\n*   It waits for the operation to complete.\n\nThe method does not include any error handling beyond the initial GET request. If the PATCH request fails, an exception is not caught, allowing the error to propagate up the call stack. This might be a problem if you're planning to use this method in a production environment.\n\nThis function takes in the following arguments:\n\n*   `project_id`: The ID of the project to which the instance belongs. This is set to `None` in the method definition.\n*   `instance`: The instance in question.\n*   `body`: The body to be patched in the instance.\n\n### Code Quality Issues\n\nThe code"
    ],
    "smoothed_bleu_4": 0.7527685320545144
  },
  {
    "doc_id": 42,
    "doc": {
      "id": 42,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.delete_instance",
      "original_string": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "delete_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "delete",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None",
      "docstring_tokens": [
        "Deletes",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L163-L180"
    },
    "target": "Deletes a Cloud SQL instance .",
    "arguments": [
      [
        "def delete_instance ( self , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n\ndef restart_instance ( self, instance_name, zone, project_id = None ) : instance = self. get_instance ( instance_name, zone = zone ) if not instance : raise ValueError ( \"Missing instance: \" + instance_name ) if not instance. status ( ). name == \"RUNNING\" : print ( \"Instance already running. Exit...\" ) return None operation_name = self. get_conn ( ). instances ( ). restart ( project = project_id, instance = instance, ). execute ( num_retries = self. num_retries ) print ( \"Operation ID: \" + operation_name ) self. _wait_for_operation_to_complete ( project_id = project_id, operation_name = operation_name ) print ( \"Instance Restarted: \" + instance_name ) return instance\n\ndef get_cpu_usage ( self, instance_name, zone, project_id = None, period = \"30d\", metric_type =\"CUMULATIVE\" ) : metrics = self. get_conn ( ). aggregate_compute_metrics ( project = project_id, scope = \"REGIONAL\", meters = [ \"cpu-total\" ], options = { \"aggregation_interval\" : \"3600s\", \"dimension\" :"
      ]
    ],
    "filtered_resps": [
      " return response\n\ndef restart_instance ( self, instance_name, zone, project_id = None ) : instance = self. get_instance ( instance_name, zone = zone ) if not instance : raise ValueError ( \"Missing instance: \" + instance_name ) if not instance. status ( ). name == \"RUNNING\" : print ( \"Instance already running. Exit...\" ) return None operation_name = self. get_conn ( ). instances ( ). restart ( project = project_id, instance = instance, ). execute ( num_retries = self. num_retries ) print ( \"Operation ID: \" + operation_name ) self. _wait_for_operation_to_complete ( project_id = project_id, operation_name = operation_name ) print ( \"Instance Restarted: \" + instance_name ) return instance\n\ndef get_cpu_usage ( self, instance_name, zone, project_id = None, period = \"30d\", metric_type =\"CUMULATIVE\" ) : metrics = self. get_conn ( ). aggregate_compute_metrics ( project = project_id, scope = \"REGIONAL\", meters = [ \"cpu-total\" ], options = { \"aggregation_interval\" : \"3600s\", \"dimension\" :"
    ],
    "smoothed_bleu_4": 0.5194564289333655
  },
  {
    "doc_id": 43,
    "doc": {
      "id": 43,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.get_database",
      "original_string": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "get",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict",
      "docstring_tokens": [
        "Retrieves",
        "a",
        "database",
        "resource",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L183-L202"
    },
    "target": "Retrieves a database resource from a Cloud SQL instance .",
    "arguments": [
      [
        "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n    - ``` ```\n    - Retrieve the connection to use for the given combination of instance name, database and optionally project ID.\n    - :param instance: instance of the database\n    - :param database: database name\n    - :param project_id: optional project ID\n    - :return: a connection or None if not found\n    - :rtype: ``Connection`` or ``None``\n    - :raises: ``UnsupportedDatabaseError``, ``DatabaseNotFound``\n\n   .. note:: This method first attempts to retrieve a connection for the specified database by using the project ID. If none is found, it fallbacks to using the instance name.\n   .. py:method:: get_database( instance, database, project_id=None )\n   .. py:parameter:: instance\n   .. py:parameter:: database\n   .. py:parameter:: project_id\n   .. py:return::\n   .. py:rtype:: Connection\n   .. py:raises:: UnsupportedDatabaseError\n   .. py:raises:: DatabaseNotFound ```\n\n    ```python\n    def get_database ( self, instance, database, project_id = None ) :\n        try :\n            return self. get_conn ( ). databases ( ). get ( project = project"
      ]
    ],
    "filtered_resps": [
      " ```\n    - ``` ```\n    - Retrieve the connection to use for the given combination of instance name, database and optionally project ID.\n    - :param instance: instance of the database\n    - :param database: database name\n    - :param project_id: optional project ID\n    - :return: a connection or None if not found\n    - :rtype: ``Connection`` or ``None``\n    - :raises: ``UnsupportedDatabaseError``, ``DatabaseNotFound``\n\n   .. note:: This method first attempts to retrieve a connection for the specified database by using the project ID. If none is found, it fallbacks to using the instance name.\n   .. py:method:: get_database( instance, database, project_id=None )\n   .. py:parameter:: instance\n   .. py:parameter:: database\n   .. py:parameter:: project_id\n   .. py:return::\n   .. py:rtype:: Connection\n   .. py:raises:: UnsupportedDatabaseError\n   .. py:raises:: DatabaseNotFound ```\n\n    ```python\n    def get_database ( self, instance, database, project_id = None ) :\n        try :\n            return self. get_conn ( ). databases ( ). get ( project = project"
    ],
    "smoothed_bleu_4": 0.6966851332067083
  },
  {
    "doc_id": 44,
    "doc": {
      "id": 44,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.create_database",
      "original_string": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "insert",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "database",
        "inside",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L205-L226"
    },
    "target": "Creates a new database inside a Cloud SQL instance .",
    "arguments": [
      [
        "def create_database ( self , instance , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response [\n### Step 5: Refactor the existing code\n\nTo make the existing code more modular and reusable for handling different database operations, we can create a helper class named `Operation` which encapsulates the logic for checking the status of a database operation.\n\n```python\nclass MySQL ( Object ) :\n# existing methods go here\n\nclass Operation ( object ) :\ndef __init__ ( self, client, operation_name, project_id ) :\nself. client = client\nself. operation_name = operation_name\nself. project_id = project_id\n\ndef wait_for_operation_to_complete ( self ) :\nwaiters = {\n'waiter_name' : self. _wait_for_operation,\n}\ntry :\nwaiters [ 'waiter_name' ] ( ). execute ( )\nexcept Client. exceptions. OperationTimeoutError :\nraise Exception ( \"operation \" + self. operation_name + \" timed out\" )\nexcept Client. exceptions. OperationNotReady :\nraise Exception ( \"operation \" + self. operation_name + \" is not ready yet\" )\nexcept Exception as e :\nraise Exception (\n\"error in waiter for operation \" + self. operation_name + \": \" + str ( e )\n)\n\ndef _wait_for_operation ( self ) :\nresult"
      ]
    ],
    "filtered_resps": [
      " return response [\n### Step 5: Refactor the existing code\n\nTo make the existing code more modular and reusable for handling different database operations, we can create a helper class named `Operation` which encapsulates the logic for checking the status of a database operation.\n\n```python\nclass MySQL ( Object ) :\n# existing methods go here\n\nclass Operation ( object ) :\ndef __init__ ( self, client, operation_name, project_id ) :\nself. client = client\nself. operation_name = operation_name\nself. project_id = project_id\n\ndef wait_for_operation_to_complete ( self ) :\nwaiters = {\n'waiter_name' : self. _wait_for_operation,\n}\ntry :\nwaiters [ 'waiter_name' ] ( ). execute ( )\nexcept Client. exceptions. OperationTimeoutError :\nraise Exception ( \"operation \" + self. operation_name + \" timed out\" )\nexcept Client. exceptions. OperationNotReady :\nraise Exception ( \"operation \" + self. operation_name + \" is not ready yet\" )\nexcept Exception as e :\nraise Exception (\n\"error in waiter for operation \" + self. operation_name + \": \" + str ( e )\n)\n\ndef _wait_for_operation ( self ) :\nresult"
    ],
    "smoothed_bleu_4": 0.547616983273439
  },
  {
    "doc_id": 45,
    "doc": {
      "id": 45,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.patch_database",
      "original_string": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "patch_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "a",
        "database",
        "resource",
        "inside",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L229-L256"
    },
    "target": "Updates a database resource inside a Cloud SQL instance .",
    "arguments": [
      [
        "def patch_database ( self , instance , database , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. refresh_instance ( instance, cache_refresh = True ) ``` ``` def patch_database ( self, instance, database, body, project_id = None ) : ''' Patch an instance's database with a network policy.\u00a0 \u00a0 \u00a0 \u00a0 :param instance: The instance of the API resource.\u00a0 \u00a0 \u00a0 \u00a0 :param database: The database of the API resource.\u00a0 \u00a0 \u00a0 \u00a0 :type database: Database\u00a0 \u00a0 \u00a0 \u00a0 :param body: The request body.\u00a0 \u00a0 \u00a0 \u00a0 :type body: dict\u00a0 \u00a0 \u00a0 \u00a0 :param project_id: The project ID.\u00a0 \u00a0 \u00a0 \u00a0 :type project_id: int\u00a0 \u00a0 \u00a0 \u00a0 :returns: The database object.\u00a0 \u00a0 \u00a0 \u00a0 :rtype: Database\u00a0 \u00a0 \u00a0 \u00a0 ''' response = self. get_conn ( ). databases ( ). patch ( project = project_id, instance = instance, database = database, body = body ). execute ( num_retries = self. num_retries )         operation_name = response [ \"name\" ]         self. _wait_for_operation_to_complete ( operation_name = operation_name )         self. refresh_instance ( instance, cache_refresh = True )         return database ```\n\nExample use cases:\n\n* Patch an instance's database with a network policy: `patch_database(instance='api', database='my_database', body={'param1"
      ]
    ],
    "filtered_resps": [
      " self. refresh_instance ( instance, cache_refresh = True ) ``` ``` def patch_database ( self, instance, database, body, project_id = None ) : ''' Patch an instance's database with a network policy.\u00a0 \u00a0 \u00a0 \u00a0 :param instance: The instance of the API resource.\u00a0 \u00a0 \u00a0 \u00a0 :param database: The database of the API resource.\u00a0 \u00a0 \u00a0 \u00a0 :type database: Database\u00a0 \u00a0 \u00a0 \u00a0 :param body: The request body.\u00a0 \u00a0 \u00a0 \u00a0 :type body: dict\u00a0 \u00a0 \u00a0 \u00a0 :param project_id: The project ID.\u00a0 \u00a0 \u00a0 \u00a0 :type project_id: int\u00a0 \u00a0 \u00a0 \u00a0 :returns: The database object.\u00a0 \u00a0 \u00a0 \u00a0 :rtype: Database\u00a0 \u00a0 \u00a0 \u00a0 ''' response = self. get_conn ( ). databases ( ). patch ( project = project_id, instance = instance, database = database, body = body ). execute ( num_retries = self. num_retries )         operation_name = response [ \"name\" ]         self. _wait_for_operation_to_complete ( operation_name = operation_name )         self. refresh_instance ( instance, cache_refresh = True )         return database ```\n\nExample use cases:\n\n* Patch an instance's database with a network policy: `patch_database(instance='api', database='my_database', body={'param1"
    ],
    "smoothed_bleu_4": 0.5990784577831185
  },
  {
    "doc_id": 46,
    "doc": {
      "id": 46,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.delete_database",
      "original_string": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "delete_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "delete",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Deletes",
        "a",
        "database",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L259-L279"
    },
    "target": "Deletes a database from a Cloud SQL instance .",
    "arguments": [
      [
        "def delete_database ( self , instance , database , project_id = None ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return operation_name [ \u2026 ] ```\n    - def delete_instance ( self, project_id, instance_name ) :\n    - Deletes an instance. Args:\n      - project_id (str): ID of the project that contains the instance.\n      - instance_name (str): Name of the instance to delete.\n\nReturns: An object containing a response describing the delete request. See: https://cloud.google.com/compute/docs/reference/rest/v1/projects/instances/delete\n```\n     def   delete_instance ( self,   project_id,   instance_name ):\n         headers   =   { \"x-cloud-project- quirks\" :   \"v9\" }\n         body   =   {\n             \"requests\" :   [\n                 {\n                     \"instance\" :   instance_name,\n                     \"projectsId\" :   projectsId,\n                     \"zone\" :   instance. zone,\n                     \"requestId\" :   str ( time. time ()):\n                 }\n             ]\n         }\n\n         resp   =   self. post ( \"instances\",   json = body,   headers = headers )\n         if   resp. ok :\n             return   resp. json ()\n         else :\n             return   {}\n```\n\n    2. Get a GCE account using"
      ]
    ],
    "filtered_resps": [
      " return operation_name [ \u2026 ] ```\n    - def delete_instance ( self, project_id, instance_name ) :\n    - Deletes an instance. Args:\n      - project_id (str): ID of the project that contains the instance.\n      - instance_name (str): Name of the instance to delete.\n\nReturns: An object containing a response describing the delete request. See: https://cloud.google.com/compute/docs/reference/rest/v1/projects/instances/delete\n```\n     def   delete_instance ( self,   project_id,   instance_name ):\n         headers   =   { \"x-cloud-project- quirks\" :   \"v9\" }\n         body   =   {\n             \"requests\" :   [\n                 {\n                     \"instance\" :   instance_name,\n                     \"projectsId\" :   projectsId,\n                     \"zone\" :   instance. zone,\n                     \"requestId\" :   str ( time. time ()):\n                 }\n             ]\n         }\n\n         resp   =   self. post ( \"instances\",   json = body,   headers = headers )\n         if   resp. ok :\n             return   resp. json ()\n         else :\n             return   {}\n```\n\n    2. Get a GCE account using"
    ],
    "smoothed_bleu_4": 0.804852625481795
  },
  {
    "doc_id": 47,
    "doc": {
      "id": 47,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.export_instance",
      "original_string": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )",
      "language": "python",
      "code": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )",
      "code_tokens": [
        "def",
        "export_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "try",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "export",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")",
        "except",
        "HttpError",
        "as",
        "ex",
        ":",
        "raise",
        "AirflowException",
        "(",
        "'Exporting instance {} failed: {}'",
        ".",
        "format",
        "(",
        "instance",
        ",",
        "ex",
        ".",
        "content",
        ")",
        ")"
      ],
      "docstring": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Exports",
        "data",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "to",
        "a",
        "Cloud",
        "Storage",
        "bucket",
        "as",
        "a",
        "SQL",
        "dump",
        "or",
        "CSV",
        "file",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L282-L310"
    },
    "target": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file .",
    "arguments": [
      [
        "def export_instance ( self , instance , body , project_id = None ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name ) except HttpError as ex : raise AirflowException ( 'Exporting instance {} failed: {}' . format ( instance , ex . content ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [ documentation ]: [https://cloud.google.com/compute/docs/reference/latest/instances/](https://cloud.google.com/compute/docs/reference/latest/instances/) Get the details of the specified instance. Returns the instance's zone, name, number of CPUs, memory size, and status. This method is used by createInstance(), getInstances(), updateInstance and deleteInstance().\n def get_instance ( self, * args, ** kwargs ) : \"\"\" Exporting instance details... \"\"\" try : response = self. get_conn ( ). instances ( ). get ( * args, ** kwargs ). execute ( num_retries = self. num_retries ) return response except HttpError : raise AirflowException ( 'getting instance failed' ) [ documentation ]: [https://cloud.google.com/compute/docs/reference/latest/instances](https://cloud.google.com/compute/docs/reference/latest/instances/) Get the details of the specified instance. Returns the instance's zone, name, number of CPUs, memory size, and status.\n def get_instances ( self, project_id = None, * args, ** kwargs ) : \"\"\" Exporting instances... \"\"\" try : response = self. get_conn ( ). instances ( ). list ( project = project_id"
      ]
    ],
    "filtered_resps": [
      " [ documentation ]: [https://cloud.google.com/compute/docs/reference/latest/instances/](https://cloud.google.com/compute/docs/reference/latest/instances/) Get the details of the specified instance. Returns the instance's zone, name, number of CPUs, memory size, and status. This method is used by createInstance(), getInstances(), updateInstance and deleteInstance().\n def get_instance ( self, * args, ** kwargs ) : \"\"\" Exporting instance details... \"\"\" try : response = self. get_conn ( ). instances ( ). get ( * args, ** kwargs ). execute ( num_retries = self. num_retries ) return response except HttpError : raise AirflowException ( 'getting instance failed' ) [ documentation ]: [https://cloud.google.com/compute/docs/reference/latest/instances](https://cloud.google.com/compute/docs/reference/latest/instances/) Get the details of the specified instance. Returns the instance's zone, name, number of CPUs, memory size, and status.\n def get_instances ( self, project_id = None, * args, ** kwargs ) : \"\"\" Exporting instances... \"\"\" try : response = self. get_conn ( ). instances ( ). list ( project = project_id"
    ],
    "smoothed_bleu_4": 0.46945026524258326
  },
  {
    "doc_id": 48,
    "doc": {
      "id": 48,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.start_proxy",
      "original_string": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return",
      "language": "python",
      "code": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return",
      "code_tokens": [
        "def",
        "start_proxy",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "_download_sql_proxy_if_needed",
        "(",
        ")",
        "if",
        "self",
        ".",
        "sql_proxy_process",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"The sql proxy is already running: {}\"",
        ".",
        "format",
        "(",
        "self",
        ".",
        "sql_proxy_process",
        ")",
        ")",
        "else",
        ":",
        "command_to_run",
        "=",
        "[",
        "self",
        ".",
        "sql_proxy_path",
        "]",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "command_line_parameters",
        ")",
        "try",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Creating directory %s\"",
        ",",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "os",
        ".",
        "makedirs",
        "(",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "except",
        "OSError",
        ":",
        "# Needed for python 2 compatibility (exists_ok missing)",
        "pass",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "_get_credential_parameters",
        "(",
        ")",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Running the command: `%s`\"",
        ",",
        "\" \"",
        ".",
        "join",
        "(",
        "command_to_run",
        ")",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "Popen",
        "(",
        "command_to_run",
        ",",
        "stdin",
        "=",
        "PIPE",
        ",",
        "stdout",
        "=",
        "PIPE",
        ",",
        "stderr",
        "=",
        "PIPE",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"The pid of cloud_sql_proxy: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "pid",
        ")",
        "while",
        "True",
        ":",
        "line",
        "=",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "stderr",
        ".",
        "readline",
        "(",
        ")",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "return_code",
        "=",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "poll",
        "(",
        ")",
        "if",
        "line",
        "==",
        "''",
        "and",
        "return_code",
        "is",
        "not",
        "None",
        ":",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "None",
        "raise",
        "AirflowException",
        "(",
        "\"The cloud_sql_proxy finished early with return code {}!\"",
        ".",
        "format",
        "(",
        "return_code",
        ")",
        ")",
        "if",
        "line",
        "!=",
        "''",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "line",
        ")",
        "if",
        "\"googleapi: Error\"",
        "in",
        "line",
        "or",
        "\"invalid instance name:\"",
        "in",
        "line",
        ":",
        "self",
        ".",
        "stop_proxy",
        "(",
        ")",
        "raise",
        "AirflowException",
        "(",
        "\"Error when starting the cloud_sql_proxy {}!\"",
        ".",
        "format",
        "(",
        "line",
        ")",
        ")",
        "if",
        "\"Ready for new connections\"",
        "in",
        "line",
        ":",
        "return"
      ],
      "docstring": "Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!",
      "docstring_tokens": [
        "Starts",
        "Cloud",
        "SQL",
        "Proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L524-L565"
    },
    "target": "Starts Cloud SQL Proxy .",
    "arguments": [
      [
        "def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : # Needed for python 2 compatibility (exists_ok missing) pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self . log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) if line != '' : self . log . info ( line ) if \"googleapi: Error\" in line or \"invalid instance name:\" in line : self . stop_proxy ( ) raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) if \"Ready for new connections\" in line : return",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` This code is complex because it follows a long series of checks and operations to ensure the cloud sql proxy service is properly started.\n\nHowever, most of the check methods and all the process management methods seem to follow a simple workflow:\n\n1. Create proxy command.\n2. Ensure the proxy is not already running.\n3. Start the proxy.\n\nThis leaves out the creation and execution of the command and the logic to connect to the Cloud SQL instance.\n\nHere is an improved version of the code, following best practices for comment, docstrings, and following the 80-character line limit.\n```\ndef start_proxy(self):\n    \"\"\"\n    Starts the Cloud SQL Proxy process.\n    \n    :returns: None\n    :raises: AirflowException\n    \"\"\"\n    self._download_sql_proxy_if_needed()\n    \n    # Ensure proxy process is not running\n    if self.sql_proxy_process:\n        raise AirflowException(\n            \"The sql proxy is already running: {}\"\n           .format(self.sql_proxy_process)\n        )\n        \n    # Create proxy command\n    command_to_run = [self.sql_proxy_path]\n    command_to_run.extend(self.command_line_parameters)\n    \n    # Execute proxy command\n    try:\n        log.info(\"Creating directory %s\","
      ]
    ],
    "filtered_resps": [
      " ``` This code is complex because it follows a long series of checks and operations to ensure the cloud sql proxy service is properly started.\n\nHowever, most of the check methods and all the process management methods seem to follow a simple workflow:\n\n1. Create proxy command.\n2. Ensure the proxy is not already running.\n3. Start the proxy.\n\nThis leaves out the creation and execution of the command and the logic to connect to the Cloud SQL instance.\n\nHere is an improved version of the code, following best practices for comment, docstrings, and following the 80-character line limit.\n```\ndef start_proxy(self):\n    \"\"\"\n    Starts the Cloud SQL Proxy process.\n    \n    :returns: None\n    :raises: AirflowException\n    \"\"\"\n    self._download_sql_proxy_if_needed()\n    \n    # Ensure proxy process is not running\n    if self.sql_proxy_process:\n        raise AirflowException(\n            \"The sql proxy is already running: {}\"\n           .format(self.sql_proxy_process)\n        )\n        \n    # Create proxy command\n    command_to_run = [self.sql_proxy_path]\n    command_to_run.extend(self.command_line_parameters)\n    \n    # Execute proxy command\n    try:\n        log.info(\"Creating directory %s\","
    ],
    "smoothed_bleu_4": 0.9814166005074654
  },
  {
    "doc_id": 49,
    "doc": {
      "id": 49,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.stop_proxy",
      "original_string": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)",
      "language": "python",
      "code": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)",
      "code_tokens": [
        "def",
        "stop_proxy",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "sql_proxy_process",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"The sql proxy is not started yet\"",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Stopping the cloud_sql_proxy pid: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "pid",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "kill",
        "(",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "None",
        "# Cleanup!",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing the socket directory: %s\"",
        ",",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "shutil",
        ".",
        "rmtree",
        "(",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ",",
        "ignore_errors",
        "=",
        "True",
        ")",
        "if",
        "self",
        ".",
        "sql_proxy_was_downloaded",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing downloaded proxy: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "# Silently ignore if the file has already been removed (concurrency)",
        "try",
        ":",
        "os",
        ".",
        "remove",
        "(",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "except",
        "OSError",
        "as",
        "e",
        ":",
        "if",
        "not",
        "e",
        ".",
        "errno",
        "==",
        "errno",
        ".",
        "ENOENT",
        ":",
        "raise",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Skipped removing proxy - it was not downloaded: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "if",
        "os",
        ".",
        "path",
        ".",
        "isfile",
        "(",
        "self",
        ".",
        "credentials_path",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing generated credentials file %s\"",
        ",",
        "self",
        ".",
        "credentials_path",
        ")",
        "# Here file cannot be delete by concurrent task (each task has its own copy)",
        "os",
        ".",
        "remove",
        "(",
        "self",
        ".",
        "credentials_path",
        ")"
      ],
      "docstring": "Stops running proxy.\n\n        You should stop the proxy after you stop using it.",
      "docstring_tokens": [
        "Stops",
        "running",
        "proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L567-L599"
    },
    "target": "Stops running proxy .",
    "arguments": [
      [
        "def stop_proxy ( self ) : if not self . sql_proxy_process : raise AirflowException ( \"The sql proxy is not started yet\" ) else : self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = None # Cleanup! self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) if self . sql_proxy_was_downloaded : self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) # Silently ignore if the file has already been removed (concurrency) try : os . remove ( self . sql_proxy_path ) except OSError as e : if not e . errno == errno . ENOENT : raise else : self . log . info ( \"Skipped removing proxy - it was not downloaded: %s\" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) # Here file cannot be delete by concurrent task (each task has its own copy) os . remove ( self . credentials_path )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " + +#-- EOF ```\n\nThis code is a part of a larger module that implements a plugin for the Airflow scheduler, specifically for managing a Cloud SQL proxy process. The modified section implements a stop functionality for the proxy, handling the process kill, socket directory removal, and file cleanup. \n\nHowever, the code quality could be improved with more descriptive variable names and method encapsulation. Here is an example of how the refactored code could look```python\ndef _stop_proxy(self):\n    \"\"\"\n    Closes and deletes the Cloud SQL proxy process and related files.\n    \"\"\"\n    if not self._proxy_running:\n        raise AirflowException(\"The sql proxy is not started yet\")\n    \n    self.log.info(\"Stopping the Cloud SQL proxy process\")\n    self._proxy_process.kill()\n    self._proxy_process = None\n    \n    self.log.info(\"Removing the socket directory %s\", self.socket_directory)\n    shutil.rmtree(self.socket_directory, ignore_errors=True)\n    \n    if self._proxy_was_downloaded:\n        self.log.info(\"Removing downloaded proxy %s\", self.proxy_path)\n        os.remove(self.proxy_path)  # ignore failure if file is already gone\n    self.log.info(\"Removing generated credentials file %s\", self.credentials"
      ]
    ],
    "filtered_resps": [
      " + +#-- EOF ```\n\nThis code is a part of a larger module that implements a plugin for the Airflow scheduler, specifically for managing a Cloud SQL proxy process. The modified section implements a stop functionality for the proxy, handling the process kill, socket directory removal, and file cleanup. \n\nHowever, the code quality could be improved with more descriptive variable names and method encapsulation. Here is an example of how the refactored code could look```python\ndef _stop_proxy(self):\n    \"\"\"\n    Closes and deletes the Cloud SQL proxy process and related files.\n    \"\"\"\n    if not self._proxy_running:\n        raise AirflowException(\"The sql proxy is not started yet\")\n    \n    self.log.info(\"Stopping the Cloud SQL proxy process\")\n    self._proxy_process.kill()\n    self._proxy_process = None\n    \n    self.log.info(\"Removing the socket directory %s\", self.socket_directory)\n    shutil.rmtree(self.socket_directory, ignore_errors=True)\n    \n    if self._proxy_was_downloaded:\n        self.log.info(\"Removing downloaded proxy %s\", self.proxy_path)\n        os.remove(self.proxy_path)  # ignore failure if file is already gone\n    self.log.info(\"Removing generated credentials file %s\", self.credentials"
    ],
    "smoothed_bleu_4": 0.47640903058893647
  },
  {
    "doc_id": 50,
    "doc": {
      "id": 50,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.get_proxy_version",
      "original_string": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
      "language": "python",
      "code": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
      "code_tokens": [
        "def",
        "get_proxy_version",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "_download_sql_proxy_if_needed",
        "(",
        ")",
        "command_to_run",
        "=",
        "[",
        "self",
        ".",
        "sql_proxy_path",
        "]",
        "command_to_run",
        ".",
        "extend",
        "(",
        "[",
        "'--version'",
        "]",
        ")",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "_get_credential_parameters",
        "(",
        ")",
        ")",
        "result",
        "=",
        "subprocess",
        ".",
        "check_output",
        "(",
        "command_to_run",
        ")",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "pattern",
        "=",
        "re",
        ".",
        "compile",
        "(",
        "\"^.*[V|v]ersion ([^;]*);.*$\"",
        ")",
        "m",
        "=",
        "pattern",
        ".",
        "match",
        "(",
        "result",
        ")",
        "if",
        "m",
        ":",
        "return",
        "m",
        ".",
        "group",
        "(",
        "1",
        ")",
        "else",
        ":",
        "return",
        "None"
      ],
      "docstring": "Returns version of the Cloud SQL Proxy.",
      "docstring_tokens": [
        "Returns",
        "version",
        "of",
        "the",
        "Cloud",
        "SQL",
        "Proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L601-L615"
    },
    "target": "Returns version of the Cloud SQL Proxy .",
    "arguments": [
      [
        "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n    - _get_credential_parameters ( ) \u00b6\n    - Get DBFS Credential parameter list with Azure Key Vault Credentials. Parameters can be in the following format, either using storage or dbfs: For dbfs:: credential: { \u201caccountKey\u201d : \u201c\u201d } For storage:: credential: { \u201cfs.azure.account.keyaccount-name.dfs.core.windows.net\u201d : \u201c\u201d } Returns: Parameters to run commands with\n\n    - _get_dfs_path ( self, **kwargs ) \u00b6\n    - Get full path of DFS\n\n    - _get_free_space_size ( self ) \u00b6\n    - Get the size of free space in dfs Returns: size (B)\n\n    - _get_mount_points ( self, ) :\n    - get all mount points dfs return a list of the mount points which were registered in DFS use for all cases if dfs was mounted to the user home directory\n\n    - _get_mount_root ( self, ) :\n    - Returns the location of the mount root\n\n    - _get_user_quota ( self, user ) :\n    - return user\u2019s dfs space quota. Returns: bytes\n\n    - _get_user_space ( self, user, quota=False ) :\n    - get user\u2019s total space"
      ]
    ],
    "filtered_resps": [
      "\n    - _get_credential_parameters ( ) \u00b6\n    - Get DBFS Credential parameter list with Azure Key Vault Credentials. Parameters can be in the following format, either using storage or dbfs: For dbfs:: credential: { \u201caccountKey\u201d : \u201c\u201d } For storage:: credential: { \u201cfs.azure.account.keyaccount-name.dfs.core.windows.net\u201d : \u201c\u201d } Returns: Parameters to run commands with\n\n    - _get_dfs_path ( self, **kwargs ) \u00b6\n    - Get full path of DFS\n\n    - _get_free_space_size ( self ) \u00b6\n    - Get the size of free space in dfs Returns: size (B)\n\n    - _get_mount_points ( self, ) :\n    - get all mount points dfs return a list of the mount points which were registered in DFS use for all cases if dfs was mounted to the user home directory\n\n    - _get_mount_root ( self, ) :\n    - Returns the location of the mount root\n\n    - _get_user_quota ( self, user ) :\n    - return user\u2019s dfs space quota. Returns: bytes\n\n    - _get_user_space ( self, user, quota=False ) :\n    - get user\u2019s total space"
    ],
    "smoothed_bleu_4": 0.6802031538189011
  },
  {
    "doc_id": 51,
    "doc": {
      "id": 51,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.create_connection",
      "original_string": "def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()",
      "language": "python",
      "code": "def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()",
      "code_tokens": [
        "def",
        "create_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "connection",
        "=",
        "Connection",
        "(",
        "conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ")",
        "uri",
        "=",
        "self",
        ".",
        "_generate_connection_uri",
        "(",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Creating connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connection",
        ".",
        "parse_from_uri",
        "(",
        "uri",
        ")",
        "session",
        ".",
        "add",
        "(",
        "connection",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Create",
        "connection",
        "in",
        "the",
        "Connection",
        "table",
        "according",
        "to",
        "whether",
        "it",
        "uses",
        "proxy",
        "TCP",
        "UNIX",
        "sockets",
        "SSL",
        ".",
        "Connection",
        "ID",
        "will",
        "be",
        "randomly",
        "generated",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L895-L908"
    },
    "target": "Create connection in the Connection table according to whether it uses proxy TCP UNIX sockets SSL . Connection ID will be randomly generated .",
    "arguments": [
      [
        "def create_connection ( self , session = None ) : connection = Connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( \"Creating connection %s\" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add ( connection ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return connection\n@@ -243,7 +255,7 def create_or_update ( self, ** kw ): \"\"\" Create or Update this Connection.\"\"\"\nif isinstance ( self. conn_params, dict ): self. conn_params = Connection ( conn_params = self. conn_params ) if self. conn_id : session = self. get_session () connection = session. query ( Connection ). filter_by ( conn_id = self. conn_id ). first ())\nconnection = session. query ( Connection ). filter_by ( conn_id = self. conn_id ). first ()\nelse : conn_params = self. _generate_conn_string ( ** kw )\n# create a new object from conn_params if it doesn't exist session. add ( Connection ( conn_id = self. conn_id, conn_params = conn_params ))\n\n#### 0 comments on commit aa83dad\n\nPlease sign in to comment.<|end_of_text|>"
      ]
    ],
    "filtered_resps": [
      " return connection\n@@ -243,7 +255,7 def create_or_update ( self, ** kw ): \"\"\" Create or Update this Connection.\"\"\"\nif isinstance ( self. conn_params, dict ): self. conn_params = Connection ( conn_params = self. conn_params ) if self. conn_id : session = self. get_session () connection = session. query ( Connection ). filter_by ( conn_id = self. conn_id ). first ())\nconnection = session. query ( Connection ). filter_by ( conn_id = self. conn_id ). first ()\nelse : conn_params = self. _generate_conn_string ( ** kw )\n# create a new object from conn_params if it doesn't exist session. add ( Connection ( conn_id = self. conn_id, conn_params = conn_params ))\n\n#### 0 comments on commit aa83dad\n\nPlease sign in to comment.<|end_of_text|>"
    ],
    "smoothed_bleu_4": 0.8037477573503347
  },
  {
    "doc_id": 52,
    "doc": {
      "id": 52,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.retrieve_connection",
      "original_string": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None",
      "language": "python",
      "code": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None",
      "code_tokens": [
        "def",
        "retrieve_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Retrieving connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connections",
        "=",
        "session",
        ".",
        "query",
        "(",
        "Connection",
        ")",
        ".",
        "filter",
        "(",
        "Connection",
        ".",
        "conn_id",
        "==",
        "self",
        ".",
        "db_conn_id",
        ")",
        "if",
        "connections",
        ".",
        "count",
        "(",
        ")",
        ":",
        "return",
        "connections",
        "[",
        "0",
        "]",
        "return",
        "None"
      ],
      "docstring": "Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Retrieves",
        "the",
        "dynamically",
        "created",
        "connection",
        "from",
        "the",
        "Connection",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923"
    },
    "target": "Retrieves the dynamically created connection from the Connection table .",
    "arguments": [
      [
        "def retrieve_connection ( self , session = None ) : self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : return connections [ 0 ] return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n+\n+ def close ( self ) :\n+ self. log. warn ( 'Please note you also need to run `app.close()`' )\n+\ndef close ( self ) :\nself. logger. info ( \"Closing\" )\nsuper (). close ()\nself. _database. close ()\nself. _cursor. close ()\nself. _log_streamers. close ()\ndef _close_streamer ( cls, instance ) :\nlogger = logging. getLogger ( cls. __name__ )\nlogger. info ( \"Closing\" )\ntry :\ninstance. close ()\nexcept Exception as e :\nlogger. error ( \"Error while trying to close streamer %s, %s\", instance, e )\ndef close ( self ) :\nself. log. warn ( 'Please note you also need to run `app.close()`' )\nself. _process. exit ( force = True )\nself. _close_streamer ( SQLStreamManager, self. _select_streamer )\nself. _close_streamer ( TableStreamManager, self. _table_streamers )\nself. _close_streamer ( TableStreamManager, self. _delete_streamers )\n@@ -123,7 +122,8 @@\nself. log. info ( self. _"
      ]
    ],
    "filtered_resps": [
      "\n+\n+ def close ( self ) :\n+ self. log. warn ( 'Please note you also need to run `app.close()`' )\n+\ndef close ( self ) :\nself. logger. info ( \"Closing\" )\nsuper (). close ()\nself. _database. close ()\nself. _cursor. close ()\nself. _log_streamers. close ()\ndef _close_streamer ( cls, instance ) :\nlogger = logging. getLogger ( cls. __name__ )\nlogger. info ( \"Closing\" )\ntry :\ninstance. close ()\nexcept Exception as e :\nlogger. error ( \"Error while trying to close streamer %s, %s\", instance, e )\ndef close ( self ) :\nself. log. warn ( 'Please note you also need to run `app.close()`' )\nself. _process. exit ( force = True )\nself. _close_streamer ( SQLStreamManager, self. _select_streamer )\nself. _close_streamer ( TableStreamManager, self. _table_streamers )\nself. _close_streamer ( TableStreamManager, self. _delete_streamers )\n@@ -123,7 +122,8 @@\nself. log. info ( self. _"
    ],
    "smoothed_bleu_4": 0.4622792220751632
  },
  {
    "doc_id": 53,
    "doc": {
      "id": 53,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.delete_connection",
      "original_string": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")",
      "language": "python",
      "code": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")",
      "code_tokens": [
        "def",
        "delete_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Deleting connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connections",
        "=",
        "session",
        ".",
        "query",
        "(",
        "Connection",
        ")",
        ".",
        "filter",
        "(",
        "Connection",
        ".",
        "conn_id",
        "==",
        "self",
        ".",
        "db_conn_id",
        ")",
        "if",
        "connections",
        ".",
        "count",
        "(",
        ")",
        ":",
        "connection",
        "=",
        "connections",
        "[",
        "0",
        "]",
        "session",
        ".",
        "delete",
        "(",
        "connection",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Connection was already deleted!\"",
        ")"
      ],
      "docstring": "Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Delete",
        "the",
        "dynamically",
        "created",
        "connection",
        "from",
        "the",
        "Connection",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L926-L941"
    },
    "target": "Delete the dynamically created connection from the Connection table .",
    "arguments": [
      [
        "def delete_connection ( self , session = None ) : self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( \"Connection was already deleted!\" )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # We don't need to return anything, this information will # become part of the task's state self. update_state ( state = \"SUCCESS\" ) return\nIn the delete method we first collect all the connections, check if there is one and then perform the delete operation.\n### 3.4\u2003\u2002Creating the Schemas\n\nOn a large operation like re-indexing an entire database we may have more than one task for a single connection (one for each collection that was used).\nIf we have more than one task, it can easily happen that the order of execution of the collections is not the same as the order of the tasks. For this reason we need a mechanism that ensures the execution order is always correct.\nFor this we use another concept called a \u201cschema\u201d. This is a description of how to execute your tasks in the right order.\nHere we have two schemas: \u201creindex_all\u201d and \u201creindex_some\u201d that point to the collections that we want to execute first.\nOnce the schemas has been created we will be able to fetch the correct order.\n```bash\nfrom celery import current_app\nfrom celery.signals import task_prerun\nfrom kombu import Queue, Exchange, Binding\nfrom datetime import datetime\n"
      ]
    ],
    "filtered_resps": [
      " # We don't need to return anything, this information will # become part of the task's state self. update_state ( state = \"SUCCESS\" ) return\nIn the delete method we first collect all the connections, check if there is one and then perform the delete operation.\n### 3.4\u2003\u2002Creating the Schemas\n\nOn a large operation like re-indexing an entire database we may have more than one task for a single connection (one for each collection that was used).\nIf we have more than one task, it can easily happen that the order of execution of the collections is not the same as the order of the tasks. For this reason we need a mechanism that ensures the execution order is always correct.\nFor this we use another concept called a \u201cschema\u201d. This is a description of how to execute your tasks in the right order.\nHere we have two schemas: \u201creindex_all\u201d and \u201creindex_some\u201d that point to the collections that we want to execute first.\nOnce the schemas has been created we will be able to fetch the correct order.\n```bash\nfrom celery import current_app\nfrom celery.signals import task_prerun\nfrom kombu import Queue, Exchange, Binding\nfrom datetime import datetime\n"
    ],
    "smoothed_bleu_4": 0.6578708668064159
  },
  {
    "doc_id": 54,
    "doc": {
      "id": 54,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.get_sqlproxy_runner",
      "original_string": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
      "language": "python",
      "code": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
      "code_tokens": [
        "def",
        "get_sqlproxy_runner",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "use_proxy",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"Proxy runner can only be retrieved in case of use_proxy = True\"",
        ")",
        "return",
        "CloudSqlProxyRunner",
        "(",
        "path_prefix",
        "=",
        "self",
        ".",
        "sql_proxy_unique_path",
        ",",
        "instance_specification",
        "=",
        "self",
        ".",
        "_get_sqlproxy_instance_specification",
        "(",
        ")",
        ",",
        "project_id",
        "=",
        "self",
        ".",
        "project_id",
        ",",
        "sql_proxy_version",
        "=",
        "self",
        ".",
        "sql_proxy_version",
        ",",
        "sql_proxy_binary_path",
        "=",
        "self",
        ".",
        "sql_proxy_binary_path",
        ")"
      ],
      "docstring": "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner",
      "docstring_tokens": [
        "Retrieve",
        "Cloud",
        "SQL",
        "Proxy",
        "runner",
        ".",
        "It",
        "is",
        "used",
        "to",
        "manage",
        "the",
        "proxy",
        "lifecycle",
        "per",
        "task",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L943-L959"
    },
    "target": "Retrieve Cloud SQL Proxy runner . It is used to manage the proxy lifecycle per task .",
    "arguments": [
      [
        "def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( \"Proxy runner can only be retrieved in case of use_proxy = True\" ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " else : assert self. use_proxy, \"use_proxy should be True\" self. _sql_proxy_runner = CloudSqlProxyRunner (\n```\n- (None=None) if'self' not in locals() else self.get_sqlproxy_runner())\n```\n\nThe code is a method of a class, and it seems to be related to GCP (Google Cloud Platform) service. The code is written in Python and uses the `self` keyword to refer to the instance of the class. The method is called `get_sqlproxy_runner` and it returns a `CloudSqlProxyRunner` object. The object is created with several parameters, including `path_prefix`, `instance_specification`, `project_id`, `sql_proxy_version`, and `sql_proxy_binary_path`. \n\nThe code raises an exception if `use_proxy` is `False`. \n\nHere is the code with high-quality documentation added:\n```\nclass GcpConnectorConfig:\n   ...\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Get the SQL proxy runner.\n\n        Returns:\n            CloudSqlProxyRunner: The SQL proxy runner object.\n\n        Raises:\n            AirflowException: If use_proxy is False.\n        \"\"\"\n\n        if not self.use_proxy:\n            raise AirflowException"
      ]
    ],
    "filtered_resps": [
      " else : assert self. use_proxy, \"use_proxy should be True\" self. _sql_proxy_runner = CloudSqlProxyRunner (\n```\n- (None=None) if'self' not in locals() else self.get_sqlproxy_runner())\n```\n\nThe code is a method of a class, and it seems to be related to GCP (Google Cloud Platform) service. The code is written in Python and uses the `self` keyword to refer to the instance of the class. The method is called `get_sqlproxy_runner` and it returns a `CloudSqlProxyRunner` object. The object is created with several parameters, including `path_prefix`, `instance_specification`, `project_id`, `sql_proxy_version`, and `sql_proxy_binary_path`. \n\nThe code raises an exception if `use_proxy` is `False`. \n\nHere is the code with high-quality documentation added:\n```\nclass GcpConnectorConfig:\n   ...\n\n    def get_sqlproxy_runner(self):\n        \"\"\"\n        Get the SQL proxy runner.\n\n        Returns:\n            CloudSqlProxyRunner: The SQL proxy runner object.\n\n        Raises:\n            AirflowException: If use_proxy is False.\n        \"\"\"\n\n        if not self.use_proxy:\n            raise AirflowException"
    ],
    "smoothed_bleu_4": 1.5488610198864232
  },
  {
    "doc_id": 55,
    "doc": {
      "id": 55,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.get_database_hook",
      "original_string": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook",
      "language": "python",
      "code": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook",
      "code_tokens": [
        "def",
        "get_database_hook",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "database_type",
        "==",
        "'postgres'",
        ":",
        "self",
        ".",
        "db_hook",
        "=",
        "PostgresHook",
        "(",
        "postgres_conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ",",
        "schema",
        "=",
        "self",
        ".",
        "database",
        ")",
        "else",
        ":",
        "self",
        ".",
        "db_hook",
        "=",
        "MySqlHook",
        "(",
        "mysql_conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ",",
        "schema",
        "=",
        "self",
        ".",
        "database",
        ")",
        "return",
        "self",
        ".",
        "db_hook"
      ],
      "docstring": "Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.",
      "docstring_tokens": [
        "Retrieve",
        "database",
        "hook",
        ".",
        "This",
        "is",
        "the",
        "actual",
        "Postgres",
        "or",
        "MySQL",
        "database",
        "hook",
        "that",
        "uses",
        "proxy",
        "or",
        "connects",
        "directly",
        "to",
        "the",
        "Google",
        "Cloud",
        "SQL",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L961-L972"
    },
    "target": "Retrieve database hook . This is the actual Postgres or MySQL database hook that uses proxy or connects directly to the Google Cloud SQL database .",
    "arguments": [
      [
        "def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\ndef get_table_list ( self ): #... <== this is where we hit the failure: AttributeError: 'NoneType' object has no attribute'maximize_batch_size' return self. db_hook. get_table_names ( self. database, self. username, None ) #... rest of function... ``` \nAs shown in the [Stack Overflow](https://stackoverflow.com/questions/65534296/airflow-on-premises-docker-error-python-object-has-no-attribute) discussion, we have to use the `DefaultHook` instead of creating a custom hook.\nHere is the corrected code snippet from the `Custom_Database_Timestamp_Sensor` class:\n\n```python\ndef get_database_hook ( self ) : self. db_hook = DefaultHook ( dbtype = self. is_redshift, redshift_conn_id = self. db_conn_id ). get_ui_hook () return self. db_hook\n```\n\nThis should resolve the `AttributeError: 'NoneType' object has no attribute'maximize_batch_size'` error.  Just replace the code with the one that matches your specific sensor code. Please let me know if you have any issues. The solution is in the link you provided so I assume you already looked"
      ]
    ],
    "filtered_resps": [
      "\n\ndef get_table_list ( self ): #... <== this is where we hit the failure: AttributeError: 'NoneType' object has no attribute'maximize_batch_size' return self. db_hook. get_table_names ( self. database, self. username, None ) #... rest of function... ``` \nAs shown in the [Stack Overflow](https://stackoverflow.com/questions/65534296/airflow-on-premises-docker-error-python-object-has-no-attribute) discussion, we have to use the `DefaultHook` instead of creating a custom hook.\nHere is the corrected code snippet from the `Custom_Database_Timestamp_Sensor` class:\n\n```python\ndef get_database_hook ( self ) : self. db_hook = DefaultHook ( dbtype = self. is_redshift, redshift_conn_id = self. db_conn_id ). get_ui_hook () return self. db_hook\n```\n\nThis should resolve the `AttributeError: 'NoneType' object has no attribute'maximize_batch_size'` error.  Just replace the code with the one that matches your specific sensor code. Please let me know if you have any issues. The solution is in the link you provided so I assume you already looked"
    ],
    "smoothed_bleu_4": 0.9720766415429193
  },
  {
    "doc_id": 56,
    "doc": {
      "id": 56,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.cleanup_database_hook",
      "original_string": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)",
      "language": "python",
      "code": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)",
      "code_tokens": [
        "def",
        "cleanup_database_hook",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "database_type",
        "==",
        "'postgres'",
        ":",
        "if",
        "hasattr",
        "(",
        "self",
        ".",
        "db_hook",
        ",",
        "'conn'",
        ")",
        "and",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        "and",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        ".",
        "notices",
        ":",
        "for",
        "output",
        "in",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        ".",
        "notices",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "output",
        ")"
      ],
      "docstring": "Clean up database hook after it was used.",
      "docstring_tokens": [
        "Clean",
        "up",
        "database",
        "hook",
        "after",
        "it",
        "was",
        "used",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L974-L982"
    },
    "target": "Clean up database hook after it was used .",
    "arguments": [
      [
        "def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. db_hook. conn. clear_notices ( )\n             - ```sql\n- FROM public. t1\n- WHERE false\n- LIMIT 10000 ;\n- ``` \u00b6\n\n             - ```sql\n- DROP TABLE public. t1\n- ``` \u00b6\n\n             - ```sql\n- DROP TABLE IF EXISTS public. t1 ;\n- ``` \u00b6\n\n            - \u2018 pgbouncer \u2019: PostgreSQL bouncer is a PostgreSQL connection pooler for PostgreSQL. It is written in C and it\u2019s designed to work on multiple platforms. With this hook your database will be cleaned before the functional tests. \u00b6\n\n            - \u2018 pgbouncer \u2019: PostgreSQL bouncer is a PostgreSQL connection pooler for PostgreSQL. It is written in C and it\u2019s designed to work on multiple platforms. With this cleanup hook the testdb and the test schemas are dropped from the PostgreSQL database specified by `` pgbouncer_pool_db `` attribute, even if not specified.\n            - \u2018 redis \u2019: This hook cleans the test databases in case of Redis database specified by `` redis_db `` attribute. In other words, the database is flushed before running the tests (i.e., the test database is erased).\n            - \u2018 myetest \u2019: This hook cleans the"
      ]
    ],
    "filtered_resps": [
      " self. db_hook. conn. clear_notices ( )\n             - ```sql\n- FROM public. t1\n- WHERE false\n- LIMIT 10000 ;\n- ``` \u00b6\n\n             - ```sql\n- DROP TABLE public. t1\n- ``` \u00b6\n\n             - ```sql\n- DROP TABLE IF EXISTS public. t1 ;\n- ``` \u00b6\n\n            - \u2018 pgbouncer \u2019: PostgreSQL bouncer is a PostgreSQL connection pooler for PostgreSQL. It is written in C and it\u2019s designed to work on multiple platforms. With this hook your database will be cleaned before the functional tests. \u00b6\n\n            - \u2018 pgbouncer \u2019: PostgreSQL bouncer is a PostgreSQL connection pooler for PostgreSQL. It is written in C and it\u2019s designed to work on multiple platforms. With this cleanup hook the testdb and the test schemas are dropped from the PostgreSQL database specified by `` pgbouncer_pool_db `` attribute, even if not specified.\n            - \u2018 redis \u2019: This hook cleans the test databases in case of Redis database specified by `` redis_db `` attribute. In other words, the database is flushed before running the tests (i.e., the test database is erased).\n            - \u2018 myetest \u2019: This hook cleans the"
    ],
    "smoothed_bleu_4": 0.5886459914026825
  },
  {
    "doc_id": 57,
    "doc": {
      "id": 57,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.reserve_free_tcp_port",
      "original_string": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",
      "language": "python",
      "code": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",
      "code_tokens": [
        "def",
        "reserve_free_tcp_port",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "reserved_tcp_socket",
        "=",
        "socket",
        ".",
        "socket",
        "(",
        "socket",
        ".",
        "AF_INET",
        ",",
        "socket",
        ".",
        "SOCK_STREAM",
        ")",
        "self",
        ".",
        "reserved_tcp_socket",
        ".",
        "bind",
        "(",
        "(",
        "'127.0.0.1'",
        ",",
        "0",
        ")",
        ")",
        "self",
        ".",
        "sql_proxy_tcp_port",
        "=",
        "self",
        ".",
        "reserved_tcp_socket",
        ".",
        "getsockname",
        "(",
        ")",
        "[",
        "1",
        "]"
      ],
      "docstring": "Reserve free TCP port to be used by Cloud SQL Proxy",
      "docstring_tokens": [
        "Reserve",
        "free",
        "TCP",
        "port",
        "to",
        "be",
        "used",
        "by",
        "Cloud",
        "SQL",
        "Proxy"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990"
    },
    "target": "Reserve free TCP port to be used by Cloud SQL Proxy",
    "arguments": [
      [
        "def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. reserved_tcp_socket. listen ( 50 ) return self. reserved_tcp_socket def execute_shell_command ( self, args ) : return subprocess. Popen ( args, stderr = subprocess. STDOUT, stdout = subprocess. PIPE ). stdout. read ( ) def remote_execute ( self, cmd ) : self. send_json ( { 'cmd' : cmd } ) ans = self. socket. recv ( 1024 ) if not ans : raise CommunicationError ( ) ans = json. loads ( ans ) if ( ans ['status' ]!= 'ok' ) : return ans return ans [ 'output' ] def get_proxy_port ( self ) : return self. sql_proxy_tcp_port + 2 def set_sql_proxy_port ( self, sql_proxy_port ) : self. sql_proxy_tcp_port = sql_proxy_port get_process_id = lambda func : id ( inspect. currentframe ( ). f_back ) def install_packages ( context ) : import os, subprocess os. system ( \"\"\"cd /tmp/tap-installer-context/downloads && bash./tap-installer- %s.sh\"\"\" % context ) return subprocess. Popen ( [ 'pip', 'install', '--allow-external', 'tap"
      ]
    ],
    "filtered_resps": [
      " self. reserved_tcp_socket. listen ( 50 ) return self. reserved_tcp_socket def execute_shell_command ( self, args ) : return subprocess. Popen ( args, stderr = subprocess. STDOUT, stdout = subprocess. PIPE ). stdout. read ( ) def remote_execute ( self, cmd ) : self. send_json ( { 'cmd' : cmd } ) ans = self. socket. recv ( 1024 ) if not ans : raise CommunicationError ( ) ans = json. loads ( ans ) if ( ans ['status' ]!= 'ok' ) : return ans return ans [ 'output' ] def get_proxy_port ( self ) : return self. sql_proxy_tcp_port + 2 def set_sql_proxy_port ( self, sql_proxy_port ) : self. sql_proxy_tcp_port = sql_proxy_port get_process_id = lambda func : id ( inspect. currentframe ( ). f_back ) def install_packages ( context ) : import os, subprocess os. system ( \"\"\"cd /tmp/tap-installer-context/downloads && bash./tap-installer- %s.sh\"\"\" % context ) return subprocess. Popen ( [ 'pip', 'install', '--allow-external', 'tap"
    ],
    "smoothed_bleu_4": 0.506435023307207
  },
  {
    "doc_id": 58,
    "doc": {
      "id": 58,
      "repo": "apache/airflow",
      "path": "airflow/contrib/operators/mlengine_operator.py",
      "func_name": "_normalize_mlengine_job_id",
      "original_string": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id",
      "language": "python",
      "code": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id",
      "code_tokens": [
        "def",
        "_normalize_mlengine_job_id",
        "(",
        "job_id",
        ")",
        ":",
        "# Add a prefix when a job_id starts with a digit or a template",
        "match",
        "=",
        "re",
        ".",
        "search",
        "(",
        "r'\\d|\\{{2}'",
        ",",
        "job_id",
        ")",
        "if",
        "match",
        "and",
        "match",
        ".",
        "start",
        "(",
        ")",
        "==",
        "0",
        ":",
        "job",
        "=",
        "'z_{}'",
        ".",
        "format",
        "(",
        "job_id",
        ")",
        "else",
        ":",
        "job",
        "=",
        "job_id",
        "# Clean up 'bad' characters except templates",
        "tracker",
        "=",
        "0",
        "cleansed_job_id",
        "=",
        "''",
        "for",
        "m",
        "in",
        "re",
        ".",
        "finditer",
        "(",
        "r'\\{{2}.+?\\}{2}'",
        ",",
        "job",
        ")",
        ":",
        "cleansed_job_id",
        "+=",
        "re",
        ".",
        "sub",
        "(",
        "r'[^0-9a-zA-Z]+'",
        ",",
        "'_'",
        ",",
        "job",
        "[",
        "tracker",
        ":",
        "m",
        ".",
        "start",
        "(",
        ")",
        "]",
        ")",
        "cleansed_job_id",
        "+=",
        "job",
        "[",
        "m",
        ".",
        "start",
        "(",
        ")",
        ":",
        "m",
        ".",
        "end",
        "(",
        ")",
        "]",
        "tracker",
        "=",
        "m",
        ".",
        "end",
        "(",
        ")",
        "# Clean up last substring or the full string if no templates",
        "cleansed_job_id",
        "+=",
        "re",
        ".",
        "sub",
        "(",
        "r'[^0-9a-zA-Z]+'",
        ",",
        "'_'",
        ",",
        "job",
        "[",
        "tracker",
        ":",
        "]",
        ")",
        "return",
        "cleansed_job_id"
      ],
      "docstring": "Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.",
      "docstring_tokens": [
        "Replaces",
        "invalid",
        "MLEngine",
        "job_id",
        "characters",
        "with",
        "_",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mlengine_operator.py#L29-L62"
    },
    "target": "Replaces invalid MLEngine job_id characters with _ .",
    "arguments": [
      [
        "def _normalize_mlengine_job_id ( job_id ) : # Add a prefix when a job_id starts with a digit or a template match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id # Clean up 'bad' characters except templates tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) # Clean up last substring or the full string if no templates cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n### Step 5: Clean up the cleansed job_id to make it fit 63 characters.\n```_clean_up_ascii_job_id(job_id)_\n```\n``` python\ndef _clean_up_ascii_job_id(job_id): # Fit to 63 characters for full job_id, otherwise 63 - 2 char threshold limit = 63 - 2 if len(job_id) > limit: temp_name = '_{}' = str(limit) return re.sub(r'[^a-zA-Z0-9]', '_', temp_name) + re.sub(r'[^a-zA-Z0-9]', '_', job_id[:limit - len(temp_name)]) + re.sub(r'[^a-zA-Z0-9]', '_', temp_name) else: return job_id ```\n### Step 5: Create a dictionary mapping cleansed job_id to original job_id.\n```_job_id_mapping()_\n```\n``` python\ndef _job_id_mapping(): job_id_maps = {} maps_file_path = \"path/machinelearningenginejobidmapping.txt\" try: with open(maps_file_path, 'r') as map_file: for line in map_file.readlines(): old_job_id, new_job_id = line.strip().split(',') job_id_maps[new"
      ]
    ],
    "filtered_resps": [
      " ```\n### Step 5: Clean up the cleansed job_id to make it fit 63 characters.\n```_clean_up_ascii_job_id(job_id)_\n```\n``` python\ndef _clean_up_ascii_job_id(job_id): # Fit to 63 characters for full job_id, otherwise 63 - 2 char threshold limit = 63 - 2 if len(job_id) > limit: temp_name = '_{}' = str(limit) return re.sub(r'[^a-zA-Z0-9]', '_', temp_name) + re.sub(r'[^a-zA-Z0-9]', '_', job_id[:limit - len(temp_name)]) + re.sub(r'[^a-zA-Z0-9]', '_', temp_name) else: return job_id ```\n### Step 5: Create a dictionary mapping cleansed job_id to original job_id.\n```_job_id_mapping()_\n```\n``` python\ndef _job_id_mapping(): job_id_maps = {} maps_file_path = \"path/machinelearningenginejobidmapping.txt\" try: with open(maps_file_path, 'r') as map_file: for line in map_file.readlines(): old_job_id, new_job_id = line.strip().split(',') job_id_maps[new"
    ],
    "smoothed_bleu_4": 0.7395024793551657
  },
  {
    "doc_id": 59,
    "doc": {
      "id": 59,
      "repo": "apache/airflow",
      "path": "airflow/contrib/sensors/ftp_sensor.py",
      "func_name": "FTPSensor._get_error_code",
      "original_string": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e",
      "language": "python",
      "code": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e",
      "code_tokens": [
        "def",
        "_get_error_code",
        "(",
        "self",
        ",",
        "e",
        ")",
        ":",
        "try",
        ":",
        "matches",
        "=",
        "self",
        ".",
        "error_code_pattern",
        ".",
        "match",
        "(",
        "str",
        "(",
        "e",
        ")",
        ")",
        "code",
        "=",
        "int",
        "(",
        "matches",
        ".",
        "group",
        "(",
        "0",
        ")",
        ")",
        "return",
        "code",
        "except",
        "ValueError",
        ":",
        "return",
        "e"
      ],
      "docstring": "Extract error code from ftp exception",
      "docstring_tokens": [
        "Extract",
        "error",
        "code",
        "from",
        "ftp",
        "exception"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/ftp_sensor.py#L69-L76"
    },
    "target": "Extract error code from ftp exception",
    "arguments": [
      [
        "def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except ValueError : return e",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\n  - ```... ( details omitted )... errors   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()] ```\n\nUsing this technique, we can simplify the iteration over the system exceptions.\n\n# Simplifying error message construction\n\n## Step 1: Introduce a new list (Simpler) to store the error codes\n```python\n# Previous code...\nSimpler   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()]\n# ( details omitted )...\nfor   code   in   Simple :\n     # ( details omitted )...\n```\n\n## Step 2: Replace the original list (errors) with the new list (Simpler)\n```python\n# Previous code...\nSimpler   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()]\nerrors   =   Simpller\n```\n\n## Step 3: Remove the debugging print statement\n```python\n# Previous code...\nSimpler   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()]\nerrors   =   Simpller\n# The"
      ]
    ],
    "filtered_resps": [
      "\n\n  - ```... ( details omitted )... errors   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()] ```\n\nUsing this technique, we can simplify the iteration over the system exceptions.\n\n# Simplifying error message construction\n\n## Step 1: Introduce a new list (Simpler) to store the error codes\n```python\n# Previous code...\nSimpler   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()]\n# ( details omitted )...\nfor   code   in   Simple :\n     # ( details omitted )...\n```\n\n## Step 2: Replace the original list (errors) with the new list (Simpler)\n```python\n# Previous code...\nSimpler   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()]\nerrors   =   Simpller\n```\n\n## Step 3: Remove the debugging print statement\n```python\n# Previous code...\nSimpler   =   [ _get_error_code ( e )   for   e   in   system. exceptions ()]\nerrors   =   Simpller\n# The"
    ],
    "smoothed_bleu_4": 0.5164885277173531
  },
  {
    "doc_id": 60,
    "doc": {
      "id": 60,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "clear_dag_runs",
      "original_string": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)",
      "language": "python",
      "code": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)",
      "code_tokens": [
        "def",
        "clear_dag_runs",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "drs",
        "=",
        "session",
        ".",
        "query",
        "(",
        "DagRun",
        ")",
        ".",
        "filter",
        "(",
        "DagRun",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ",",
        ")",
        ".",
        "all",
        "(",
        ")",
        "for",
        "dr",
        "in",
        "drs",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Deleting DagRun :: {}'",
        ".",
        "format",
        "(",
        "dr",
        ")",
        ")",
        "session",
        ".",
        "delete",
        "(",
        "dr",
        ")"
      ],
      "docstring": "Remove any existing DAG runs for the perf test DAGs.",
      "docstring_tokens": [
        "Remove",
        "any",
        "existing",
        "DAG",
        "runs",
        "for",
        "the",
        "perf",
        "test",
        "DAGs",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L138-L148"
    },
    "target": "Remove any existing DAG runs for the perf test DAGs .",
    "arguments": [
      [
        "def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session . delete ( dr )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " session. commit ( )\n- def run_dag ( dag_run_id ) : dag_run = session. query ( DagRun ). get ( dag_run_id ) session. commit ( ) dag_run. run ( ) session. commit ( )\n- def mark_dag_as_failed ( dag_run_id, reason = 'Mark dag as failed' ) : session. query ( DagRun ). get ( dag_run_id ).mark_success ( reason ) session. commit ( )\n- def mark_dag_as_skipped ( dag_return_code = 1, log_level = logging. INFO ) : session. commit ( )\n- def get_airflow_gcs_buckets ( ) -> List [ str ] : session = settings. Session ( ) buckets = session. query ( Bucket ). filter ( Bucket. project_id == settings. GCP_PROJECT_ID ). all ( ) return buckets\n- def get_airflow_gcs_project_storage ( ) -> int : return settings. USER_BUCKET_OBJECT_LIMIT * GIGABYTES\n- def save_airflow_gcs_data ( key, data ) : bucket_name = key. split ( '/', 1 ) [ 0 ] session = settings. Session ( ) gcsutil = bucket. Client"
      ]
    ],
    "filtered_resps": [
      " session. commit ( )\n- def run_dag ( dag_run_id ) : dag_run = session. query ( DagRun ). get ( dag_run_id ) session. commit ( ) dag_run. run ( ) session. commit ( )\n- def mark_dag_as_failed ( dag_run_id, reason = 'Mark dag as failed' ) : session. query ( DagRun ). get ( dag_run_id ).mark_success ( reason ) session. commit ( )\n- def mark_dag_as_skipped ( dag_return_code = 1, log_level = logging. INFO ) : session. commit ( )\n- def get_airflow_gcs_buckets ( ) -> List [ str ] : session = settings. Session ( ) buckets = session. query ( Bucket ). filter ( Bucket. project_id == settings. GCP_PROJECT_ID ). all ( ) return buckets\n- def get_airflow_gcs_project_storage ( ) -> int : return settings. USER_BUCKET_OBJECT_LIMIT * GIGABYTES\n- def save_airflow_gcs_data ( key, data ) : bucket_name = key. split ( '/', 1 ) [ 0 ] session = settings. Session ( ) gcsutil = bucket. Client"
    ],
    "smoothed_bleu_4": 0.4433225728922534
  },
  {
    "doc_id": 61,
    "doc": {
      "id": 61,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "clear_dag_task_instances",
      "original_string": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()",
      "language": "python",
      "code": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()",
      "code_tokens": [
        "def",
        "clear_dag_task_instances",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "TI",
        "=",
        "TaskInstance",
        "tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "for",
        "ti",
        "in",
        "tis",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Deleting TaskInstance :: {}'",
        ".",
        "format",
        "(",
        "ti",
        ")",
        ")",
        "session",
        ".",
        "delete",
        "(",
        "ti",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Remove any existing task instances for the perf test DAGs.",
      "docstring_tokens": [
        "Remove",
        "any",
        "existing",
        "task",
        "instances",
        "for",
        "the",
        "perf",
        "test",
        "DAGs",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L151-L166"
    },
    "target": "Remove any existing task instances for the perf test DAGs .",
    "arguments": [
      [
        "def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " session. expunge ( ti ) return ```\n- 56\n- 57\n- In the next code block, we have the method clear_dag_run_dagRuns, which deletes all DAG Runs DAG RUN\n- 58\n- 59\n- ```from airflow import DAG from airflow.models import DAGRun class ClearOrphanedDagRun ( object ) : def clear_dag_run ( self, session, dag_runs, *dag_run_ids ) : for dr_id in dag_run_ids : dag_run_to_delete = session. query ( DAGRun ). filter ( DAGRun. dag_id. in_ ( DAG_IDS ) ). filter ( DAGRun. dag_run_id == dr_id ). first ( ) if dag_run_to_delete : session. delete ( dag_run_to_delete ) session. commit ( ) def clear_dag_run_dagRuns ( self, *dag_run_ids ) : session = settings. Session ( ) self. clear_dag_run ( session, DAGRun, * dag_run_ids ) ```\n- 60\n- 61\n- Lastly, we have the method __main__, which we use as the entry point for the script. The script\u2019s purpose is to clear orphaned DAGs"
      ]
    ],
    "filtered_resps": [
      " session. expunge ( ti ) return ```\n- 56\n- 57\n- In the next code block, we have the method clear_dag_run_dagRuns, which deletes all DAG Runs DAG RUN\n- 58\n- 59\n- ```from airflow import DAG from airflow.models import DAGRun class ClearOrphanedDagRun ( object ) : def clear_dag_run ( self, session, dag_runs, *dag_run_ids ) : for dr_id in dag_run_ids : dag_run_to_delete = session. query ( DAGRun ). filter ( DAGRun. dag_id. in_ ( DAG_IDS ) ). filter ( DAGRun. dag_run_id == dr_id ). first ( ) if dag_run_to_delete : session. delete ( dag_run_to_delete ) session. commit ( ) def clear_dag_run_dagRuns ( self, *dag_run_ids ) : session = settings. Session ( ) self. clear_dag_run ( session, DAGRun, * dag_run_ids ) ```\n- 60\n- 61\n- Lastly, we have the method __main__, which we use as the entry point for the script. The script\u2019s purpose is to clear orphaned DAGs"
    ],
    "smoothed_bleu_4": 0.6462253122529223
  },
  {
    "doc_id": 62,
    "doc": {
      "id": 62,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "set_dags_paused_state",
      "original_string": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
      "language": "python",
      "code": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
      "code_tokens": [
        "def",
        "set_dags_paused_state",
        "(",
        "is_paused",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "dms",
        "=",
        "session",
        ".",
        "query",
        "(",
        "DagModel",
        ")",
        ".",
        "filter",
        "(",
        "DagModel",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        "for",
        "dm",
        "in",
        "dms",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Setting DAG :: {} is_paused={}'",
        ".",
        "format",
        "(",
        "dm",
        ",",
        "is_paused",
        ")",
        ")",
        "dm",
        ".",
        "is_paused",
        "=",
        "is_paused",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Toggle the pause state of the DAGs in the test.",
      "docstring_tokens": [
        "Toggle",
        "the",
        "pause",
        "state",
        "of",
        "the",
        "DAGs",
        "in",
        "the",
        "test",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L169-L179"
    },
    "target": "Toggle the pause state of the DAGs in the test .",
    "arguments": [
      [
        "def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return is_paused\n```\n\n**Example Use Case**\n\nTo pause a DAG, call the `set_dags_paused_state` function with `is_paused=True`, replacing `DAG_IDS` with the ID(s) of the DAG(s) you want to pause:\n```python\nset_dags_paused_state ( True, DAG_IDS = ['my_dag_1','my_dag_2' ] )\n```\nTo resume a paused DAG, call the function with `is_paused=False`:\n```python\nset_dags_paused_state ( False, DAG_IDS = ['my_dag_1','my_dag_2' ] )\n```\nNote that this function modifies the DAG's `is_paused` attribute in the database and requires a SQLAlchemy session to execute. Make sure you have a working session setup before calling this function. **Deploys to** - **Deployment Environment** - **Azure** - **Azure Blob Storage** - **Azure Kubernetes Service** ( AKS ) - **Azure Service Bus** - **Azure Storage Queue** - **DigitalOcean** - **Google Cloud Platform** ( GCP ) - **GCP Artifact Registry** - **GCP Cloud"
      ]
    ],
    "filtered_resps": [
      " return is_paused\n```\n\n**Example Use Case**\n\nTo pause a DAG, call the `set_dags_paused_state` function with `is_paused=True`, replacing `DAG_IDS` with the ID(s) of the DAG(s) you want to pause:\n```python\nset_dags_paused_state ( True, DAG_IDS = ['my_dag_1','my_dag_2' ] )\n```\nTo resume a paused DAG, call the function with `is_paused=False`:\n```python\nset_dags_paused_state ( False, DAG_IDS = ['my_dag_1','my_dag_2' ] )\n```\nNote that this function modifies the DAG's `is_paused` attribute in the database and requires a SQLAlchemy session to execute. Make sure you have a working session setup before calling this function. **Deploys to** - **Deployment Environment** - **Azure** - **Azure Blob Storage** - **Azure Kubernetes Service** ( AKS ) - **Azure Service Bus** - **Azure Storage Queue** - **DigitalOcean** - **Google Cloud Platform** ( GCP ) - **GCP Artifact Registry** - **GCP Cloud"
    ],
    "smoothed_bleu_4": 0.8104949236118748
  },
  {
    "doc_id": 63,
    "doc": {
      "id": 63,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "SchedulerMetricsJob.print_stats",
      "original_string": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()",
      "language": "python",
      "code": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()",
      "code_tokens": [
        "def",
        "print_stats",
        "(",
        "self",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "TI",
        "=",
        "TaskInstance",
        "tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "successful_tis",
        "=",
        "[",
        "x",
        "for",
        "x",
        "in",
        "tis",
        "if",
        "x",
        ".",
        "state",
        "==",
        "State",
        ".",
        "SUCCESS",
        "]",
        "ti_perf",
        "=",
        "[",
        "(",
        "ti",
        ".",
        "dag_id",
        ",",
        "ti",
        ".",
        "task_id",
        ",",
        "ti",
        ".",
        "execution_date",
        ",",
        "(",
        "ti",
        ".",
        "queued_dttm",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "(",
        "ti",
        ".",
        "start_date",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "(",
        "ti",
        ".",
        "end_date",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "ti",
        ".",
        "duration",
        ")",
        "for",
        "ti",
        "in",
        "successful_tis",
        "]",
        "ti_perf_df",
        "=",
        "pd",
        ".",
        "DataFrame",
        "(",
        "ti_perf",
        ",",
        "columns",
        "=",
        "[",
        "'dag_id'",
        ",",
        "'task_id'",
        ",",
        "'execution_date'",
        ",",
        "'queue_delay'",
        ",",
        "'start_delay'",
        ",",
        "'land_time'",
        ",",
        "'duration'",
        "]",
        ")",
        "print",
        "(",
        "'Performance Results'",
        ")",
        "print",
        "(",
        "'###################'",
        ")",
        "for",
        "dag_id",
        "in",
        "DAG_IDS",
        ":",
        "print",
        "(",
        "'DAG {}'",
        ".",
        "format",
        "(",
        "dag_id",
        ")",
        ")",
        "print",
        "(",
        "ti_perf_df",
        "[",
        "ti_perf_df",
        "[",
        "'dag_id'",
        "]",
        "==",
        "dag_id",
        "]",
        ")",
        "print",
        "(",
        "'###################'",
        ")",
        "if",
        "len",
        "(",
        "tis",
        ")",
        ">",
        "len",
        "(",
        "successful_tis",
        ")",
        ":",
        "print",
        "(",
        "\"WARNING!! The following task instances haven't completed\"",
        ")",
        "print",
        "(",
        "pd",
        ".",
        "DataFrame",
        "(",
        "[",
        "(",
        "ti",
        ".",
        "dag_id",
        ",",
        "ti",
        ".",
        "task_id",
        ",",
        "ti",
        ".",
        "execution_date",
        ",",
        "ti",
        ".",
        "state",
        ")",
        "for",
        "ti",
        "in",
        "filter",
        "(",
        "lambda",
        "x",
        ":",
        "x",
        ".",
        "state",
        "!=",
        "State",
        ".",
        "SUCCESS",
        ",",
        "tis",
        ")",
        "]",
        ",",
        "columns",
        "=",
        "[",
        "'dag_id'",
        ",",
        "'task_id'",
        ",",
        "'execution_date'",
        ",",
        "'state'",
        "]",
        ")",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Print operational metrics for the scheduler test.",
      "docstring_tokens": [
        "Print",
        "operational",
        "metrics",
        "for",
        "the",
        "scheduler",
        "test",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L65-L101"
    },
    "target": "Print operational metrics for the scheduler test .",
    "arguments": [
      [
        "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nThis code creates a session with Airflow, retrieves TaskInstance objects matching certain condition and prints statistics on these tasks. It queries successfully completed tasks, calculates various performance metrics, prints statistics by dagger per dagId, and provides warning information about uncompleted tasks.\n\n### Usage:\n\n1\\. Install the necessary libraries by running `pip install apache-airflow` in your command line. \n\n2\\. Make sure you have the connection to your Airflow database of which you want to query. Create a new file in your Airflow directory i.e., `airflow.cfg` if it does not already exist. \n\n3\\. Define the `dags = [\"<DAG_Name>\"]` in `airflow.cfg` file to point to your DAG with the name you specified. You can modify this if you have multiple dags and want to run performance analysis for multiple dags.\n\n4\\. Provide the necessary details in `airflow.cfg` to point to your Airflow database, i.e., `sql_alchemy_conn` for Postgres and MySQL to perform the Airflow database connection for query execution.\n\n5\\. You can run this code in python script (preferably outside of airflow) to check for performance results.\n\n6\\. Provide the appropriate"
      ]
    ],
    "filtered_resps": [
      " ```\n\nThis code creates a session with Airflow, retrieves TaskInstance objects matching certain condition and prints statistics on these tasks. It queries successfully completed tasks, calculates various performance metrics, prints statistics by dagger per dagId, and provides warning information about uncompleted tasks.\n\n### Usage:\n\n1\\. Install the necessary libraries by running `pip install apache-airflow` in your command line. \n\n2\\. Make sure you have the connection to your Airflow database of which you want to query. Create a new file in your Airflow directory i.e., `airflow.cfg` if it does not already exist. \n\n3\\. Define the `dags = [\"<DAG_Name>\"]` in `airflow.cfg` file to point to your DAG with the name you specified. You can modify this if you have multiple dags and want to run performance analysis for multiple dags.\n\n4\\. Provide the necessary details in `airflow.cfg` to point to your Airflow database, i.e., `sql_alchemy_conn` for Postgres and MySQL to perform the Airflow database connection for query execution.\n\n5\\. You can run this code in python script (preferably outside of airflow) to check for performance results.\n\n6\\. Provide the appropriate"
    ],
    "smoothed_bleu_4": 0.5651234100161556
  },
  {
    "doc_id": 64,
    "doc": {
      "id": 64,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "SchedulerMetricsJob.heartbeat",
      "original_string": "def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()",
      "language": "python",
      "code": "def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()",
      "code_tokens": [
        "def",
        "heartbeat",
        "(",
        "self",
        ")",
        ":",
        "super",
        "(",
        "SchedulerMetricsJob",
        ",",
        "self",
        ")",
        ".",
        "heartbeat",
        "(",
        ")",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "# Get all the relevant task instances",
        "TI",
        "=",
        "TaskInstance",
        "successful_tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "state",
        ".",
        "in_",
        "(",
        "[",
        "State",
        ".",
        "SUCCESS",
        "]",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "dagbag",
        "=",
        "DagBag",
        "(",
        "SUBDIR",
        ")",
        "dags",
        "=",
        "[",
        "dagbag",
        ".",
        "dags",
        "[",
        "dag_id",
        "]",
        "for",
        "dag_id",
        "in",
        "DAG_IDS",
        "]",
        "# the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.",
        "num_task_instances",
        "=",
        "sum",
        "(",
        "[",
        "(",
        "timezone",
        ".",
        "utcnow",
        "(",
        ")",
        "-",
        "task",
        ".",
        "start_date",
        ")",
        ".",
        "days",
        "for",
        "dag",
        "in",
        "dags",
        "for",
        "task",
        "in",
        "dag",
        ".",
        "tasks",
        "]",
        ")",
        "if",
        "(",
        "len",
        "(",
        "successful_tis",
        ")",
        "==",
        "num_task_instances",
        "or",
        "(",
        "timezone",
        ".",
        "utcnow",
        "(",
        ")",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ">",
        "MAX_RUNTIME_SECS",
        ")",
        ":",
        "if",
        "len",
        "(",
        "successful_tis",
        ")",
        "==",
        "num_task_instances",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"All tasks processed! Printing stats.\"",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Test timeout reached. Printing available stats.\"",
        ")",
        "self",
        ".",
        "print_stats",
        "(",
        ")",
        "set_dags_paused_state",
        "(",
        "True",
        ")",
        "sys",
        ".",
        "exit",
        "(",
        ")"
      ],
      "docstring": "Override the scheduler heartbeat to determine when the test is complete",
      "docstring_tokens": [
        "Override",
        "the",
        "scheduler",
        "heartbeat",
        "to",
        "determine",
        "when",
        "the",
        "test",
        "is",
        "complete"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L103-L135"
    },
    "target": "Override the scheduler heartbeat to determine when the test is complete",
    "arguments": [
      [
        "def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) # Get all the relevant task instances TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval. num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` The code is designed to monitor a set of DAGs which includes two DAGs with a daily schedule. It runs a series of tests to simulate execution of the tasks within the DAGs and prints statistics such as task and dag runtime. This code assumes that there are two DAGs named `perf_dag_1` and `perf_dag_2`. If all tasks are run successfully before the maximum allowed runtime, the code will stop the test, otherwise it will continue until the test runs for a maximum specified period.\nIn conclusion, the code provided is a Python script that extends the `SchedulerMetricsJob` class to create a test job that simulates task execution of a specified directory of DAGs by Apache Airflow. The script includes various functions to execute tasks within DAGs, log information, and print statistics.\n\n### Step 5: Validate the Code\nTo validate the code, we need to ensure that it is correctly parsing the DAGs in the specified directory, executing the tasks, logging information, and printing statistics correctly.\n\n```python\n# Example usage:\nfrom airflow.utils.state import State\nfrom datetime import datetime, timedelta\n\ndag_ids = ['perf_dag_1', 'perf_dag_2']\ndag_bag ="
      ]
    ],
    "filtered_resps": [
      " ``` The code is designed to monitor a set of DAGs which includes two DAGs with a daily schedule. It runs a series of tests to simulate execution of the tasks within the DAGs and prints statistics such as task and dag runtime. This code assumes that there are two DAGs named `perf_dag_1` and `perf_dag_2`. If all tasks are run successfully before the maximum allowed runtime, the code will stop the test, otherwise it will continue until the test runs for a maximum specified period.\nIn conclusion, the code provided is a Python script that extends the `SchedulerMetricsJob` class to create a test job that simulates task execution of a specified directory of DAGs by Apache Airflow. The script includes various functions to execute tasks within DAGs, log information, and print statistics.\n\n### Step 5: Validate the Code\nTo validate the code, we need to ensure that it is correctly parsing the DAGs in the specified directory, executing the tasks, logging information, and printing statistics correctly.\n\n```python\n# Example usage:\nfrom airflow.utils.state import State\nfrom datetime import datetime, timedelta\n\ndag_ids = ['perf_dag_1', 'perf_dag_2']\ndag_bag ="
    ],
    "smoothed_bleu_4": 0.7192272681497477
  },
  {
    "doc_id": 65,
    "doc": {
      "id": 65,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/aws_lambda_hook.py",
      "func_name": "AwsLambdaHook.invoke_lambda",
      "original_string": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response",
      "language": "python",
      "code": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response",
      "code_tokens": [
        "def",
        "invoke_lambda",
        "(",
        "self",
        ",",
        "payload",
        ")",
        ":",
        "awslambda_conn",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        "response",
        "=",
        "awslambda_conn",
        ".",
        "invoke",
        "(",
        "FunctionName",
        "=",
        "self",
        ".",
        "function_name",
        ",",
        "InvocationType",
        "=",
        "self",
        ".",
        "invocation_type",
        ",",
        "LogType",
        "=",
        "self",
        ".",
        "log_type",
        ",",
        "Payload",
        "=",
        "payload",
        ",",
        "Qualifier",
        "=",
        "self",
        ".",
        "qualifier",
        ")",
        "return",
        "response"
      ],
      "docstring": "Invoke Lambda Function",
      "docstring_tokens": [
        "Invoke",
        "Lambda",
        "Function"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_lambda_hook.py#L53-L68"
    },
    "target": "Invoke Lambda Function",
    "arguments": [
      [
        "def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( FunctionName = self . function_name , InvocationType = self . invocation_type , LogType = self . log_type , Payload = payload , Qualifier = self . qualifier ) return response",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\n- def invoke_sync ( self, payload, ** kwargs ) :\n    \"\"\"\n    Invokes a Lambda function synchronously and returns the payload as\n    a dict or list from the invocation response.\n\n    Args:\n        payload (object): JSON Data (JSON string, dictionary, etc.)\n\n    Raises:\n        :py:exc:`botocore.exceptions.ClientError`: if invocation fails\n    \"\"\"\n    response = invoke_lambda ( self, payload, ** kwargs )\n    body = response. get ( 'Payload' )\n    if body is None :\n        raise self. exception ( response )\n    return load_pyobj_from_json ( body. read ( ) )\n\n- def list_aliases ( self ) :\n    \"\"\"\n    Returns a list of Lambda function aliases.\n    \"\"\"\n    alias = self. get_conn ( ). list_aliases ( FunctionName = self. function_name )\n    return [ Alias. __call__ ( item ) for item in alias ]\n\n- def list_invocations ( self ) :\n    \"\"\"\n    Returns a list of Lambda function invocations.\n    \"\"\"\n    invocations = self. get_conn ( ). list_invocations ( FunctionName = self. function_name )\n    return [ Invocation. __call__ ( item ) for item in"
      ]
    ],
    "filtered_resps": [
      "\n\n- def invoke_sync ( self, payload, ** kwargs ) :\n    \"\"\"\n    Invokes a Lambda function synchronously and returns the payload as\n    a dict or list from the invocation response.\n\n    Args:\n        payload (object): JSON Data (JSON string, dictionary, etc.)\n\n    Raises:\n        :py:exc:`botocore.exceptions.ClientError`: if invocation fails\n    \"\"\"\n    response = invoke_lambda ( self, payload, ** kwargs )\n    body = response. get ( 'Payload' )\n    if body is None :\n        raise self. exception ( response )\n    return load_pyobj_from_json ( body. read ( ) )\n\n- def list_aliases ( self ) :\n    \"\"\"\n    Returns a list of Lambda function aliases.\n    \"\"\"\n    alias = self. get_conn ( ). list_aliases ( FunctionName = self. function_name )\n    return [ Alias. __call__ ( item ) for item in alias ]\n\n- def list_invocations ( self ) :\n    \"\"\"\n    Returns a list of Lambda function invocations.\n    \"\"\"\n    invocations = self. get_conn ( ). list_invocations ( FunctionName = self. function_name )\n    return [ Invocation. __call__ ( item ) for item in"
    ],
    "smoothed_bleu_4": 0.6304504811769059
  },
  {
    "doc_id": 66,
    "doc": {
      "id": 66,
      "repo": "apache/airflow",
      "path": "airflow/contrib/utils/mlengine_operator_utils.py",
      "func_name": "create_evaluate_ops",
      "original_string": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation",
      "language": "python",
      "code": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation",
      "code_tokens": [
        "def",
        "create_evaluate_ops",
        "(",
        "task_prefix",
        ",",
        "data_format",
        ",",
        "input_paths",
        ",",
        "prediction_path",
        ",",
        "metric_fn_and_keys",
        ",",
        "validate_fn",
        ",",
        "batch_prediction_job_id",
        "=",
        "None",
        ",",
        "project_id",
        "=",
        "None",
        ",",
        "region",
        "=",
        "None",
        ",",
        "dataflow_options",
        "=",
        "None",
        ",",
        "model_uri",
        "=",
        "None",
        ",",
        "model_name",
        "=",
        "None",
        ",",
        "version_name",
        "=",
        "None",
        ",",
        "dag",
        "=",
        "None",
        ")",
        ":",
        "# Verify that task_prefix doesn't have any special characters except hyphen",
        "# '-', which is the only allowed non-alphanumeric character by Dataflow.",
        "if",
        "not",
        "re",
        ".",
        "match",
        "(",
        "r\"^[a-zA-Z][-A-Za-z0-9]*$\"",
        ",",
        "task_prefix",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"",
        "\"and hyphens are allowed but got: \"",
        "+",
        "task_prefix",
        ")",
        "metric_fn",
        ",",
        "metric_keys",
        "=",
        "metric_fn_and_keys",
        "if",
        "not",
        "callable",
        "(",
        "metric_fn",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"`metric_fn` param must be callable.\"",
        ")",
        "if",
        "not",
        "callable",
        "(",
        "validate_fn",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"`validate_fn` param must be callable.\"",
        ")",
        "if",
        "dag",
        "is",
        "not",
        "None",
        "and",
        "dag",
        ".",
        "default_args",
        "is",
        "not",
        "None",
        ":",
        "default_args",
        "=",
        "dag",
        ".",
        "default_args",
        "project_id",
        "=",
        "project_id",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'project_id'",
        ")",
        "region",
        "=",
        "region",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'region'",
        ")",
        "model_name",
        "=",
        "model_name",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'model_name'",
        ")",
        "version_name",
        "=",
        "version_name",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'version_name'",
        ")",
        "dataflow_options",
        "=",
        "dataflow_options",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'dataflow_default_options'",
        ")",
        "evaluate_prediction",
        "=",
        "MLEngineBatchPredictionOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-prediction\"",
        ")",
        ",",
        "project_id",
        "=",
        "project_id",
        ",",
        "job_id",
        "=",
        "batch_prediction_job_id",
        ",",
        "region",
        "=",
        "region",
        ",",
        "data_format",
        "=",
        "data_format",
        ",",
        "input_paths",
        "=",
        "input_paths",
        ",",
        "output_path",
        "=",
        "prediction_path",
        ",",
        "uri",
        "=",
        "model_uri",
        ",",
        "model_name",
        "=",
        "model_name",
        ",",
        "version_name",
        "=",
        "version_name",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "metric_fn_encoded",
        "=",
        "base64",
        ".",
        "b64encode",
        "(",
        "dill",
        ".",
        "dumps",
        "(",
        "metric_fn",
        ",",
        "recurse",
        "=",
        "True",
        ")",
        ")",
        "evaluate_summary",
        "=",
        "DataFlowPythonOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-summary\"",
        ")",
        ",",
        "py_options",
        "=",
        "[",
        "\"-m\"",
        "]",
        ",",
        "py_file",
        "=",
        "\"airflow.contrib.utils.mlengine_prediction_summary\"",
        ",",
        "dataflow_default_options",
        "=",
        "dataflow_options",
        ",",
        "options",
        "=",
        "{",
        "\"prediction_path\"",
        ":",
        "prediction_path",
        ",",
        "\"metric_fn_encoded\"",
        ":",
        "metric_fn_encoded",
        ",",
        "\"metric_keys\"",
        ":",
        "','",
        ".",
        "join",
        "(",
        "metric_keys",
        ")",
        "}",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "evaluate_summary",
        ".",
        "set_upstream",
        "(",
        "evaluate_prediction",
        ")",
        "def",
        "apply_validate_fn",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "prediction_path",
        "=",
        "kwargs",
        "[",
        "\"templates_dict\"",
        "]",
        "[",
        "\"prediction_path\"",
        "]",
        "scheme",
        ",",
        "bucket",
        ",",
        "obj",
        ",",
        "_",
        ",",
        "_",
        "=",
        "urlsplit",
        "(",
        "prediction_path",
        ")",
        "if",
        "scheme",
        "!=",
        "\"gs\"",
        "or",
        "not",
        "bucket",
        "or",
        "not",
        "obj",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"Wrong format prediction_path: %s\"",
        ",",
        "prediction_path",
        ")",
        "summary",
        "=",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "obj",
        ".",
        "strip",
        "(",
        "\"/\"",
        ")",
        ",",
        "\"prediction.summary.json\"",
        ")",
        "gcs_hook",
        "=",
        "GoogleCloudStorageHook",
        "(",
        ")",
        "summary",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "gcs_hook",
        ".",
        "download",
        "(",
        "bucket",
        ",",
        "summary",
        ")",
        ")",
        "return",
        "validate_fn",
        "(",
        "summary",
        ")",
        "evaluate_validation",
        "=",
        "PythonOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-validation\"",
        ")",
        ",",
        "python_callable",
        "=",
        "apply_validate_fn",
        ",",
        "provide_context",
        "=",
        "True",
        ",",
        "templates_dict",
        "=",
        "{",
        "\"prediction_path\"",
        ":",
        "prediction_path",
        "}",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "evaluate_validation",
        ".",
        "set_upstream",
        "(",
        "evaluate_summary",
        ")",
        "return",
        "evaluate_prediction",
        ",",
        "evaluate_summary",
        ",",
        "evaluate_validation"
      ],
      "docstring": "Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)",
      "docstring_tokens": [
        "Creates",
        "Operators",
        "needed",
        "for",
        "model",
        "evaluation",
        "and",
        "returns",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/mlengine_operator_utils.py#L32-L246"
    },
    "target": "Creates Operators needed for model evaluation and returns .",
    "arguments": [
      [
        "def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : # Verify that task_prefix doesn't have any special characters except hyphen # '-', which is the only allowed non-alphanumeric character by Dataflow. if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise AirflowException ( \"`metric_fn` param must be callable.\" ) if not callable ( validate_fn ) : raise AirflowException ( \"`validate_fn` param must be callable.\" ) if dag is not None and dag . default_args is not None : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , * * kwargs ) : prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != \"gs\" or not bucket or not obj : raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) gcs_hook = GoogleCloudStorageHook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [docs] class E2EOperator ( BaseOperator ) : \"\"\"End-to-end data processing pipeline... _end_to_end_ex:.. note:: Available in Airflow 1.10.6 and later. DataflowFlexOperator can be used instead if custom setup with Dataflow worker ``DataflowFlexOperator`` is ``DataFlowFlexOperator``. Dataflow with a GCS location Dataflow for prediction. (``DataflowPredictionOperator`` is ``DataFlowPredictionOperator``.) E2EOperator takes the following parameters: ``location``: Location of the pipeline in the GCP project. ``gcs_location``: GCS location for temporary files,. Default is /tmp. ``dataflow_default_options``: Base options for all tasks executed in the. ``predictions_gcs_location``: GCS location where predictions are stored. ``input` (path on GCS from which input data will be read): ``output` (path on GCS where output will end up): `` prediction_input` (path on GCS from which input data will be read for prediction purposes): ``models` (list of which to select from): ``label_gcs_location` (GCS location where label data is stored):"
      ]
    ],
    "filtered_resps": [
      " [docs] class E2EOperator ( BaseOperator ) : \"\"\"End-to-end data processing pipeline... _end_to_end_ex:.. note:: Available in Airflow 1.10.6 and later. DataflowFlexOperator can be used instead if custom setup with Dataflow worker ``DataflowFlexOperator`` is ``DataFlowFlexOperator``. Dataflow with a GCS location Dataflow for prediction. (``DataflowPredictionOperator`` is ``DataFlowPredictionOperator``.) E2EOperator takes the following parameters: ``location``: Location of the pipeline in the GCP project. ``gcs_location``: GCS location for temporary files,. Default is /tmp. ``dataflow_default_options``: Base options for all tasks executed in the. ``predictions_gcs_location``: GCS location where predictions are stored. ``input` (path on GCS from which input data will be read): ``output` (path on GCS where output will end up): `` prediction_input` (path on GCS from which input data will be read for prediction purposes): ``models` (list of which to select from): ``label_gcs_location` (GCS location where label data is stored):"
    ],
    "smoothed_bleu_4": 0.515604687231702
  },
  {
    "doc_id": 67,
    "doc": {
      "id": 67,
      "repo": "apache/airflow",
      "path": "airflow/utils/file.py",
      "func_name": "mkdirs",
      "original_string": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
      "language": "python",
      "code": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
      "code_tokens": [
        "def",
        "mkdirs",
        "(",
        "path",
        ",",
        "mode",
        ")",
        ":",
        "try",
        ":",
        "o_umask",
        "=",
        "os",
        ".",
        "umask",
        "(",
        "0",
        ")",
        "os",
        ".",
        "makedirs",
        "(",
        "path",
        ",",
        "mode",
        ")",
        "except",
        "OSError",
        ":",
        "if",
        "not",
        "os",
        ".",
        "path",
        ".",
        "isdir",
        "(",
        "path",
        ")",
        ":",
        "raise",
        "finally",
        ":",
        "os",
        ".",
        "umask",
        "(",
        "o_umask",
        ")"
      ],
      "docstring": "Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int",
      "docstring_tokens": [
        "Creates",
        "the",
        "directory",
        "specified",
        "by",
        "path",
        "creating",
        "intermediate",
        "directories",
        "as",
        "necessary",
        ".",
        "If",
        "directory",
        "already",
        "exists",
        "this",
        "is",
        "a",
        "no",
        "-",
        "op",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/file.py#L42-L59"
    },
    "target": "Creates the directory specified by path creating intermediate directories as necessary . If directory already exists this is a no - op .",
    "arguments": [
      [
        "def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except OSError : if not os . path . isdir ( path ) : raise finally : os . umask ( o_umask )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def remove ( fn ): try : os. unlink ( fn ) except OSError : if os. path. isdir ( fn ): shutil. rmtree ( fn ) else : raise import os, shutil def rmtree ( fn ): for root, dirs, files in os. walk ( fn, topdown = False ): for file in files : os. unlink ( os. path. join ( root, file )) for dir in dirs : shutil. rmtree ( os. path. join ( root, dir )) os. rmdir ( root ) def symlink ( src, dst ): os. symlink ( src, dst ) def touch ( path ): with open ( path,'a'): pass def touchall ( basedir, pattern ): for f in glob. glob ( os. path. join ( basedir, pattern ) ): touch ( f ) def untar ( tarball, cwd = None ): if not cwd : cwd = os. curdir return tarfile. open ( tarball ). extractall ( cwd = cwd ) def uniquify ( root, name ): root, ext = os. path. splitext ( root ) i = 1 while os. path. exists ( os. path. join ( root, name"
      ]
    ],
    "filtered_resps": [
      " def remove ( fn ): try : os. unlink ( fn ) except OSError : if os. path. isdir ( fn ): shutil. rmtree ( fn ) else : raise import os, shutil def rmtree ( fn ): for root, dirs, files in os. walk ( fn, topdown = False ): for file in files : os. unlink ( os. path. join ( root, file )) for dir in dirs : shutil. rmtree ( os. path. join ( root, dir )) os. rmdir ( root ) def symlink ( src, dst ): os. symlink ( src, dst ) def touch ( path ): with open ( path,'a'): pass def touchall ( basedir, pattern ): for f in glob. glob ( os. path. join ( basedir, pattern ) ): touch ( f ) def untar ( tarball, cwd = None ): if not cwd : cwd = os. curdir return tarfile. open ( tarball ). extractall ( cwd = cwd ) def uniquify ( root, name ): root, ext = os. path. splitext ( root ) i = 1 while os. path. exists ( os. path. join ( root, name"
    ],
    "smoothed_bleu_4": 0.6381625014317357
  },
  {
    "doc_id": 68,
    "doc": {
      "id": 68,
      "repo": "apache/airflow",
      "path": "airflow/operators/check_operator.py",
      "func_name": "_convert_to_float_if_possible",
      "original_string": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str\n    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret",
      "language": "python",
      "code": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str\n    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret",
      "code_tokens": [
        "def",
        "_convert_to_float_if_possible",
        "(",
        "s",
        ")",
        ":",
        "try",
        ":",
        "ret",
        "=",
        "float",
        "(",
        "s",
        ")",
        "except",
        "(",
        "ValueError",
        ",",
        "TypeError",
        ")",
        ":",
        "ret",
        "=",
        "s",
        "return",
        "ret"
      ],
      "docstring": "A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str",
      "docstring_tokens": [
        "A",
        "small",
        "helper",
        "function",
        "to",
        "convert",
        "a",
        "string",
        "to",
        "a",
        "numeric",
        "value",
        "if",
        "appropriate"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/check_operator.py#L98-L110"
    },
    "target": "A small helper function to convert a string to a numeric value if appropriate",
    "arguments": [
      [
        "def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( ValueError , TypeError ) : ret = s return ret",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` You can use the function as follows: ``` py >>>_convert_to_float_if_possible(42) 42.0 >>>_convert_to_float_if_possible('10') 10.0 >>>_convert_to_float_if_possible('x') x ```\n    - 4.3.2.5.1.1\n    -\n        - Purpose:\n        - Given a set of time values (expressed as seconds since the epoch), return the sum of all values.\n        - Args:\n        -\n            - v ** time values given as a list, or single value as a float\n        - Returns: sum of time values\n    ``` python defsum_time_values(v): ret=0 for x in_v: try: if isinstance(x,basestring): x=float(x) except(ValueError,TypeError): x=0.0 ret+=x returnret ``` You can use the function as follows: ``` py >>>sum_time_values([10.5,20.5,'23']) 54.0 >>>sum_time_values(34) 34.0 >>>sum_time_values(['x']) 0.0 >>>sum_time_values('a') 0.0 >>>sum_time_values(['a','b','c'])"
      ]
    ],
    "filtered_resps": [
      " ``` You can use the function as follows: ``` py >>>_convert_to_float_if_possible(42) 42.0 >>>_convert_to_float_if_possible('10') 10.0 >>>_convert_to_float_if_possible('x') x ```\n    - 4.3.2.5.1.1\n    -\n        - Purpose:\n        - Given a set of time values (expressed as seconds since the epoch), return the sum of all values.\n        - Args:\n        -\n            - v ** time values given as a list, or single value as a float\n        - Returns: sum of time values\n    ``` python defsum_time_values(v): ret=0 for x in_v: try: if isinstance(x,basestring): x=float(x) except(ValueError,TypeError): x=0.0 ret+=x returnret ``` You can use the function as follows: ``` py >>>sum_time_values([10.5,20.5,'23']) 54.0 >>>sum_time_values(34) 34.0 >>>sum_time_values(['x']) 0.0 >>>sum_time_values('a') 0.0 >>>sum_time_values(['a','b','c'])"
    ],
    "smoothed_bleu_4": 0.5582778400076863
  },
  {
    "doc_id": 69,
    "doc": {
      "id": 69,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "make_aware",
      "original_string": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\n            \"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)",
      "language": "python",
      "code": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\n            \"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)",
      "code_tokens": [
        "def",
        "make_aware",
        "(",
        "value",
        ",",
        "timezone",
        "=",
        "None",
        ")",
        ":",
        "if",
        "timezone",
        "is",
        "None",
        ":",
        "timezone",
        "=",
        "TIMEZONE",
        "# Check that we won't overwrite the timezone of an aware datetime.",
        "if",
        "is_localized",
        "(",
        "value",
        ")",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"make_aware expects a naive datetime, got %s\"",
        "%",
        "value",
        ")",
        "if",
        "hasattr",
        "(",
        "value",
        ",",
        "'fold'",
        ")",
        ":",
        "# In case of python 3.6 we want to do the same that pendulum does for python3.5",
        "# i.e in case we move clock back we want to schedule the run at the time of the second",
        "# instance of the same clock time rather than the first one.",
        "# Fold parameter has no impact in other cases so we can safely set it to 1 here",
        "value",
        "=",
        "value",
        ".",
        "replace",
        "(",
        "fold",
        "=",
        "1",
        ")",
        "if",
        "hasattr",
        "(",
        "timezone",
        ",",
        "'localize'",
        ")",
        ":",
        "# This method is available for pytz time zones.",
        "return",
        "timezone",
        ".",
        "localize",
        "(",
        "value",
        ")",
        "elif",
        "hasattr",
        "(",
        "timezone",
        ",",
        "'convert'",
        ")",
        ":",
        "# For pendulum",
        "return",
        "timezone",
        ".",
        "convert",
        "(",
        "value",
        ")",
        "else",
        ":",
        "# This may be wrong around DST changes!",
        "return",
        "value",
        ".",
        "replace",
        "(",
        "tzinfo",
        "=",
        "timezone",
        ")"
      ],
      "docstring": "Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone",
      "docstring_tokens": [
        "Make",
        "a",
        "naive",
        "datetime",
        ".",
        "datetime",
        "in",
        "a",
        "given",
        "time",
        "zone",
        "aware",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L98-L128"
    },
    "target": "Make a naive datetime . datetime in a given time zone aware .",
    "arguments": [
      [
        "def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Check that we won't overwrite the timezone of an aware datetime. if is_localized ( value ) : raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) if hasattr ( value , 'fold' ) : # In case of python 3.6 we want to do the same that pendulum does for python3.5 # i.e in case we move clock back we want to schedule the run at the time of the second # instance of the same clock time rather than the first one. # Fold parameter has no impact in other cases so we can safely set it to 1 here value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : # This method is available for pytz time zones. return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : # For pendulum return timezone . convert ( value ) else : # This may be wrong around DST changes! return value . replace ( tzinfo = timezone )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def convert_timezone ( schedule, timezone ): \"\"\" Converts a schedule from one timezone to another ```  from pytz import UTC, timezone from atoolkit import schedule new_timezone = timezone('America/New_York') # 1) Convert a naive datetime naive = pytz.utc.localize(datetime.utcfromtimestamp(1456003400)). in_timezone(new_timezone) schedule.append(naive) # 2) Convert an aware datetime aware = timezone('US/Eastern').localize(datetime.datetime. fromtimestamp(1456003400)) schedule.append(convert_timezone(schedule, new_timezone)) ``` \"\"\" __schedules = defaultdict ( list ) __schedules [ timezone. utcoffset (). seconds ]. extend ( schedule ) # Add timezone into schedule for the new timezone __schedules [ timezone. utcoffset (). seconds ]. append ( timezone ) for i in [ x for x in sorted (__schedules.keys ())]: schedule = __schedules [ i ] # Fill out the missing values i = 0 j = 0 while i \u2260__schedule.length(): __schedules [ i ]. append ( __schedules [ j ]. last ()); j += 1; i += 1 def make_iterable ( value"
      ]
    ],
    "filtered_resps": [
      " def convert_timezone ( schedule, timezone ): \"\"\" Converts a schedule from one timezone to another ```  from pytz import UTC, timezone from atoolkit import schedule new_timezone = timezone('America/New_York') # 1) Convert a naive datetime naive = pytz.utc.localize(datetime.utcfromtimestamp(1456003400)). in_timezone(new_timezone) schedule.append(naive) # 2) Convert an aware datetime aware = timezone('US/Eastern').localize(datetime.datetime. fromtimestamp(1456003400)) schedule.append(convert_timezone(schedule, new_timezone)) ``` \"\"\" __schedules = defaultdict ( list ) __schedules [ timezone. utcoffset (). seconds ]. extend ( schedule ) # Add timezone into schedule for the new timezone __schedules [ timezone. utcoffset (). seconds ]. append ( timezone ) for i in [ x for x in sorted (__schedules.keys ())]: schedule = __schedules [ i ] # Fill out the missing values i = 0 j = 0 while i \u2260__schedule.length(): __schedules [ i ]. append ( __schedules [ j ]. last ()); j += 1; i += 1 def make_iterable ( value"
    ],
    "smoothed_bleu_4": 1.285455191338665
  },
  {
    "doc_id": 70,
    "doc": {
      "id": 70,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "make_naive",
      "original_string": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive",
      "language": "python",
      "code": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive",
      "code_tokens": [
        "def",
        "make_naive",
        "(",
        "value",
        ",",
        "timezone",
        "=",
        "None",
        ")",
        ":",
        "if",
        "timezone",
        "is",
        "None",
        ":",
        "timezone",
        "=",
        "TIMEZONE",
        "# Emulate the behavior of astimezone() on Python < 3.6.",
        "if",
        "is_naive",
        "(",
        "value",
        ")",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"make_naive() cannot be applied to a naive datetime\"",
        ")",
        "o",
        "=",
        "value",
        ".",
        "astimezone",
        "(",
        "timezone",
        ")",
        "# cross library compatibility",
        "naive",
        "=",
        "dt",
        ".",
        "datetime",
        "(",
        "o",
        ".",
        "year",
        ",",
        "o",
        ".",
        "month",
        ",",
        "o",
        ".",
        "day",
        ",",
        "o",
        ".",
        "hour",
        ",",
        "o",
        ".",
        "minute",
        ",",
        "o",
        ".",
        "second",
        ",",
        "o",
        ".",
        "microsecond",
        ")",
        "return",
        "naive"
      ],
      "docstring": "Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime",
      "docstring_tokens": [
        "Make",
        "an",
        "aware",
        "datetime",
        ".",
        "datetime",
        "naive",
        "in",
        "a",
        "given",
        "time",
        "zone",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L131-L157"
    },
    "target": "Make an aware datetime . datetime naive in a given time zone .",
    "arguments": [
      [
        "def make_naive ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Emulate the behavior of astimezone() on Python < 3.6. if is_naive ( value ) : raise ValueError ( \"make_naive() cannot be applied to a naive datetime\" ) o = value . astimezone ( timezone ) # cross library compatibility naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return naive",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\ndef is_naive ( value ) : return not hasattr ( value, '_tzinfo' ) and not isinstance ( value, tzinfo )\n\ndef make_aware ( naivedt, default = None, delta = None, timezone = None ) : \"\"\" Convert to local timezone using the given timezone naive dt obj '''' if delta is not None : naivedt = naivedt + delta if timezone is None : timezone = TIMESTAMPEP if default is None : if is_naive ( naivedt ) : timezone = pytz. UTC naivedt = pytz. timezone ( timezone ). normalize ( naivedt. astimezone ( timezone )) else : naivedt = pytz. timezone ( timezone ). localize ( naivedt ) return naivedt\n\ndef get_timezone () : return TIMEZONE\n\ndef make_timezone ( value ) : return pytz. timezone ( value )\n\ndef normalize ( dt, delta = None ) : \"\"\":param dt: a datetime.datetime object (or equivalent) '''' if delta is None : return dt + timedelta ( 0, 0, 0 ) return dt + delta\n\ndef _safediv ( a, b ) : \"\"\"Division to prevent overflow"
      ]
    ],
    "filtered_resps": [
      "\n\ndef is_naive ( value ) : return not hasattr ( value, '_tzinfo' ) and not isinstance ( value, tzinfo )\n\ndef make_aware ( naivedt, default = None, delta = None, timezone = None ) : \"\"\" Convert to local timezone using the given timezone naive dt obj '''' if delta is not None : naivedt = naivedt + delta if timezone is None : timezone = TIMESTAMPEP if default is None : if is_naive ( naivedt ) : timezone = pytz. UTC naivedt = pytz. timezone ( timezone ). normalize ( naivedt. astimezone ( timezone )) else : naivedt = pytz. timezone ( timezone ). localize ( naivedt ) return naivedt\n\ndef get_timezone () : return TIMEZONE\n\ndef make_timezone ( value ) : return pytz. timezone ( value )\n\ndef normalize ( dt, delta = None ) : \"\"\":param dt: a datetime.datetime object (or equivalent) '''' if delta is None : return dt + timedelta ( 0, 0, 0 ) return dt + delta\n\ndef _safediv ( a, b ) : \"\"\"Division to prevent overflow"
    ],
    "smoothed_bleu_4": 1.198154358857071
  },
  {
    "doc_id": 71,
    "doc": {
      "id": 71,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "datetime",
      "original_string": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)",
      "language": "python",
      "code": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)",
      "code_tokens": [
        "def",
        "datetime",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'tzinfo'",
        "not",
        "in",
        "kwargs",
        ":",
        "kwargs",
        "[",
        "'tzinfo'",
        "]",
        "=",
        "TIMEZONE",
        "return",
        "dt",
        ".",
        "datetime",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime",
      "docstring_tokens": [
        "Wrapper",
        "around",
        "datetime",
        ".",
        "datetime",
        "that",
        "adds",
        "settings",
        ".",
        "TIMEZONE",
        "if",
        "tzinfo",
        "not",
        "specified"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L160-L169"
    },
    "target": "Wrapper around datetime . datetime that adds settings . TIMEZONE if tzinfo not specified",
    "arguments": [
      [
        "def datetime ( * args , * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt . datetime ( * args , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def utctime ( ) : return datetime ( 0, 0, 0, tzinfo = TIMEZONE ) def utcnow ( ) : return datetime. now ( TIMEZONE ) def utctimetuple ( dt ) : return datetime. timedelta ( * dt. utctimetuple ( ) [ 1 : 5 ] ) def timetuple ( self ) : return self. utc ( ). strftime ( \" %a %d %b \" ) def date ( self ) : return datetime ( * self. timetuple ( ) [ : 3 ] ) def tick ( * args, * * kwargs ) : dt = self. datetime ( * args [ : 5 ], * * kwargs ) if len ( args ) > 5 : raise TypeError ( \"extra arguments\" ) dt = utctime ( ) + dt return self. fromtimestamp ( datetime ( dt ). timestamp ( ), tzinfo = TIMEZONE ) def fromtimestring ( timestring, * * args ) : timestring = timestring [ 1 : - 1 ] dt = datetime. strptime ( timestring, args. format. format ) return self ( dt ) def asctime ( self"
      ]
    ],
    "filtered_resps": [
      " def utctime ( ) : return datetime ( 0, 0, 0, tzinfo = TIMEZONE ) def utcnow ( ) : return datetime. now ( TIMEZONE ) def utctimetuple ( dt ) : return datetime. timedelta ( * dt. utctimetuple ( ) [ 1 : 5 ] ) def timetuple ( self ) : return self. utc ( ). strftime ( \" %a %d %b \" ) def date ( self ) : return datetime ( * self. timetuple ( ) [ : 3 ] ) def tick ( * args, * * kwargs ) : dt = self. datetime ( * args [ : 5 ], * * kwargs ) if len ( args ) > 5 : raise TypeError ( \"extra arguments\" ) dt = utctime ( ) + dt return self. fromtimestamp ( datetime ( dt ). timestamp ( ), tzinfo = TIMEZONE ) def fromtimestring ( timestring, * * args ) : timestring = timestring [ 1 : - 1 ] dt = datetime. strptime ( timestring, args. format. format ) return self ( dt ) def asctime ( self"
    ],
    "smoothed_bleu_4": 1.0329130507988409
  },
  {
    "doc_id": 72,
    "doc": {
      "id": 72,
      "repo": "apache/airflow",
      "path": "airflow/hooks/druid_hook.py",
      "func_name": "DruidDbApiHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.\n        \"\"\"\n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.\n        \"\"\"\n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "druid_broker_conn_id",
        ")",
        "druid_broker_conn",
        "=",
        "connect",
        "(",
        "host",
        "=",
        "conn",
        ".",
        "host",
        ",",
        "port",
        "=",
        "conn",
        ".",
        "port",
        ",",
        "path",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'endpoint'",
        ",",
        "'/druid/v2/sql'",
        ")",
        ",",
        "scheme",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'schema'",
        ",",
        "'http'",
        ")",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "'Get the connection to druid broker on %s'",
        ",",
        "conn",
        ".",
        "host",
        ")",
        "return",
        "druid_broker_conn"
      ],
      "docstring": "Establish a connection to druid broker.",
      "docstring_tokens": [
        "Establish",
        "a",
        "connection",
        "to",
        "druid",
        "broker",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/druid_hook.py#L127-L139"
    },
    "target": "Establish a connection to druid broker .",
    "arguments": [
      [
        "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # Return Connection object to be used by # executor ```\n\nComment: The `get_conn()` function is used to establish a persistent connection to the Druid broker. It retrieves a connection object from the database connections defined in the Airflow DAG, uses the `connect()` function to create a connection to the Druid broker, and returns the connection object.\n\nThe `connect()` function is used to create a database connection. It takes in several parameters, including the host, port, path, and schema. The `extra_dejson` attribute of the connection object is used to retrieve the endpoint and schema from the Druid broker connection.\n\nThe function then logs a message indicating that it has established a connection to the Druid broker on the specified host. Finally, it returns the connection object, which can be used to execute queries on the Druid broker. \n\nThis function is likely used as part of an larger Airflow DAG that interacts with a Druid database. \n\n## Step 3: Understand the execution of the function\n\nThe `get_conn()` function is executed on multiple occasions, as indicated by the `@retry()` decorator with `tries=3` and `delay=3`. This means that if the function fails to establish a connection to the Druid broker, it"
      ]
    ],
    "filtered_resps": [
      " # Return Connection object to be used by # executor ```\n\nComment: The `get_conn()` function is used to establish a persistent connection to the Druid broker. It retrieves a connection object from the database connections defined in the Airflow DAG, uses the `connect()` function to create a connection to the Druid broker, and returns the connection object.\n\nThe `connect()` function is used to create a database connection. It takes in several parameters, including the host, port, path, and schema. The `extra_dejson` attribute of the connection object is used to retrieve the endpoint and schema from the Druid broker connection.\n\nThe function then logs a message indicating that it has established a connection to the Druid broker on the specified host. Finally, it returns the connection object, which can be used to execute queries on the Druid broker. \n\nThis function is likely used as part of an larger Airflow DAG that interacts with a Druid database. \n\n## Step 3: Understand the execution of the function\n\nThe `get_conn()` function is executed on multiple occasions, as indicated by the `@retry()` decorator with `tries=3` and `delay=3`. This means that if the function fails to establish a connection to the Druid broker, it"
    ],
    "smoothed_bleu_4": 1.6578557121333901
  },
  {
    "doc_id": 73,
    "doc": {
      "id": 73,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.get_conn",
      "original_string": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session",
      "language": "python",
      "code": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ",",
        "headers",
        "=",
        "None",
        ")",
        ":",
        "session",
        "=",
        "requests",
        ".",
        "Session",
        "(",
        ")",
        "if",
        "self",
        ".",
        "http_conn_id",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "http_conn_id",
        ")",
        "if",
        "\"://\"",
        "in",
        "conn",
        ".",
        "host",
        ":",
        "self",
        ".",
        "base_url",
        "=",
        "conn",
        ".",
        "host",
        "else",
        ":",
        "# schema defaults to HTTP",
        "schema",
        "=",
        "conn",
        ".",
        "schema",
        "if",
        "conn",
        ".",
        "schema",
        "else",
        "\"http\"",
        "self",
        ".",
        "base_url",
        "=",
        "schema",
        "+",
        "\"://\"",
        "+",
        "conn",
        ".",
        "host",
        "if",
        "conn",
        ".",
        "port",
        ":",
        "self",
        ".",
        "base_url",
        "=",
        "self",
        ".",
        "base_url",
        "+",
        "\":\"",
        "+",
        "str",
        "(",
        "conn",
        ".",
        "port",
        ")",
        "if",
        "conn",
        ".",
        "login",
        ":",
        "session",
        ".",
        "auth",
        "=",
        "(",
        "conn",
        ".",
        "login",
        ",",
        "conn",
        ".",
        "password",
        ")",
        "if",
        "conn",
        ".",
        "extra",
        ":",
        "try",
        ":",
        "session",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "conn",
        ".",
        "extra_dejson",
        ")",
        "except",
        "TypeError",
        ":",
        "self",
        ".",
        "log",
        ".",
        "warn",
        "(",
        "'Connection to %s has invalid extra field.'",
        ",",
        "conn",
        ".",
        "host",
        ")",
        "if",
        "headers",
        ":",
        "session",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "headers",
        ")",
        "return",
        "session"
      ],
      "docstring": "Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict",
      "docstring_tokens": [
        "Returns",
        "http",
        "session",
        "for",
        "use",
        "with",
        "requests"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L53-L83"
    },
    "target": "Returns http session for use with requests",
    "arguments": [
      [
        "def get_conn ( self , headers = None ) : session = requests . Session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if \"://\" in conn . host : self . base_url = conn . host else : # schema defaults to HTTP schema = conn . schema if conn . schema else \"http\" self . base_url = schema + \"://\" + conn . host if conn . port : self . base_url = self . base_url + \":\" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " else : return None  # If the connection does not exist, raise an exception return self. get_connection ( self. http_conn_id ) [ \"uri\" ]  # You can't use requests.Session. You can use the requests library directly def request ( self, path, method = \"GET\", headers = None, data = None, timeout = None, return_response = False ) : method = method. upper ( ) if method == \"GET\" : conn = self. get_connection ( self. http_conn_id )[ \"uri\" ] if data : conn += \"?\" + urllib.parse. urlencode ( data ) response = requests. get ( conn + path ) else : response = requests. request ( method, conn + path, headers = headers, data = data ) self. xcom_push ('request-method', method ) self. xcom_push ( response. status_code,'response-status-code' ) self. xcom_push ( response. headers,'response-headers' ) if return_response : return response else : return response. text  # This is a custom module. We need to import it. # NOTE: The @task decorator should not be used. from dataflow_engine. import utils from common"
      ]
    ],
    "filtered_resps": [
      " else : return None  # If the connection does not exist, raise an exception return self. get_connection ( self. http_conn_id ) [ \"uri\" ]  # You can't use requests.Session. You can use the requests library directly def request ( self, path, method = \"GET\", headers = None, data = None, timeout = None, return_response = False ) : method = method. upper ( ) if method == \"GET\" : conn = self. get_connection ( self. http_conn_id )[ \"uri\" ] if data : conn += \"?\" + urllib.parse. urlencode ( data ) response = requests. get ( conn + path ) else : response = requests. request ( method, conn + path, headers = headers, data = data ) self. xcom_push ('request-method', method ) self. xcom_push ( response. status_code,'response-status-code' ) self. xcom_push ( response. headers,'response-headers' ) if return_response : return response else : return response. text  # This is a custom module. We need to import it. # NOTE: The @task decorator should not be used. from dataflow_engine. import utils from common"
    ],
    "smoothed_bleu_4": 0.5311626017533897
  },
  {
    "doc_id": 74,
    "doc": {
      "id": 74,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.run",
      "original_string": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)",
      "language": "python",
      "code": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)",
      "code_tokens": [
        "def",
        "run",
        "(",
        "self",
        ",",
        "endpoint",
        ",",
        "data",
        "=",
        "None",
        ",",
        "headers",
        "=",
        "None",
        ",",
        "extra_options",
        "=",
        "None",
        ")",
        ":",
        "extra_options",
        "=",
        "extra_options",
        "or",
        "{",
        "}",
        "session",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        "headers",
        ")",
        "if",
        "self",
        ".",
        "base_url",
        "and",
        "not",
        "self",
        ".",
        "base_url",
        ".",
        "endswith",
        "(",
        "'/'",
        ")",
        "and",
        "endpoint",
        "and",
        "not",
        "endpoint",
        ".",
        "startswith",
        "(",
        "'/'",
        ")",
        ":",
        "url",
        "=",
        "self",
        ".",
        "base_url",
        "+",
        "'/'",
        "+",
        "endpoint",
        "else",
        ":",
        "url",
        "=",
        "(",
        "self",
        ".",
        "base_url",
        "or",
        "''",
        ")",
        "+",
        "(",
        "endpoint",
        "or",
        "''",
        ")",
        "req",
        "=",
        "None",
        "if",
        "self",
        ".",
        "method",
        "==",
        "'GET'",
        ":",
        "# GET uses params",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "params",
        "=",
        "data",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "elif",
        "self",
        ".",
        "method",
        "==",
        "'HEAD'",
        ":",
        "# HEAD doesn't use params",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "else",
        ":",
        "# Others use data",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "data",
        "=",
        "data",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "prepped_request",
        "=",
        "session",
        ".",
        "prepare_request",
        "(",
        "req",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Sending '%s' to url: %s\"",
        ",",
        "self",
        ".",
        "method",
        ",",
        "url",
        ")",
        "return",
        "self",
        ".",
        "run_and_check",
        "(",
        "session",
        ",",
        "prepped_request",
        ",",
        "extra_options",
        ")"
      ],
      "docstring": "Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict",
      "docstring_tokens": [
        "Performs",
        "the",
        "request"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L85-L131"
    },
    "target": "Performs the request",
    "arguments": [
      [
        "def run ( self , endpoint , data = None , headers = None , extra_options = None ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = None if self . method == 'GET' : # GET uses params req = requests . Request ( self . method , url , params = data , headers = headers ) elif self . method == 'HEAD' : # HEAD doesn't use params req = requests . Request ( self . method , url , headers = headers ) else : # Others use data req = requests . Request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( \"Sending '%s' to url: %s\" , self . method , url ) return self . run_and_check ( session , prepped_request , extra_options )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_conn ( self, headers = None ) : \"connect to the remote server and return a session object which is expected to have a close method\" conn = requests. Session ( ) headers = headers or { } for k, v in headers. items ( ) : conn. headers [ k ] = v conn. verify = self. verify conn. auth = self. auth return conn def run_and_check ( self, conn, prepped_request, extra_options ) : try : response = conn. send ( prepped_request, ** extra_options ) # TODO - check if response is OK if not ( 200 = = response. status_code < 300 ) : # status code is used for the timeout exceptions except requests. ConnectionError as e : # raise : self. exception_class ( 'connection_error' )( e ) except requests. Timeout as e : # the other exception should have been caught first res = self. exception_class ( 'timeout' )( e ) ( self. exception_class ('request_error' ) (\n          - \"Timeout while calling %s %s\", self. method, url ) )( e )\n\n    - def run_and_check ( self, session, prepped_request, extra_options ) :\n       "
      ]
    ],
    "filtered_resps": [
      " def get_conn ( self, headers = None ) : \"connect to the remote server and return a session object which is expected to have a close method\" conn = requests. Session ( ) headers = headers or { } for k, v in headers. items ( ) : conn. headers [ k ] = v conn. verify = self. verify conn. auth = self. auth return conn def run_and_check ( self, conn, prepped_request, extra_options ) : try : response = conn. send ( prepped_request, ** extra_options ) # TODO - check if response is OK if not ( 200 = = response. status_code < 300 ) : # status code is used for the timeout exceptions except requests. ConnectionError as e : # raise : self. exception_class ( 'connection_error' )( e ) except requests. Timeout as e : # the other exception should have been caught first res = self. exception_class ( 'timeout' )( e ) ( self. exception_class ('request_error' ) (\n          - \"Timeout while calling %s %s\", self. method, url ) )( e )\n\n    - def run_and_check ( self, session, prepped_request, extra_options ) :\n       "
    ],
    "smoothed_bleu_4": 0.45520119072873444
  },
  {
    "doc_id": 75,
    "doc": {
      "id": 75,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.check_response",
      "original_string": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)",
      "language": "python",
      "code": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)",
      "code_tokens": [
        "def",
        "check_response",
        "(",
        "self",
        ",",
        "response",
        ")",
        ":",
        "try",
        ":",
        "response",
        ".",
        "raise_for_status",
        "(",
        ")",
        "except",
        "requests",
        ".",
        "exceptions",
        ".",
        "HTTPError",
        ":",
        "self",
        ".",
        "log",
        ".",
        "error",
        "(",
        "\"HTTP error: %s\"",
        ",",
        "response",
        ".",
        "reason",
        ")",
        "if",
        "self",
        ".",
        "method",
        "not",
        "in",
        "[",
        "'GET'",
        ",",
        "'HEAD'",
        "]",
        ":",
        "self",
        ".",
        "log",
        ".",
        "error",
        "(",
        "response",
        ".",
        "text",
        ")",
        "raise",
        "AirflowException",
        "(",
        "str",
        "(",
        "response",
        ".",
        "status_code",
        ")",
        "+",
        "\":\"",
        "+",
        "response",
        ".",
        "reason",
        ")"
      ],
      "docstring": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response",
      "docstring_tokens": [
        "Checks",
        "the",
        "status",
        "code",
        "and",
        "raise",
        "an",
        "AirflowException",
        "exception",
        "on",
        "non",
        "2XX",
        "or",
        "3XX",
        "status",
        "codes"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L133-L147"
    },
    "target": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes",
    "arguments": [
      [
        "def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . HTTPError : self . log . error ( \"HTTP error: %s\" , response . reason ) if self . method not in [ 'GET' , 'HEAD' ] : self . log . error ( response . text ) raise AirflowException ( str ( response . status_code ) + \":\" + response . reason )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except requests. exceptions. ConnectionError : self. log. error ( \"Connection error:\" ) raise AirflowException ( str ( response. status_code ) + \":\" + str ( requests. exceptions. ConnectionError ))\n```\n\nWe can improve this method by adding some additional error handling and logging to provide more context in case of any unexpected errors. Here's a revised version of the `check_response` method:\n\n```python\ndef check_response ( self, response ) : try : response. raise_for_status ( ) except requests. exceptions. HTTPError as e : if e. response. status_code == 401 : self. log. error ( \"Authorization failed. Please check your credentials.\" ) else : self. log. error ( \"HTTP error: %s\", e. reason ) except requests. exceptions. ConnectionError as e : self. log. error ( \"Connection error: %s\", str ( e )) except requests. exceptions. Timeout as e : self. log. error ( \"Request timed out: %s\", str ( e )) except requests. exceptions. ConnectionError : self. log. error ( \"Connection error: Unable to connect to the server.\" ) raise AirflowException ( str ( response"
      ]
    ],
    "filtered_resps": [
      " except requests. exceptions. ConnectionError : self. log. error ( \"Connection error:\" ) raise AirflowException ( str ( response. status_code ) + \":\" + str ( requests. exceptions. ConnectionError ))\n```\n\nWe can improve this method by adding some additional error handling and logging to provide more context in case of any unexpected errors. Here's a revised version of the `check_response` method:\n\n```python\ndef check_response ( self, response ) : try : response. raise_for_status ( ) except requests. exceptions. HTTPError as e : if e. response. status_code == 401 : self. log. error ( \"Authorization failed. Please check your credentials.\" ) else : self. log. error ( \"HTTP error: %s\", e. reason ) except requests. exceptions. ConnectionError as e : self. log. error ( \"Connection error: %s\", str ( e )) except requests. exceptions. Timeout as e : self. log. error ( \"Request timed out: %s\", str ( e )) except requests. exceptions. ConnectionError : self. log. error ( \"Connection error: Unable to connect to the server.\" ) raise AirflowException ( str ( response"
    ],
    "smoothed_bleu_4": 0.6322973837787925
  },
  {
    "doc_id": 76,
    "doc": {
      "id": 76,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.run_and_check",
      "original_string": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex",
      "language": "python",
      "code": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex",
      "code_tokens": [
        "def",
        "run_and_check",
        "(",
        "self",
        ",",
        "session",
        ",",
        "prepped_request",
        ",",
        "extra_options",
        ")",
        ":",
        "extra_options",
        "=",
        "extra_options",
        "or",
        "{",
        "}",
        "try",
        ":",
        "response",
        "=",
        "session",
        ".",
        "send",
        "(",
        "prepped_request",
        ",",
        "stream",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"stream\"",
        ",",
        "False",
        ")",
        ",",
        "verify",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"verify\"",
        ",",
        "True",
        ")",
        ",",
        "proxies",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"proxies\"",
        ",",
        "{",
        "}",
        ")",
        ",",
        "cert",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"cert\"",
        ")",
        ",",
        "timeout",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"timeout\"",
        ")",
        ",",
        "allow_redirects",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"allow_redirects\"",
        ",",
        "True",
        ")",
        ")",
        "if",
        "extra_options",
        ".",
        "get",
        "(",
        "'check_response'",
        ",",
        "True",
        ")",
        ":",
        "self",
        ".",
        "check_response",
        "(",
        "response",
        ")",
        "return",
        "response",
        "except",
        "requests",
        ".",
        "exceptions",
        ".",
        "ConnectionError",
        "as",
        "ex",
        ":",
        "self",
        ".",
        "log",
        ".",
        "warn",
        "(",
        "str",
        "(",
        "ex",
        ")",
        "+",
        "' Tenacity will retry to execute the operation'",
        ")",
        "raise",
        "ex"
      ],
      "docstring": "Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict",
      "docstring_tokens": [
        "Grabs",
        "extra",
        "options",
        "like",
        "timeout",
        "and",
        "actually",
        "runs",
        "the",
        "request",
        "checking",
        "for",
        "the",
        "result"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181"
    },
    "target": "Grabs extra options like timeout and actually runs the request checking for the result",
    "arguments": [
      [
        "def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( \"stream\" , False ) , verify = extra_options . get ( \"verify\" , True ) , proxies = extra_options . get ( \"proxies\" , { } ) , cert = extra_options . get ( \"cert\" ) , timeout = extra_options . get ( \"timeout\" ) , allow_redirects = extra_options . get ( \"allow_redirects\" , True ) ) if extra_options . get ( 'check_response' , True ) : self . check_response ( response ) return response except requests . exceptions . ConnectionError as ex : self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) raise ex",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " from requests. exceptions import SSLError, Timeout, ConnectionError def install_hook ( self, hook_manager, ** extras ) : \"\"\"Install a Tenacity retry hook. The hook can be an object with the methods'should_retry' and 'wait' or a dictionary.\n        - A dictionary is expected to have the keys'should_retry' and \u2018wait\u2019 and its values are functions.\n\n        :param hook_manager: An object which manages our hook.\n        :param extras: Keyword arguments to be passed to \u201cTenacity\u201d instances which manage retries.\n        :return: A function which executes the actual call with retrying.\n        \u201d\u201d\u201d self. _hook = hook_manager if isinstance ( self. _hook, object ) else { \"should_retry\" : self. _hook ['should_retry' ], \"wait\" : self. _hook [ \"wait\" ] } def reraise_last_runtime_error ( self ) : \"\"\" Raises the last RuntimeError exception that was thrown. :rtype: RuntimeError \"\"\" try : if hasattr ( self, \"last_runtime_error\" ) : raise self. last_runtime_error finally : self. last_runtime_error = None [docs] def check_response ( self, response, ** kwargs ) : \"\"\""
      ]
    ],
    "filtered_resps": [
      " from requests. exceptions import SSLError, Timeout, ConnectionError def install_hook ( self, hook_manager, ** extras ) : \"\"\"Install a Tenacity retry hook. The hook can be an object with the methods'should_retry' and 'wait' or a dictionary.\n        - A dictionary is expected to have the keys'should_retry' and \u2018wait\u2019 and its values are functions.\n\n        :param hook_manager: An object which manages our hook.\n        :param extras: Keyword arguments to be passed to \u201cTenacity\u201d instances which manage retries.\n        :return: A function which executes the actual call with retrying.\n        \u201d\u201d\u201d self. _hook = hook_manager if isinstance ( self. _hook, object ) else { \"should_retry\" : self. _hook ['should_retry' ], \"wait\" : self. _hook [ \"wait\" ] } def reraise_last_runtime_error ( self ) : \"\"\" Raises the last RuntimeError exception that was thrown. :rtype: RuntimeError \"\"\" try : if hasattr ( self, \"last_runtime_error\" ) : raise self. last_runtime_error finally : self. last_runtime_error = None [docs] def check_response ( self, response, ** kwargs ) : \"\"\""
    ],
    "smoothed_bleu_4": 0.533165119341805
  },
  {
    "doc_id": 77,
    "doc": {
      "id": 77,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "create_session",
      "original_string": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
      "language": "python",
      "code": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
      "code_tokens": [
        "def",
        "create_session",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "try",
        ":",
        "yield",
        "session",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "except",
        "Exception",
        ":",
        "session",
        ".",
        "rollback",
        "(",
        ")",
        "raise",
        "finally",
        ":",
        "session",
        ".",
        "close",
        "(",
        ")"
      ],
      "docstring": "Contextmanager that will create and teardown a session.",
      "docstring_tokens": [
        "Contextmanager",
        "that",
        "will",
        "create",
        "and",
        "teardown",
        "a",
        "session",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L32-L44"
    },
    "target": "Contextmanager that will create and teardown a session .",
    "arguments": [
      [
        "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` But as in the previous example, this is redundant, since we can just use a with statement with the session object directly. ``` with settings. Session ( ) as session : try : yield session session. commit ( ) except Exception : session. rollback ( ) raise ```\n  - In the example above, session is accessed after it is returned by the create_session() generator. This will produce an AttributeError. It should be accessed only while it is still in scope. ``` try : session = create_session ( ). next ( ) s1 = session. query ( SomethingNew ). all ( ) except StopIteration : assert False ``` Here \u201csession\u201d is accessed after the scope ends, thus producing an error.\n  - Generators, in general, are designed to provide an iterator for its contents as they are computed rather than loading the entire contents into memory. The above examples illustrate well why the create_session() generator is a bad approach for creating a session, and we should only define it like so when we have to use its iterator semantics, which is rare. The above code samples are also terrible in terms of exception handling: a generator will just stop executing if an exception occurs within it. Thus, we would expect something like the following"
      ]
    ],
    "filtered_resps": [
      " ``` But as in the previous example, this is redundant, since we can just use a with statement with the session object directly. ``` with settings. Session ( ) as session : try : yield session session. commit ( ) except Exception : session. rollback ( ) raise ```\n  - In the example above, session is accessed after it is returned by the create_session() generator. This will produce an AttributeError. It should be accessed only while it is still in scope. ``` try : session = create_session ( ). next ( ) s1 = session. query ( SomethingNew ). all ( ) except StopIteration : assert False ``` Here \u201csession\u201d is accessed after the scope ends, thus producing an error.\n  - Generators, in general, are designed to provide an iterator for its contents as they are computed rather than loading the entire contents into memory. The above examples illustrate well why the create_session() generator is a bad approach for creating a session, and we should only define it like so when we have to use its iterator semantics, which is rare. The above code samples are also terrible in terms of exception handling: a generator will just stop executing if an exception occurs within it. Thus, we would expect something like the following"
    ],
    "smoothed_bleu_4": 0.7975906182793249
  },
  {
    "doc_id": 78,
    "doc": {
      "id": 78,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "provide_session",
      "original_string": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper",
      "language": "python",
      "code": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper",
      "code_tokens": [
        "def",
        "provide_session",
        "(",
        "func",
        ")",
        ":",
        "@",
        "wraps",
        "(",
        "func",
        ")",
        "def",
        "wrapper",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "arg_session",
        "=",
        "'session'",
        "func_params",
        "=",
        "func",
        ".",
        "__code__",
        ".",
        "co_varnames",
        "session_in_args",
        "=",
        "arg_session",
        "in",
        "func_params",
        "and",
        "func_params",
        ".",
        "index",
        "(",
        "arg_session",
        ")",
        "<",
        "len",
        "(",
        "args",
        ")",
        "session_in_kwargs",
        "=",
        "arg_session",
        "in",
        "kwargs",
        "if",
        "session_in_kwargs",
        "or",
        "session_in_args",
        ":",
        "return",
        "func",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "else",
        ":",
        "with",
        "create_session",
        "(",
        ")",
        "as",
        "session",
        ":",
        "kwargs",
        "[",
        "arg_session",
        "]",
        "=",
        "session",
        "return",
        "func",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "return",
        "wrapper"
      ],
      "docstring": "Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.",
      "docstring_tokens": [
        "Function",
        "decorator",
        "that",
        "provides",
        "a",
        "session",
        "if",
        "it",
        "isn",
        "t",
        "provided",
        ".",
        "If",
        "you",
        "want",
        "to",
        "reuse",
        "a",
        "session",
        "or",
        "run",
        "the",
        "function",
        "as",
        "part",
        "of",
        "a",
        "database",
        "transaction",
        "you",
        "pass",
        "it",
        "to",
        "the",
        "function",
        "if",
        "not",
        "this",
        "wrapper",
        "will",
        "create",
        "one",
        "and",
        "close",
        "it",
        "for",
        "you",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L47-L70"
    },
    "target": "Function decorator that provides a session if it isn t provided . If you want to reuse a session or run the function as part of a database transaction you pass it to the function if not this wrapper will create one and close it for you .",
    "arguments": [
      [
        "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , * * kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , * * kwargs ) return wrapper",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\n\nThis documentation is generated from the code sample and provides a detailed explanation of the `provide_session` decorator's functionality.\n\n**Notes**\n\n* The `create_session` function is not defined in the code sample and is assumed to be a function that creates a session object.\n* The `session_in_args` and `session_in_kwargs` variables are used to check if the `session` argument is present in the function's arguments or keyword arguments, respectively.\n* The `wrapper` function calls the wrapped function with the created session object as an argument if `session` is not already present in the arguments or keyword arguments.\n* The `wrapper` function returns the result of the wrapped function.\n* If `session` is not present in the arguments or keyword arguments and the `create_session` function fails to create a session object, the code will raise an exception.  For a real-world implementation, you'd want to handle this case more robustly. \n\nTo review this, go through the code and ensure it is properly aligned with the specification. If you have any concerns or questions, feel free to ask.<|eot_id|>"
      ]
    ],
    "filtered_resps": [
      "\n```\n\nThis documentation is generated from the code sample and provides a detailed explanation of the `provide_session` decorator's functionality.\n\n**Notes**\n\n* The `create_session` function is not defined in the code sample and is assumed to be a function that creates a session object.\n* The `session_in_args` and `session_in_kwargs` variables are used to check if the `session` argument is present in the function's arguments or keyword arguments, respectively.\n* The `wrapper` function calls the wrapped function with the created session object as an argument if `session` is not already present in the arguments or keyword arguments.\n* The `wrapper` function returns the result of the wrapped function.\n* If `session` is not present in the arguments or keyword arguments and the `create_session` function fails to create a session object, the code will raise an exception.  For a real-world implementation, you'd want to handle this case more robustly. \n\nTo review this, go through the code and ensure it is properly aligned with the specification. If you have any concerns or questions, feel free to ask.<|eot_id|>"
    ],
    "smoothed_bleu_4": 2.0018658495189254
  },
  {
    "doc_id": 79,
    "doc": {
      "id": 79,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "resetdb",
      "original_string": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
      "language": "python",
      "code": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
      "code_tokens": [
        "def",
        "resetdb",
        "(",
        ")",
        ":",
        "from",
        "airflow",
        "import",
        "models",
        "# alembic adds significant import time, so we import it lazily",
        "from",
        "alembic",
        ".",
        "migration",
        "import",
        "MigrationContext",
        "log",
        ".",
        "info",
        "(",
        "\"Dropping tables that exist\"",
        ")",
        "models",
        ".",
        "base",
        ".",
        "Base",
        ".",
        "metadata",
        ".",
        "drop_all",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "mc",
        "=",
        "MigrationContext",
        ".",
        "configure",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "if",
        "mc",
        ".",
        "_version",
        ".",
        "exists",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        ":",
        "mc",
        ".",
        "_version",
        ".",
        "drop",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "from",
        "flask_appbuilder",
        ".",
        "models",
        ".",
        "sqla",
        "import",
        "Base",
        "Base",
        ".",
        "metadata",
        ".",
        "drop_all",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "initdb",
        "(",
        ")"
      ],
      "docstring": "Clear out the database",
      "docstring_tokens": [
        "Clear",
        "out",
        "the",
        "database"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L312-L331"
    },
    "target": "Clear out the database",
    "arguments": [
      [
        "def resetdb ( ) : from airflow import models # alembic adds significant import time, so we import it lazily from alembic . migration import MigrationContext log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def initdb ( ) : from airflow import models from flask_appbuilder. models. sqla import Base from alembic. migration import MigrationContext from sqlalchemy. exc import IntegrityError log. info ( \"Creating tables that exist\" ) Base. metadata. create_all ( settings. engine ) mc = MigrationContext. configure ( settings. engine ) mc. _version. create ( settings. engine ) models. base. Base. metadata. create_all ( settings. engine ) # create all databases table for extra database log. info ( \"Initializing db\" ) from airflow. db. db import db try : db. create_all ( ) except IntegrityError : log. warning ( \"Error while initializing the database. Most likely the database is not properly set up. Please refer to the official sqlite documentation for correct initialization: https://sqlite.org/foreignkeys.html\" ) settings. flask_appbuilder. init_app ( app = settings. flask_app ) from flask_appbuilder. models. sqla import Base Base. metadata. create_all ( settings. engine ) return\ndef initdb ( ) :... ``` ``` from flask_appbuilder. basepkg import BaseView @app. route ( '/init_db', methods = [ \"POST\""
      ]
    ],
    "filtered_resps": [
      " def initdb ( ) : from airflow import models from flask_appbuilder. models. sqla import Base from alembic. migration import MigrationContext from sqlalchemy. exc import IntegrityError log. info ( \"Creating tables that exist\" ) Base. metadata. create_all ( settings. engine ) mc = MigrationContext. configure ( settings. engine ) mc. _version. create ( settings. engine ) models. base. Base. metadata. create_all ( settings. engine ) # create all databases table for extra database log. info ( \"Initializing db\" ) from airflow. db. db import db try : db. create_all ( ) except IntegrityError : log. warning ( \"Error while initializing the database. Most likely the database is not properly set up. Please refer to the official sqlite documentation for correct initialization: https://sqlite.org/foreignkeys.html\" ) settings. flask_appbuilder. init_app ( app = settings. flask_app ) from flask_appbuilder. models. sqla import Base Base. metadata. create_all ( settings. engine ) return\ndef initdb ( ) :... ``` ``` from flask_appbuilder. basepkg import BaseView @app. route ( '/init_db', methods = [ \"POST\""
    ],
    "smoothed_bleu_4": 0.5584288846755676
  },
  {
    "doc_id": 80,
    "doc": {
      "id": 80,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook._get_pretty_exception_message",
      "original_string": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)",
      "language": "python",
      "code": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)",
      "code_tokens": [
        "def",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ":",
        "if",
        "(",
        "hasattr",
        "(",
        "e",
        ",",
        "'message'",
        ")",
        "and",
        "'errorName'",
        "in",
        "e",
        ".",
        "message",
        "and",
        "'message'",
        "in",
        "e",
        ".",
        "message",
        ")",
        ":",
        "return",
        "(",
        "'{name}: {message}'",
        ".",
        "format",
        "(",
        "name",
        "=",
        "e",
        ".",
        "message",
        "[",
        "'errorName'",
        "]",
        ",",
        "message",
        "=",
        "e",
        ".",
        "message",
        "[",
        "'message'",
        "]",
        ")",
        ")",
        "else",
        ":",
        "return",
        "str",
        "(",
        "e",
        ")"
      ],
      "docstring": "Parses some DatabaseError to provide a better error message",
      "docstring_tokens": [
        "Parses",
        "some",
        "DatabaseError",
        "to",
        "provide",
        "a",
        "better",
        "error",
        "message"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L67-L78"
    },
    "target": "Parses some DatabaseError to provide a better error message",
    "arguments": [
      [
        "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "[ 0 : 75 ] # Cut off at 75 characters to help with very long error messages def _convert_exception_to_error ( e [, showFullException\u00a0= False ] ) : if ( isinstance ( e, Exception ) or isinstance ( e, Error ) ) :\n\u21e6\n\nNow, let\u2019s consider an example that uses this defined function `convert_exception_to_error()` to convert an exception to an error object:\n```\n# Example usage\ntry :\n    # Simulate an exception\n    raise MyException ( \"Test Error\" ):\nexcept MyException as e :\n    # Convert the exception to an error object\n    error_obj = convert_exception_to_error ( e )\n    # Raise the error object as if it was the original error\n    raise CustomError ( error_obj )\n```\nIn this example, the `convert_exception_to_error()` function takes the exception object `e` and returns an error object that is raised by the `CustomError()` constructor.\n\n### Step 3: Handle the error object\nThe final step is to handle the raised error object using a try-except block:\n```\ntry :\n    # Code that might raise an exception and be converted to an error object\n    raise MyException ("
      ]
    ],
    "filtered_resps": [
      "[ 0 : 75 ] # Cut off at 75 characters to help with very long error messages def _convert_exception_to_error ( e [, showFullException\u00a0= False ] ) : if ( isinstance ( e, Exception ) or isinstance ( e, Error ) ) :\n\u21e6\n\nNow, let\u2019s consider an example that uses this defined function `convert_exception_to_error()` to convert an exception to an error object:\n```\n# Example usage\ntry :\n    # Simulate an exception\n    raise MyException ( \"Test Error\" ):\nexcept MyException as e :\n    # Convert the exception to an error object\n    error_obj = convert_exception_to_error ( e )\n    # Raise the error object as if it was the original error\n    raise CustomError ( error_obj )\n```\nIn this example, the `convert_exception_to_error()` function takes the exception object `e` and returns an error object that is raised by the `CustomError()` constructor.\n\n### Step 3: Handle the error object\nThe final step is to handle the raised error object using a try-except block:\n```\ntry :\n    # Code that might raise an exception and be converted to an error object\n    raise MyException ("
    ],
    "smoothed_bleu_4": 0.5322877336795901
  },
  {
    "doc_id": 81,
    "doc": {
      "id": 81,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.get_records",
      "original_string": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
      "language": "python",
      "code": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
      "code_tokens": [
        "def",
        "get_records",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "try",
        ":",
        "return",
        "super",
        "(",
        ")",
        ".",
        "get_records",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")",
        "except",
        "DatabaseError",
        "as",
        "e",
        ":",
        "raise",
        "PrestoException",
        "(",
        "self",
        ".",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ")"
      ],
      "docstring": "Get a set of records from Presto",
      "docstring_tokens": [
        "Get",
        "a",
        "set",
        "of",
        "records",
        "from",
        "Presto"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L80-L88"
    },
    "target": "Get a set of records from Presto",
    "arguments": [
      [
        "def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " from e pass def execute ( self, hql, parameters = None ) : try : self. cursor. execute ( self. _strip_sql ( hql ), parameters ) return except DatabaseError as e : if e. args [ 1 ][ 0 ] == 'HY000' and isinstance ( e. args [ 1 ][ 1 ], bytearray ) : raise PrestoException ( self. _get_pretty_exception_message ( e ) ) raise PrestoException ( f 'Database execution failed with code {e.return_code}. Returned'f '{len ( e. args [ 1 ][ 1 ] )} bytes\\n' f '{e.args[1][1].decode ( \"utf-8\", errors = \"replace\" )}') from e def is_broken ( self ) : if ( hasattr ( self, '_has_broken_cursor' ) and self._has_broken_cursor ): return True return False def _has_broken_cursor ( self ) : try : self. _check_cursor_status () except DatabaseError : return True return False def _get_active_cursor ( self ) : self. cursor = Session. _get_cursor ( self. conn_wrapper, timeout = 1000 ) def _check_cursor"
      ]
    ],
    "filtered_resps": [
      " from e pass def execute ( self, hql, parameters = None ) : try : self. cursor. execute ( self. _strip_sql ( hql ), parameters ) return except DatabaseError as e : if e. args [ 1 ][ 0 ] == 'HY000' and isinstance ( e. args [ 1 ][ 1 ], bytearray ) : raise PrestoException ( self. _get_pretty_exception_message ( e ) ) raise PrestoException ( f 'Database execution failed with code {e.return_code}. Returned'f '{len ( e. args [ 1 ][ 1 ] )} bytes\\n' f '{e.args[1][1].decode ( \"utf-8\", errors = \"replace\" )}') from e def is_broken ( self ) : if ( hasattr ( self, '_has_broken_cursor' ) and self._has_broken_cursor ): return True return False def _has_broken_cursor ( self ) : try : self. _check_cursor_status () except DatabaseError : return True return False def _get_active_cursor ( self ) : self. cursor = Session. _get_cursor ( self. conn_wrapper, timeout = 1000 ) def _check_cursor"
    ],
    "smoothed_bleu_4": 0.44833663759284287
  },
  {
    "doc_id": 82,
    "doc": {
      "id": 82,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.get_pandas_df",
      "original_string": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df",
      "language": "python",
      "code": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df",
      "code_tokens": [
        "def",
        "get_pandas_df",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "import",
        "pandas",
        "cursor",
        "=",
        "self",
        ".",
        "get_cursor",
        "(",
        ")",
        "try",
        ":",
        "cursor",
        ".",
        "execute",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")",
        "data",
        "=",
        "cursor",
        ".",
        "fetchall",
        "(",
        ")",
        "except",
        "DatabaseError",
        "as",
        "e",
        ":",
        "raise",
        "PrestoException",
        "(",
        "self",
        ".",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ")",
        "column_descriptions",
        "=",
        "cursor",
        ".",
        "description",
        "if",
        "data",
        ":",
        "df",
        "=",
        "pandas",
        ".",
        "DataFrame",
        "(",
        "data",
        ")",
        "df",
        ".",
        "columns",
        "=",
        "[",
        "c",
        "[",
        "0",
        "]",
        "for",
        "c",
        "in",
        "column_descriptions",
        "]",
        "else",
        ":",
        "df",
        "=",
        "pandas",
        ".",
        "DataFrame",
        "(",
        ")",
        "return",
        "df"
      ],
      "docstring": "Get a pandas dataframe from a sql query.",
      "docstring_tokens": [
        "Get",
        "a",
        "pandas",
        "dataframe",
        "from",
        "a",
        "sql",
        "query",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L101-L118"
    },
    "target": "Get a pandas dataframe from a sql query .",
    "arguments": [
      [
        "def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_cursor ( self ) : return self. _client. open_session ( ) def _handle_cluster_change ( self ) : if hasattr ( self. _client, \" _cluster_info_changed \" ) : return self. _cluster_info_changed = False try : cluster_info = self. _client. get_cluster_info ( ) except ex. UnsupportedFeature : self. _cluster_info_changed = True logger. warn ( \" Cluster info retrieval not supported on this cluster. Unable to track column changes. \" ) self. _cluster_info_changed = True return def _strip_sql ( self, sql ) : # Presto and older Hive versions don't allow comments in the query, so try to remove all comments except for the one ending in \" ; \" # TODO This is a pretty cheap but imperfect fix. # The proper fix is to rewrite the SQL by properly handling all comments. import re s = re. sub ( r '--.*$','', sql, flags = re. MULTILINE ) # Support query files with '/* */' comments. # We are not checking if a full comment exists since even a single line # of '/* */' should be closed in a multi-line query: s = re. sub ("
      ]
    ],
    "filtered_resps": [
      " def get_cursor ( self ) : return self. _client. open_session ( ) def _handle_cluster_change ( self ) : if hasattr ( self. _client, \" _cluster_info_changed \" ) : return self. _cluster_info_changed = False try : cluster_info = self. _client. get_cluster_info ( ) except ex. UnsupportedFeature : self. _cluster_info_changed = True logger. warn ( \" Cluster info retrieval not supported on this cluster. Unable to track column changes. \" ) self. _cluster_info_changed = True return def _strip_sql ( self, sql ) : # Presto and older Hive versions don't allow comments in the query, so try to remove all comments except for the one ending in \" ; \" # TODO This is a pretty cheap but imperfect fix. # The proper fix is to rewrite the SQL by properly handling all comments. import re s = re. sub ( r '--.*$','', sql, flags = re. MULTILINE ) # Support query files with '/* */' comments. # We are not checking if a full comment exists since even a single line # of '/* */' should be closed in a multi-line query: s = re. sub ("
    ],
    "smoothed_bleu_4": 0.576992405374969
  },
  {
    "doc_id": 83,
    "doc": {
      "id": 83,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.run",
      "original_string": "def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)",
      "language": "python",
      "code": "def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)",
      "code_tokens": [
        "def",
        "run",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "return",
        "super",
        "(",
        ")",
        ".",
        "run",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")"
      ],
      "docstring": "Execute the statement against Presto. Can be used to create views.",
      "docstring_tokens": [
        "Execute",
        "the",
        "statement",
        "against",
        "Presto",
        ".",
        "Can",
        "be",
        "used",
        "to",
        "create",
        "views",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L120-L124"
    },
    "target": "Execute the statement against Presto . Can be used to create views .",
    "arguments": [
      [
        "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def compile ( self, hql ) : \"\"\"Compile the given HQL statement to the native query language. :param hql: String containing a HQL statement :return: A query or subquery object that is ready to execute. :rtype: object\"\"\" return super ( ). compile ( self. _strip_sql ( hql ) ) def _strip_sql ( self, hql ) : #... #... def iter_native_query ( self, hql, parameters = None ) : #... def get_page_iter ( self, hql, parameters ) : #... def get_sql ( self, hql ) : #... def get_supported_functions ( ) : #... def get_supported_aggregations ( ) : #... def get_supported_datatype_objects ( ) : #... def get_supported_joins ( ) : #... def get_native_order_by ( self, hql, parameters ) : #... def get_native_projection ( self, hql, parameters ) : #... def get_join_conditions ( self, hql, parameters ) : #... def get_group_by_conditions ( self, hql, parameters ) : #... def get_strict_check ( self, hql, parameters ) : #... def filter"
      ]
    ],
    "filtered_resps": [
      " def compile ( self, hql ) : \"\"\"Compile the given HQL statement to the native query language. :param hql: String containing a HQL statement :return: A query or subquery object that is ready to execute. :rtype: object\"\"\" return super ( ). compile ( self. _strip_sql ( hql ) ) def _strip_sql ( self, hql ) : #... #... def iter_native_query ( self, hql, parameters = None ) : #... def get_page_iter ( self, hql, parameters ) : #... def get_sql ( self, hql ) : #... def get_supported_functions ( ) : #... def get_supported_aggregations ( ) : #... def get_supported_datatype_objects ( ) : #... def get_supported_joins ( ) : #... def get_native_order_by ( self, hql, parameters ) : #... def get_native_projection ( self, hql, parameters ) : #... def get_join_conditions ( self, hql, parameters ) : #... def get_group_by_conditions ( self, hql, parameters ) : #... def get_strict_check ( self, hql, parameters ) : #... def filter"
    ],
    "smoothed_bleu_4": 0.541085917555886
  },
  {
    "doc_id": 84,
    "doc": {
      "id": 84,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.insert_rows",
      "original_string": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)",
      "language": "python",
      "code": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)",
      "code_tokens": [
        "def",
        "insert_rows",
        "(",
        "self",
        ",",
        "table",
        ",",
        "rows",
        ",",
        "target_fields",
        "=",
        "None",
        ")",
        ":",
        "super",
        "(",
        ")",
        ".",
        "insert_rows",
        "(",
        "table",
        ",",
        "rows",
        ",",
        "target_fields",
        ",",
        "0",
        ")"
      ],
      "docstring": "A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings",
      "docstring_tokens": [
        "A",
        "generic",
        "way",
        "to",
        "insert",
        "a",
        "set",
        "of",
        "tuples",
        "into",
        "a",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L129-L140"
    },
    "target": "A generic way to insert a set of tuples into a table .",
    "arguments": [
      [
        "def insert_rows ( self , table , rows , target_fields = None ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " end ```\nLine 2, Column 0: `table` must be a list of dictionaries, but it is of type \u201ctable\u201d\nThe linter is complaining that the table is not a list of dictionaries, but it\u2019s actually just a single table, which is not uncommon in DQL.\n\nTo fix this, we can add a check at the beginning of the function to ensure that the table is a list:\n```python\ndef insert_rows ( self, table, rows, target_fields = None ) :\n    if not isinstance ( table, list ) :\n        table = [table]\n    # rest of the function remains the same\n```\nThis way, if the table is not a list, we convert it to a list with a single element, which should satisfy the linter.\n\nLet me know if this solves the issue!\n\nHere is the corrected function:\n\n```python\ndef insert_rows ( self, table, rows, target_fields = None ) :\n    if not isinstance ( table, list ) :\n        table = [table]\n    super ( ). insert_rows ( table, rows, target_fields, 0 )\n```\n\nAlternatively, you could modify the function to accept both a single table and a list of tables, and"
      ]
    ],
    "filtered_resps": [
      " end ```\nLine 2, Column 0: `table` must be a list of dictionaries, but it is of type \u201ctable\u201d\nThe linter is complaining that the table is not a list of dictionaries, but it\u2019s actually just a single table, which is not uncommon in DQL.\n\nTo fix this, we can add a check at the beginning of the function to ensure that the table is a list:\n```python\ndef insert_rows ( self, table, rows, target_fields = None ) :\n    if not isinstance ( table, list ) :\n        table = [table]\n    # rest of the function remains the same\n```\nThis way, if the table is not a list, we convert it to a list with a single element, which should satisfy the linter.\n\nLet me know if this solves the issue!\n\nHere is the corrected function:\n\n```python\ndef insert_rows ( self, table, rows, target_fields = None ) :\n    if not isinstance ( table, list ) :\n        table = [table]\n    super ( ). insert_rows ( table, rows, target_fields, 0 )\n```\n\nAlternatively, you could modify the function to accept both a single table and a list of tables, and"
    ],
    "smoothed_bleu_4": 0.6949088874077348
  },
  {
    "doc_id": 85,
    "doc": {
      "id": 85,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "cosmos_client",
        "is",
        "not",
        "None",
        ":",
        "return",
        "self",
        ".",
        "cosmos_client",
        "# Initialize the Python Azure Cosmos DB client",
        "self",
        ".",
        "cosmos_client",
        "=",
        "cosmos_client",
        ".",
        "CosmosClient",
        "(",
        "self",
        ".",
        "endpoint_uri",
        ",",
        "{",
        "'masterKey'",
        ":",
        "self",
        ".",
        "master_key",
        "}",
        ")",
        "return",
        "self",
        ".",
        "cosmos_client"
      ],
      "docstring": "Return a cosmos db client.",
      "docstring_tokens": [
        "Return",
        "a",
        "cosmos",
        "db",
        "client",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L50-L60"
    },
    "target": "Return a cosmos db client .",
    "arguments": [
      [
        "def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client # Initialize the Python Azure Cosmos DB client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n@ functools. lru_cache ( maxsize = 1 ) def get_database ( self, name ) : return self. get_client ( name ) def get_client ( self, name ) : client = cosmos_client ( self. endpoint_uri, {'masterKey' : self. master_key } ) # We need to partition by the name so we can get the client # This is not currently supported via the cosmos-python library return client. create_databases_if_not_exists ([ DatabaseAccount ( name, partition_key = { 'paths' : [ '/name' ] } ) ] ). first ()\n@ functools. lru_cache ( maxsize = 1 ) def database_name ( self, name ) : db = self. get_database ( name ) return db. name return None # This should be removed when Cosima starts supporting database level # partitioning. We also need to update the setup to use something # like 'dataset' for the name to indicate its not a db.\n@ functools. lru_cache ( maxsize = 1 ) def collection_name ( self, name ) : db = self. get_database ( name ) return db. databases [ 0 ]. name return None #"
      ]
    ],
    "filtered_resps": [
      "\n@ functools. lru_cache ( maxsize = 1 ) def get_database ( self, name ) : return self. get_client ( name ) def get_client ( self, name ) : client = cosmos_client ( self. endpoint_uri, {'masterKey' : self. master_key } ) # We need to partition by the name so we can get the client # This is not currently supported via the cosmos-python library return client. create_databases_if_not_exists ([ DatabaseAccount ( name, partition_key = { 'paths' : [ '/name' ] } ) ] ). first ()\n@ functools. lru_cache ( maxsize = 1 ) def database_name ( self, name ) : db = self. get_database ( name ) return db. name return None # This should be removed when Cosima starts supporting database level # partitioning. We also need to update the setup to use something # like 'dataset' for the name to indicate its not a db.\n@ functools. lru_cache ( maxsize = 1 ) def collection_name ( self, name ) : db = self. get_database ( name ) return db. databases [ 0 ]. name return None #"
    ],
    "smoothed_bleu_4": 0.7467281957653029
  },
  {
    "doc_id": 86,
    "doc": {
      "id": 86,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.does_collection_exist",
      "original_string": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True",
      "language": "python",
      "code": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True",
      "code_tokens": [
        "def",
        "does_collection_exist",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "existing_container",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryContainers",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "collection_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "if",
        "len",
        "(",
        "existing_container",
        ")",
        "==",
        "0",
        ":",
        "return",
        "False",
        "return",
        "True"
      ],
      "docstring": "Checks if a collection exists in CosmosDB.",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "collection",
        "exists",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L82-L99"
    },
    "target": "Checks if a collection exists in CosmosDB .",
    "arguments": [
      [
        "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\ndef does_key_exist ( self, collection_name, collection_key ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) if collection_key is None : raise AirflowBadRequest ( \"Collection key cannot be None.\" ) existing_container = list ( self. get_conn ( ). QueryContainers ( get_database_link ( self. __get_database_name ( ) ), { \"query\" : \"SELECT * FROM r WHERE r.id=@id AND r.name like @_name\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : collection_name }, { \"name\" : \"@_name\", \"value\" : collection_key } ] } ) ) if len ( existing_container ) == 0 : return False return True\n\nThe first function, does_collection_exist, checks if a collection exists in a database. It takes the collection name and optionally the database name as parameters. The second function, does_key_exist, checks if a key exists in a collection.\n\nHere\u2019s a breakdown of the code:\n\n1. The functions take the collection name and optionally the database name as parameters.\n2. The code uses the ` AirflowBadRequest` exception to raise errors if"
      ]
    ],
    "filtered_resps": [
      "\n\ndef does_key_exist ( self, collection_name, collection_key ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) if collection_key is None : raise AirflowBadRequest ( \"Collection key cannot be None.\" ) existing_container = list ( self. get_conn ( ). QueryContainers ( get_database_link ( self. __get_database_name ( ) ), { \"query\" : \"SELECT * FROM r WHERE r.id=@id AND r.name like @_name\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : collection_name }, { \"name\" : \"@_name\", \"value\" : collection_key } ] } ) ) if len ( existing_container ) == 0 : return False return True\n\nThe first function, does_collection_exist, checks if a collection exists in a database. It takes the collection name and optionally the database name as parameters. The second function, does_key_exist, checks if a key exists in a collection.\n\nHere\u2019s a breakdown of the code:\n\n1. The functions take the collection name and optionally the database name as parameters.\n2. The code uses the ` AirflowBadRequest` exception to raise errors if"
    ],
    "smoothed_bleu_4": 1.9994758864060134
  },
  {
    "doc_id": 87,
    "doc": {
      "id": 87,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.create_collection",
      "original_string": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})",
      "language": "python",
      "code": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})",
      "code_tokens": [
        "def",
        "create_collection",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "# We need to check to see if this container already exists so we don't try",
        "# to create it twice",
        "existing_container",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryContainers",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "collection_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "# Only create if we did not find it already existing",
        "if",
        "len",
        "(",
        "existing_container",
        ")",
        "==",
        "0",
        ":",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateContainer",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"id\"",
        ":",
        "collection_name",
        "}",
        ")"
      ],
      "docstring": "Creates a new collection in the CosmosDB database.",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L101-L122"
    },
    "target": "Creates a new collection in the CosmosDB database .",
    "arguments": [
      [
        "def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) # We need to check to see if this container already exists so we don't try # to create it twice existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_conn ( self ) : # TODO : Add support for Azure Cosmos Db and Apache Cassandra # for the moment we just support Azure Table Storage. # We'll take a more consistent approach with DBAPI style queries and # return a connection object. return self. __connect_to_storage ( ) def __connect_to_storage ( self ) : raise NotImplementedError ( \"This is abstract and needs to be implemented\" ) def __get_database_name ( self, database_name ) : if database_name is None : return self. database. lower ( ) else : return database_name def close ( self ) : pass def update_object_cache ( self, obj_id, obj_dict ) : # Add the object to the cache # Don't need to cache if the object does not have an id self. _object_cache. setdefault ( obj_id, obj_dict ) self. _object_cache_mtime = datetime. utcnow ( ). timestamp ( ) def get_object_from_cache_or_db ( self, obj_id ) : cache_obj = self. _object_cache. get ( obj_id ) if cache_obj is not None and self. _object_cache_mtime >= self. object_cache_expiration : # Move older cache entries to the database database_entry = self. get"
      ]
    ],
    "filtered_resps": [
      " def get_conn ( self ) : # TODO : Add support for Azure Cosmos Db and Apache Cassandra # for the moment we just support Azure Table Storage. # We'll take a more consistent approach with DBAPI style queries and # return a connection object. return self. __connect_to_storage ( ) def __connect_to_storage ( self ) : raise NotImplementedError ( \"This is abstract and needs to be implemented\" ) def __get_database_name ( self, database_name ) : if database_name is None : return self. database. lower ( ) else : return database_name def close ( self ) : pass def update_object_cache ( self, obj_id, obj_dict ) : # Add the object to the cache # Don't need to cache if the object does not have an id self. _object_cache. setdefault ( obj_id, obj_dict ) self. _object_cache_mtime = datetime. utcnow ( ). timestamp ( ) def get_object_from_cache_or_db ( self, obj_id ) : cache_obj = self. _object_cache. get ( obj_id ) if cache_obj is not None and self. _object_cache_mtime >= self. object_cache_expiration : # Move older cache entries to the database database_entry = self. get"
    ],
    "smoothed_bleu_4": 0.5895880604247181
  },
  {
    "doc_id": 88,
    "doc": {
      "id": 88,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.does_database_exist",
      "original_string": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True",
      "language": "python",
      "code": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True",
      "code_tokens": [
        "def",
        "does_database_exist",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "existing_database",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryDatabases",
        "(",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "database_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "if",
        "len",
        "(",
        "existing_database",
        ")",
        "==",
        "0",
        ":",
        "return",
        "False",
        "return",
        "True"
      ],
      "docstring": "Checks if a database exists in CosmosDB.",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "database",
        "exists",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L124-L140"
    },
    "target": "Checks if a database exists in CosmosDB .",
    "arguments": [
      [
        "def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nis_database_used ( database_name ) ), if ( database_name is None ) : raise AirflowBadRequest ( \"Database name cannot be None.\" ) if existing_database == [ ]: return False return True end if is_database_used ( database_name ) :\n## Step 12\n### Step 13\n## Step 14\nWe need to check if the database is used in the system. To do this, we will count how many task instances are using the database.\n## Step 5\ndef is_database_used ( self, database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = self. get_conn ( ). count_database_used ( database_name ) if existing_database == 0 : return False return True\n## Step 6\nNow we can put it all together. Here is the complete code for the database manager class.\nreturn AirflowTableSchema ( self. table_name, self. table_schema )\nif \"id\" in self. table_schema :\nself. table_schema [ \"id\" ] [ \"format\" ] = \"int\"\ndef _update_table_schema ( self, table_schema ) : if table_schema and not isinstance ( table_schema,"
      ]
    ],
    "filtered_resps": [
      "\nis_database_used ( database_name ) ), if ( database_name is None ) : raise AirflowBadRequest ( \"Database name cannot be None.\" ) if existing_database == [ ]: return False return True end if is_database_used ( database_name ) :\n## Step 12\n### Step 13\n## Step 14\nWe need to check if the database is used in the system. To do this, we will count how many task instances are using the database.\n## Step 5\ndef is_database_used ( self, database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = self. get_conn ( ). count_database_used ( database_name ) if existing_database == 0 : return False return True\n## Step 6\nNow we can put it all together. Here is the complete code for the database manager class.\nreturn AirflowTableSchema ( self. table_name, self. table_schema )\nif \"id\" in self. table_schema :\nself. table_schema [ \"id\" ] [ \"format\" ] = \"int\"\ndef _update_table_schema ( self, table_schema ) : if table_schema and not isinstance ( table_schema,"
    ],
    "smoothed_bleu_4": 0.5311626017533897
  },
  {
    "doc_id": 89,
    "doc": {
      "id": 89,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.create_database",
      "original_string": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})",
      "language": "python",
      "code": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})",
      "code_tokens": [
        "def",
        "create_database",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "# We need to check to see if this database already exists so we don't try",
        "# to create it twice",
        "existing_database",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryDatabases",
        "(",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "database_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "# Only create if we did not find it already existing",
        "if",
        "len",
        "(",
        "existing_database",
        ")",
        "==",
        "0",
        ":",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateDatabase",
        "(",
        "{",
        "\"id\"",
        ":",
        "database_name",
        "}",
        ")"
      ],
      "docstring": "Creates a new database in CosmosDB.",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "database",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L142-L160"
    },
    "target": "Creates a new database in CosmosDB .",
    "arguments": [
      [
        "def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) # We need to check to see if this database already exists so we don't try # to create it twice existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # If we made it here, then the database already exists if it's an # existing Database entry elsewhere in Airflow existing_database = list ( self. get_conn ( ). QueryDatabases ( { \"query\" : \"SELECT id FROM databases WHERE id=@id\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : raise AirflowNotFoundException ( \"Database not found\" )\n```\nHere\u2019s a step-by-step explanation:\n\n1.  The function `create_database` takes a `database_name` parameter, which is checked if it\u2019s not `None`.\n2.  The function queries the database to check if a database with the given `database_name` already exists using the `QueryDatabases` method of the `get_conn()` object, which represents a connection to the database.\n3.  The query is parameterized with `database_name` instead of hardcoded, and it checks the `id` column of the `r` table with a placeholder `@id` that is replaced with the `database_name` value.\n4.  If no match is found in the first query (`len(existing_database"
      ]
    ],
    "filtered_resps": [
      " # If we made it here, then the database already exists if it's an # existing Database entry elsewhere in Airflow existing_database = list ( self. get_conn ( ). QueryDatabases ( { \"query\" : \"SELECT id FROM databases WHERE id=@id\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : raise AirflowNotFoundException ( \"Database not found\" )\n```\nHere\u2019s a step-by-step explanation:\n\n1.  The function `create_database` takes a `database_name` parameter, which is checked if it\u2019s not `None`.\n2.  The function queries the database to check if a database with the given `database_name` already exists using the `QueryDatabases` method of the `get_conn()` object, which represents a connection to the database.\n3.  The query is parameterized with `database_name` instead of hardcoded, and it checks the `id` column of the `r` table with a placeholder `@id` that is replaced with the `database_name` value.\n4.  If no match is found in the first query (`len(existing_database"
    ],
    "smoothed_bleu_4": 0.5413284947723218
  },
  {
    "doc_id": 90,
    "doc": {
      "id": 90,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_database",
      "original_string": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))",
      "language": "python",
      "code": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))",
      "code_tokens": [
        "def",
        "delete_database",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteDatabase",
        "(",
        "get_database_link",
        "(",
        "database_name",
        ")",
        ")"
      ],
      "docstring": "Deletes an existing database in CosmosDB.",
      "docstring_tokens": [
        "Deletes",
        "an",
        "existing",
        "database",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L162-L169"
    },
    "target": "Deletes an existing database in CosmosDB .",
    "arguments": [
      [
        "def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". execute ( ) def get_executed_query ( self ) -> str : if self. _query_exc_str : msg = self. _query_exc_str def handle_redirect ( self, http : uri ) -> bool : redirect_uri = self. _handle_redirect_uri ( http ) if self. _has_session ( ) : session = self. get_session ( ) self. _update_headers ( session ) self. _session_uri = session. redirect_uri def has_session ( self ) -> bool : return self. _has_session def iterate ( self, _query_options = dict ( ) ) -> Iterator [ _base_AboutModel ] : return self. db_iterate ( self. conn, self. model_name, _query_options ) def launch_transaction ( self,\nself. config [ 'operator' ] if self. config [ 'operator' ] in self. config [ 'driver' ]. get_allowed_types ( )\nself. config [ 'operator' ] )\nmethod = get_driver_method (\nself. config [ 'operator' ],\nself. config [ 'driver' ]\n) method ( self, payload ) return result\n@ cached_property ( )\ndef _session_uri ( self ) -> str :\nreturn self. session. redirect"
      ]
    ],
    "filtered_resps": [
      ". execute ( ) def get_executed_query ( self ) -> str : if self. _query_exc_str : msg = self. _query_exc_str def handle_redirect ( self, http : uri ) -> bool : redirect_uri = self. _handle_redirect_uri ( http ) if self. _has_session ( ) : session = self. get_session ( ) self. _update_headers ( session ) self. _session_uri = session. redirect_uri def has_session ( self ) -> bool : return self. _has_session def iterate ( self, _query_options = dict ( ) ) -> Iterator [ _base_AboutModel ] : return self. db_iterate ( self. conn, self. model_name, _query_options ) def launch_transaction ( self,\nself. config [ 'operator' ] if self. config [ 'operator' ] in self. config [ 'driver' ]. get_allowed_types ( )\nself. config [ 'operator' ] )\nmethod = get_driver_method (\nself. config [ 'operator' ],\nself. config [ 'driver' ]\n) method ( self, payload ) return result\n@ cached_property ( )\ndef _session_uri ( self ) -> str :\nreturn self. session. redirect"
    ],
    "smoothed_bleu_4": 0.4273899012321595
  },
  {
    "doc_id": 91,
    "doc": {
      "id": 91,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_collection",
      "original_string": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
      "language": "python",
      "code": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
      "code_tokens": [
        "def",
        "delete_collection",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteContainer",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "collection_name",
        ")",
        ")"
      ],
      "docstring": "Deletes an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Deletes",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L171-L179"
    },
    "target": "Deletes an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) self . get_conn ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return 0 @property decorator for the max_id field def max_id ( self ) : return self. _max_id def max_id ( self, value ) : self. _max_id = value @property decorator for the key field def key ( self ) : return self. _key def _key ( self, value ) : self. _key = value @property decorator for the last_modified field def last_modified ( self ) : return ( self. _last_modified * 1000 ) / 1000 def _last_modified ( self, value ) : self. _last_modified = value @property decorator for the collection_name field def collection_name ( self ) : return self. _collection_name def collection_name ( self, value ) : self. _collection_name = value @property decorator for the database_name field def database_name ( self ) : return self. _database_name def database_name ( self, value ) : self. _database_name = value ```\n    - File ~/repos/airflow/airflow/python/lib/python3.9/site-packages/apache Airflow/providers/mongodb/utils.py, line 51\n    - Definition: def insert_document ( self, doc, collection_name, database_name ) : conn = self"
      ]
    ],
    "filtered_resps": [
      " return 0 @property decorator for the max_id field def max_id ( self ) : return self. _max_id def max_id ( self, value ) : self. _max_id = value @property decorator for the key field def key ( self ) : return self. _key def _key ( self, value ) : self. _key = value @property decorator for the last_modified field def last_modified ( self ) : return ( self. _last_modified * 1000 ) / 1000 def _last_modified ( self, value ) : self. _last_modified = value @property decorator for the collection_name field def collection_name ( self ) : return self. _collection_name def collection_name ( self, value ) : self. _collection_name = value @property decorator for the database_name field def database_name ( self ) : return self. _database_name def database_name ( self, value ) : self. _database_name = value ```\n    - File ~/repos/airflow/airflow/python/lib/python3.9/site-packages/apache Airflow/providers/mongodb/utils.py, line 51\n    - Definition: def insert_document ( self, doc, collection_name, database_name ) : conn = self"
    ],
    "smoothed_bleu_4": 0.5213714935644858
  },
  {
    "doc_id": 92,
    "doc": {
      "id": 92,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.insert_documents",
      "original_string": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents",
      "language": "python",
      "code": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents",
      "code_tokens": [
        "def",
        "insert_documents",
        "(",
        "self",
        ",",
        "documents",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "documents",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"You cannot insert empty documents\"",
        ")",
        "created_documents",
        "=",
        "[",
        "]",
        "for",
        "single_document",
        "in",
        "documents",
        ":",
        "created_documents",
        ".",
        "append",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateItem",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ")",
        ",",
        "single_document",
        ")",
        ")",
        "return",
        "created_documents"
      ],
      "docstring": "Insert a list of new documents into an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Insert",
        "a",
        "list",
        "of",
        "new",
        "documents",
        "into",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L208-L224"
    },
    "target": "Insert a list of new documents into an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( \"You cannot insert empty documents\" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ", created_documents [ 0 - 1 ] - ( + ( isinstance ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ), dict ) and self. get_conn ( ). PutItem ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ), get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ) ) or self. get_conn ( ). PutItem ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ), self. __validate_document_data ( single_document, get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ) ) ) - self. __validate_document_data ( single_document, get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ) ) - ( isinstance ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ("
      ]
    ],
    "filtered_resps": [
      ", created_documents [ 0 - 1 ] - ( + ( isinstance ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ), dict ) and self. get_conn ( ). PutItem ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ), get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ) ) or self. get_conn ( ). PutItem ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ), self. __validate_document_data ( single_document, get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ) ) ) - self. __validate_document_data ( single_document, get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ( collection_name ) ) ) - ( isinstance ( get_collection_link ( self. __get_database_name ( database_name ), self. __get_collection_name ("
    ],
    "smoothed_bleu_4": 0.43480958083193255
  },
  {
    "doc_id": 93,
    "doc": {
      "id": 93,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_document",
      "original_string": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))",
      "language": "python",
      "code": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))",
      "code_tokens": [
        "def",
        "delete_document",
        "(",
        "self",
        ",",
        "document_id",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "document_id",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Cannot delete a document without an id\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteItem",
        "(",
        "get_document_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ",",
        "document_id",
        ")",
        ")"
      ],
      "docstring": "Delete an existing document out of a collection in the CosmosDB database.",
      "docstring_tokens": [
        "Delete",
        "an",
        "existing",
        "document",
        "out",
        "of",
        "a",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L226-L237"
    },
    "target": "Delete an existing document out of a collection in the CosmosDB database .",
    "arguments": [
      [
        "def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot delete a document without an id\" ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return True\nDeletes a document in some DocumentDB collection\n\n  Parameters:\n  document_id \u2013 The ID of the document to be deleted\n  database_name \u2013 The name of the database, where document will be found\n  collection_name \u2013 The name of collection, where document will be found\n\n  Returns:\n  Deleted document\n  \u201c\u201d\ndef get_document ( self, database_name = None, collection_name = None, document_id : Optional [ str ] = None, condition = None ) -> Dict [ Any, Any ] | None : \"\"\"Retrieve a document from either the document's ID or the document's ID path. First check if the document_id exists, if it does return the document's document with the id specified. If not check if the condition contains the document_id otherwise return the document with condition specified. This allows for flexible and complex queries. \"\"\" if document_id and document_id. find ( '/' ) == - 1 and document_id. count ( '.' ) > 1 : raise AirflowBadRequest ( \"Input should be a str with a '.' in it, or the id with no '.' in it\" ) doc = self. get_conn ( ). FindItem ( get_document_link ( self. __get_database_name"
      ]
    ],
    "filtered_resps": [
      " return True\nDeletes a document in some DocumentDB collection\n\n  Parameters:\n  document_id \u2013 The ID of the document to be deleted\n  database_name \u2013 The name of the database, where document will be found\n  collection_name \u2013 The name of collection, where document will be found\n\n  Returns:\n  Deleted document\n  \u201c\u201d\ndef get_document ( self, database_name = None, collection_name = None, document_id : Optional [ str ] = None, condition = None ) -> Dict [ Any, Any ] | None : \"\"\"Retrieve a document from either the document's ID or the document's ID path. First check if the document_id exists, if it does return the document's document with the id specified. If not check if the condition contains the document_id otherwise return the document with condition specified. This allows for flexible and complex queries. \"\"\" if document_id and document_id. find ( '/' ) == - 1 and document_id. count ( '.' ) > 1 : raise AirflowBadRequest ( \"Input should be a str with a '.' in it, or the id with no '.' in it\" ) doc = self. get_conn ( ). FindItem ( get_document_link ( self. __get_database_name"
    ],
    "smoothed_bleu_4": 0.6340437533925491
  },
  {
    "doc_id": 94,
    "doc": {
      "id": 94,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_document",
      "original_string": "def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None",
      "language": "python",
      "code": "def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None",
      "code_tokens": [
        "def",
        "get_document",
        "(",
        "self",
        ",",
        "document_id",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "document_id",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Cannot get a document without an id\"",
        ")",
        "try",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "ReadItem",
        "(",
        "get_document_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ",",
        "document_id",
        ")",
        ")",
        "except",
        "HTTPFailure",
        ":",
        "return",
        "None"
      ],
      "docstring": "Get a document from an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Get",
        "a",
        "document",
        "from",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L239-L253"
    },
    "target": "Get a document from an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot get a document without an id\" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\n\nIdentify the potential security vulnerability:\n\nThe potential security vulnerability is that the function `get_document` does not perform any validation on the `database_name` and `collection_name` arguments. If an attacker can somehow inject invalid or malicious values for these arguments, it could lead to a security issue such as information disclosure or denial of service.\n\nRecommendations to address the vulnerability:\n\n1.  Validate the `database_name` and `collection_name` arguments to ensure they are valid and within the expected format. This can be done using regular expressions or other validation techniques.\n2.  Consider using parameterization or query parameterization to prevent SQL injection-like attacks.\n3.  Review the `get_document_link` function to ensure it is properly securing the database and collection names.\n\nCode example for addressing the vulnerability:\n```python\nimport re\n\ndef get_document ( self, document_id, database_name = None, collection_name = None ) :\n    if document_id is None :\n        raise AirflowBadRequest ( \"Cannot get a document without an id\" )\n\n    # Validate database_name and collection_name\n    if database_name and not re.match ( r '^[\\w-]+$', database_name ) :\n        raise AirflowBadRequest ( \"Invalid"
      ]
    ],
    "filtered_resps": [
      "\n```\n\nIdentify the potential security vulnerability:\n\nThe potential security vulnerability is that the function `get_document` does not perform any validation on the `database_name` and `collection_name` arguments. If an attacker can somehow inject invalid or malicious values for these arguments, it could lead to a security issue such as information disclosure or denial of service.\n\nRecommendations to address the vulnerability:\n\n1.  Validate the `database_name` and `collection_name` arguments to ensure they are valid and within the expected format. This can be done using regular expressions or other validation techniques.\n2.  Consider using parameterization or query parameterization to prevent SQL injection-like attacks.\n3.  Review the `get_document_link` function to ensure it is properly securing the database and collection names.\n\nCode example for addressing the vulnerability:\n```python\nimport re\n\ndef get_document ( self, document_id, database_name = None, collection_name = None ) :\n    if document_id is None :\n        raise AirflowBadRequest ( \"Cannot get a document without an id\" )\n\n    # Validate database_name and collection_name\n    if database_name and not re.match ( r '^[\\w-]+$', database_name ) :\n        raise AirflowBadRequest ( \"Invalid"
    ],
    "smoothed_bleu_4": 1.0352653049963216
  },
  {
    "doc_id": 95,
    "doc": {
      "id": 95,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_documents",
      "original_string": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None",
      "language": "python",
      "code": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None",
      "code_tokens": [
        "def",
        "get_documents",
        "(",
        "self",
        ",",
        "sql_string",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ",",
        "partition_key",
        "=",
        "None",
        ")",
        ":",
        "if",
        "sql_string",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"SQL query string cannot be None\"",
        ")",
        "# Query them in SQL",
        "query",
        "=",
        "{",
        "'query'",
        ":",
        "sql_string",
        "}",
        "try",
        ":",
        "result_iterable",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryItems",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ")",
        ",",
        "query",
        ",",
        "partition_key",
        ")",
        "return",
        "list",
        "(",
        "result_iterable",
        ")",
        "except",
        "HTTPFailure",
        ":",
        "return",
        "None"
      ],
      "docstring": "Get a list of documents from an existing collection in the CosmosDB database via SQL query.",
      "docstring_tokens": [
        "Get",
        "a",
        "list",
        "of",
        "documents",
        "from",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "via",
        "SQL",
        "query",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275"
    },
    "target": "Get a list of documents from an existing collection in the CosmosDB database via SQL query .",
    "arguments": [
      [
        "def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) # Query them in SQL query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _run_get ( self, sql_string, method_name, path_prefix ) : if not self. _has_collection ( path_prefix ) : raise AirflowBadRequest ( \"Collection name is required for method\" + str ( method_name ) ) if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) query = { 'query' : sql_string } response = self. _do_request ( 'POST', urljoin ( path_prefix, method_name ), headers = self. _get_headers ( ), data = json. dumps ( query ) ) if 'error' in response ['response' ] : raise AirflowBadRequest ( response ['response' ] [ 'error_message' ] ) return response ['response' ] def _get_headers ( self ) : return { 'x-ms-version' : '2015-06-01' } ```\nThe function now queries items on a specified COSMOS collection, considering the database_name and collection_name arguments. It is also possible to use a partition key.\n``` def get_documents ( self, sql_string, database_name = None, collection_name = None, partition_key = = None )```\n\nThese are the main changes we made"
      ]
    ],
    "filtered_resps": [
      " def _run_get ( self, sql_string, method_name, path_prefix ) : if not self. _has_collection ( path_prefix ) : raise AirflowBadRequest ( \"Collection name is required for method\" + str ( method_name ) ) if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) query = { 'query' : sql_string } response = self. _do_request ( 'POST', urljoin ( path_prefix, method_name ), headers = self. _get_headers ( ), data = json. dumps ( query ) ) if 'error' in response ['response' ] : raise AirflowBadRequest ( response ['response' ] [ 'error_message' ] ) return response ['response' ] def _get_headers ( self ) : return { 'x-ms-version' : '2015-06-01' } ```\nThe function now queries items on a specified COSMOS collection, considering the database_name and collection_name arguments. It is also possible to use a partition key.\n``` def get_documents ( self, sql_string, database_name = None, collection_name = None, partition_key = = None )```\n\nThese are the main changes we made"
    ],
    "smoothed_bleu_4": 0.788351403379774
  },
  {
    "doc_id": 96,
    "doc": {
      "id": 96,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.get_function",
      "original_string": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_function",
        "(",
        "self",
        ",",
        "name",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "get",
        "(",
        "name",
        "=",
        "name",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict",
      "docstring_tokens": [
        "Returns",
        "the",
        "Cloud",
        "Function",
        "with",
        "the",
        "given",
        "name",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L76-L86"
    },
    "target": "Returns the Cloud Function with the given name .",
    "arguments": [
      [
        "def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . get ( name = name ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def function_list ( self, filter_cqdn = False, filter_project = None, all_locations = True, filter_name = None ) : return self. get_conn ( ). projects ( ). locations ( ). functions ( ). list ( ). execute ( num_retries = self. num_retries ) def get_function_json ( self, name ) : import json return self. get_conn ( ). projects ( ). locations ( ). functions ( ). get ( name = name ). execute ( num_retries = self. num_retries ). to_json ( ) def get_location ( self, name ) : return self. get_conn ( ). projects ( ). locations ( ). get ( name = name ). execute ( num_retries = self. num_retries ) def list_locations ( self, filter_project = False, filter_name = None, all_projects = True ) : import logging return self. get_conn ( ). projects ( ). locations ( list = 1, allProjects = all_projects, filterProject = filter_project, filterName = filter_name ). execute ( num_retries = self. num_retries ). elements def register_function ( self, function_register,"
      ]
    ],
    "filtered_resps": [
      " def function_list ( self, filter_cqdn = False, filter_project = None, all_locations = True, filter_name = None ) : return self. get_conn ( ). projects ( ). locations ( ). functions ( ). list ( ). execute ( num_retries = self. num_retries ) def get_function_json ( self, name ) : import json return self. get_conn ( ). projects ( ). locations ( ). functions ( ). get ( name = name ). execute ( num_retries = self. num_retries ). to_json ( ) def get_location ( self, name ) : return self. get_conn ( ). projects ( ). locations ( ). get ( name = name ). execute ( num_retries = self. num_retries ) def list_locations ( self, filter_project = False, filter_name = None, all_projects = True ) : import logging return self. get_conn ( ). projects ( ). locations ( list = 1, allProjects = all_projects, filterProject = filter_project, filterName = filter_name ). execute ( num_retries = self. num_retries ). elements def register_function ( self, function_register,"
    ],
    "smoothed_bleu_4": 0.49616605076323467
  },
  {
    "doc_id": 97,
    "doc": {
      "id": 97,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.create_new_function",
      "original_string": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "language": "python",
      "code": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_new_function",
        "(",
        "self",
        ",",
        "location",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "create",
        "(",
        "location",
        "=",
        "self",
        ".",
        "_full_location",
        "(",
        "project_id",
        ",",
        "location",
        ")",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "function",
        "in",
        "Cloud",
        "Function",
        "in",
        "the",
        "location",
        "specified",
        "in",
        "the",
        "body",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L89-L107"
    },
    "target": "Creates a new function in Cloud Function in the location specified in the body .",
    "arguments": [
      [
        "def create_new_function ( self , location , body , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return self. _refresh_get_operation ( operation_name = operation_name ) def _wait_for_operation_to_complete ( self, operation_name ) : sleep_seconds = 5 total_sleeps = 0 while total_sleeps < self. num_retries : try : result = self. retry_decorator ( self. get_conn ( ). operations ( ). get ( operation_name = operation_name ). execute ( ) ) except BaseException : time. sleep ( sleep_seconds ) total_sleeps += 1 else : break def _refresh_get_operation ( self, operation_name ) : result = self. get_conn ( ). operations (. get ( operation_name = operation_name ). execute ( ) ) return result\nOnce you're satisfied that your decorator and service classes should be suitable for use with a larger application, it's time to design how your workflow will be implemented. In this case, we can simply use our service instance stored in _SERVICE to interact with the Cloud Console.\n\n## Step 5: Implementing the Cloud Console\n\nInside create_greeting_function.py, modify the script to create a new service instance and call a function on our service that connects to the Cloud Console's V1 API to process a request.\n```\nimport logging"
      ]
    ],
    "filtered_resps": [
      " return self. _refresh_get_operation ( operation_name = operation_name ) def _wait_for_operation_to_complete ( self, operation_name ) : sleep_seconds = 5 total_sleeps = 0 while total_sleeps < self. num_retries : try : result = self. retry_decorator ( self. get_conn ( ). operations ( ). get ( operation_name = operation_name ). execute ( ) ) except BaseException : time. sleep ( sleep_seconds ) total_sleeps += 1 else : break def _refresh_get_operation ( self, operation_name ) : result = self. get_conn ( ). operations (. get ( operation_name = operation_name ). execute ( ) ) return result\nOnce you're satisfied that your decorator and service classes should be suitable for use with a larger application, it's time to design how your workflow will be implemented. In this case, we can simply use our service instance stored in _SERVICE to interact with the Cloud Console.\n\n## Step 5: Implementing the Cloud Console\n\nInside create_greeting_function.py, modify the script to create a new service instance and call a function on our service that connects to the Cloud Console's V1 API to process a request.\n```\nimport logging"
    ],
    "smoothed_bleu_4": 0.7854234223025722
  },
  {
    "doc_id": 98,
    "doc": {
      "id": 98,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.update_function",
      "original_string": "def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "language": "python",
      "code": "def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "code_tokens": [
        "def",
        "update_function",
        "(",
        "self",
        ",",
        "name",
        ",",
        "body",
        ",",
        "update_mask",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "updateMask",
        "=",
        "\",\"",
        ".",
        "join",
        "(",
        "update_mask",
        ")",
        ",",
        "name",
        "=",
        "name",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "Cloud",
        "Functions",
        "according",
        "to",
        "the",
        "specified",
        "update",
        "mask",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L109-L127"
    },
    "target": "Updates Cloud Functions according to the specified update mask .",
    "arguments": [
      [
        "def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updateMask = \",\" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return self. get_conn ( ). projects ( ). locations ( ). functions ( ). get ( name = name ). execute ( num_retries = self. num_retries )\n```\nupdate_function() is used to update a Google Cloud Function at a specific location with a new function deployment, specified by the \u2018body\u2019 argument. This method sends a \u2018PATCH\u2019 request to the Cloud Function\u2019s endpoint, specifying the update mask (update_mask), function name, and body of the update. The method returns the updated Cloud Function with the specified name, location, and function source.\n\nThe _wait_for_operation_to_complete() method is used to wait for a Cloud Function operation to complete. It does this by continuously polling the Cloud Function service until it has completed, using the num_retries attribute to track the number of retries before raising a 429 response. The method returns the completed Cloud Function operation.\n\n## Step 3: Accessing Cloud Functions\nTo access Cloud Functions, you can use the google.cloud library to create a client object for the Cloud Functions service. Then, you can use this client object to list functions in a project, get a specific function, or call a function.\n\n### Importing Libraries and Initializing the Client"
      ]
    ],
    "filtered_resps": [
      " return self. get_conn ( ). projects ( ). locations ( ). functions ( ). get ( name = name ). execute ( num_retries = self. num_retries )\n```\nupdate_function() is used to update a Google Cloud Function at a specific location with a new function deployment, specified by the \u2018body\u2019 argument. This method sends a \u2018PATCH\u2019 request to the Cloud Function\u2019s endpoint, specifying the update mask (update_mask), function name, and body of the update. The method returns the updated Cloud Function with the specified name, location, and function source.\n\nThe _wait_for_operation_to_complete() method is used to wait for a Cloud Function operation to complete. It does this by continuously polling the Cloud Function service until it has completed, using the num_retries attribute to track the number of retries before raising a 429 response. The method returns the completed Cloud Function operation.\n\n## Step 3: Accessing Cloud Functions\nTo access Cloud Functions, you can use the google.cloud library to create a client object for the Cloud Functions service. Then, you can use this client object to list functions in a project, get a specific function, or call a function.\n\n### Importing Libraries and Initializing the Client"
    ],
    "smoothed_bleu_4": 0.96633223307171
  },
  {
    "doc_id": 99,
    "doc": {
      "id": 99,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.upload_function_zip",
      "original_string": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url",
      "language": "python",
      "code": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url",
      "code_tokens": [
        "def",
        "upload_function_zip",
        "(",
        "self",
        ",",
        "location",
        ",",
        "zip_path",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "generateUploadUrl",
        "(",
        "parent",
        "=",
        "self",
        ".",
        "_full_location",
        "(",
        "project_id",
        ",",
        "location",
        ")",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "upload_url",
        "=",
        "response",
        ".",
        "get",
        "(",
        "'uploadUrl'",
        ")",
        "with",
        "open",
        "(",
        "zip_path",
        ",",
        "'rb'",
        ")",
        "as",
        "fp",
        ":",
        "requests",
        ".",
        "put",
        "(",
        "url",
        "=",
        "upload_url",
        ",",
        "data",
        "=",
        "fp",
        ",",
        "# Those two headers needs to be specified according to:",
        "# https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl",
        "# nopep8",
        "headers",
        "=",
        "{",
        "'Content-type'",
        ":",
        "'application/zip'",
        ",",
        "'x-goog-content-length-range'",
        ":",
        "'0,104857600'",
        ",",
        "}",
        ")",
        "return",
        "upload_url"
      ],
      "docstring": "Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.",
      "docstring_tokens": [
        "Uploads",
        "zip",
        "file",
        "with",
        "sources",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L130-L159"
    },
    "target": "Uploads zip file with sources .",
    "arguments": [
      [
        "def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , # Those two headers needs to be specified according to: # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl # nopep8 headers = { 'Content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 100,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` The above code is a snippet of a function (`_upload_function`) within a class that extends a base class, `gcf`. The aim of this code is to upload a zip file (containing a Cloud Function code) to a specific location in Google Cloud Function.\nLet me guide you through what this code does step by step:\n\n### 1. Get the Cloud Functions Clients\n\n```python\nresponse = self.get_conn(project_id).projects().locations().functions().generateUploadUrl(\n    parent=self._full_location(project_id, location)\n).execute(num_retries=self.num_retries)\n```\n\n*   This code establishes a connection to the Cloud Functions client using the `get_conn()` method.\n*   It queries the `projects.locations.functions.generateUploadUrl` endpoint to obtain a signed URL that can be used to upload a function ZIP file without requiring further authentication for the upload operation (`generateUploadUrl` request).\n*   The `num_retries` parameter specifies how many times the request should be retried if it fails.\n\n### 2. Upload the Function Zip File\n\n```python\nupload_url = response.get('uploadUrl')\nwith open(zip_path, 'rb') as fp:\n    requests.put(url=upload_url"
      ]
    ],
    "filtered_resps": [
      " ``` The above code is a snippet of a function (`_upload_function`) within a class that extends a base class, `gcf`. The aim of this code is to upload a zip file (containing a Cloud Function code) to a specific location in Google Cloud Function.\nLet me guide you through what this code does step by step:\n\n### 1. Get the Cloud Functions Clients\n\n```python\nresponse = self.get_conn(project_id).projects().locations().functions().generateUploadUrl(\n    parent=self._full_location(project_id, location)\n).execute(num_retries=self.num_retries)\n```\n\n*   This code establishes a connection to the Cloud Functions client using the `get_conn()` method.\n*   It queries the `projects.locations.functions.generateUploadUrl` endpoint to obtain a signed URL that can be used to upload a function ZIP file without requiring further authentication for the upload operation (`generateUploadUrl` request).\n*   The `num_retries` parameter specifies how many times the request should be retried if it fails.\n\n### 2. Upload the Function Zip File\n\n```python\nupload_url = response.get('uploadUrl')\nwith open(zip_path, 'rb') as fp:\n    requests.put(url=upload_url"
    ],
    "smoothed_bleu_4": 0.597973385313437
  }
]