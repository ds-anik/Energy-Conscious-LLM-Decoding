[
  {
    "doc_id": 0,
    "doc": {
      "id": 0,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/miomio.py",
      "func_name": "sina_xml_to_url_list",
      "original_string": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl",
      "language": "python",
      "code": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl",
      "code_tokens": [
        "def",
        "sina_xml_to_url_list",
        "(",
        "xml_data",
        ")",
        ":",
        "rawurl",
        "=",
        "[",
        "]",
        "dom",
        "=",
        "parseString",
        "(",
        "xml_data",
        ")",
        "for",
        "node",
        "in",
        "dom",
        ".",
        "getElementsByTagName",
        "(",
        "'durl'",
        ")",
        ":",
        "url",
        "=",
        "node",
        ".",
        "getElementsByTagName",
        "(",
        "'url'",
        ")",
        "[",
        "0",
        "]",
        "rawurl",
        ".",
        "append",
        "(",
        "url",
        ".",
        "childNodes",
        "[",
        "0",
        "]",
        ".",
        "data",
        ")",
        "return",
        "rawurl"
      ],
      "docstring": "str->list\n    Convert XML to URL List.\n    From Biligrab.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "list",
        "Convert",
        "XML",
        "to",
        "URL",
        "List",
        ".",
        "From",
        "Biligrab",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51"
    },
    "target": "str - > list Convert XML to URL List . From Biligrab .",
    "arguments": [
      [
        "def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName ( 'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n2 3\n## Chapter 7\u00a0\u00a0Working with Files\nIn this chapter, we will discuss the various ways to work with files, including how to copy, delete, move, and manage directories.\n### Working with Files\nBefore you can start working with files, you need to import the os module and get a list of the names of files in a directory. You can use the listdir() function from the os module to achieve this:\n```\nimport os\nfiles = os.listdir('.')\n```\nThe listdir() function returns a list of files and directories in the current working directory ( '.' ). The '.' refers to the current directory. Use a loop to iterate over the files.\n```\nfor file in listdir():\n```\nYou can use the isfile() and isdir() functions to check whether a name is a file or directory. If a name is a file, isfile(name) returns True ; otherwise, it returns False. If a name is a directory, isdir(name) returns True ; otherwise, it returns False.\n```\nif os.path.isfile( name ):\nif os.path.isdir( name ):\n```\nTo read from a file, use the open() function with the 'r' or '"
      ]
    ],
    "filtered_resps": [
      "\n2 3\n## Chapter 7\u00a0\u00a0Working with Files\nIn this chapter, we will discuss the various ways to work with files, including how to copy, delete, move, and manage directories.\n### Working with Files\nBefore you can start working with files, you need to import the os module and get a list of the names of files in a directory. You can use the listdir() function from the os module to achieve this:\n```\nimport os\nfiles = os.listdir('.')\n```\nThe listdir() function returns a list of files and directories in the current working directory ( '.' ). The '.' refers to the current directory. Use a loop to iterate over the files.\n```\nfor file in listdir():\n```\nYou can use the isfile() and isdir() functions to check whether a name is a file or directory. If a name is a file, isfile(name) returns True ; otherwise, it returns False. If a name is a directory, isdir(name) returns True ; otherwise, it returns False.\n```\nif os.path.isfile( name ):\nif os.path.isdir( name ):\n```\nTo read from a file, use the open() function with the 'r' or '"
    ],
    "smoothed_bleu_4": 0.5856284828590567
  },
  {
    "doc_id": 1,
    "doc": {
      "id": 1,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/dailymotion.py",
      "func_name": "dailymotion_download",
      "original_string": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",
      "language": "python",
      "code": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)",
      "code_tokens": [
        "def",
        "dailymotion_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "get_content",
        "(",
        "rebuilt_url",
        "(",
        "url",
        ")",
        ")",
        "info",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "match1",
        "(",
        "html",
        ",",
        "r'qualities\":({.+?}),\"'",
        ")",
        ")",
        "title",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'\"video_title\"\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "or",
        "match1",
        "(",
        "html",
        ",",
        "r'\"title\"\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "title",
        "=",
        "unicodize",
        "(",
        "title",
        ")",
        "for",
        "quality",
        "in",
        "[",
        "'1080'",
        ",",
        "'720'",
        ",",
        "'480'",
        ",",
        "'380'",
        ",",
        "'240'",
        ",",
        "'144'",
        ",",
        "'auto'",
        "]",
        ":",
        "try",
        ":",
        "real_url",
        "=",
        "info",
        "[",
        "quality",
        "]",
        "[",
        "1",
        "]",
        "[",
        "\"url\"",
        "]",
        "if",
        "real_url",
        ":",
        "break",
        "except",
        "KeyError",
        ":",
        "pass",
        "mime",
        ",",
        "ext",
        ",",
        "size",
        "=",
        "url_info",
        "(",
        "real_url",
        ")",
        "print_info",
        "(",
        "site_info",
        ",",
        "title",
        ",",
        "mime",
        ",",
        "size",
        ")",
        "if",
        "not",
        "info_only",
        ":",
        "download_urls",
        "(",
        "[",
        "real_url",
        "]",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ")"
      ],
      "docstring": "Downloads Dailymotion videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "Dailymotion",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35"
    },
    "target": "Downloads Dailymotion videos by URL .",
    "arguments": [
      [
        "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ \"url\" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [ sourcecode ] _________________________________________ ## Step 3: Extract video title\nMatch video title from the provided HTML content.\n```python\ntitle = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"')\nor\ntitle = match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n```\n## Step 4: Unicodize video title\nConvert video title to Unicode.\n```python\ntitle = unicodize(title)\n```\n## Step 5: Get video qualities and URLs\nLoop through the qualities, extract video URLs from the information dictionary, and add them to the list.\n```python\nfor quality in ['1080', '720', '480', '380', '240', '144', 'auto']:\n    try:\n        real_url = info[quality][1][\"url\"]\n        if real_url:\n            break\n    except KeyError:\n        pass\n```\n## Step 6: Print video information\nPrint video title, size, and MIME type.\n```python\nprint_info(site_info, title, mime, size)\n```\n## Step 7: Download video\nDownload the video file using the real URL.\n```python\nif"
      ]
    ],
    "filtered_resps": [
      " [ sourcecode ] _________________________________________ ## Step 3: Extract video title\nMatch video title from the provided HTML content.\n```python\ntitle = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"')\nor\ntitle = match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n```\n## Step 4: Unicodize video title\nConvert video title to Unicode.\n```python\ntitle = unicodize(title)\n```\n## Step 5: Get video qualities and URLs\nLoop through the qualities, extract video URLs from the information dictionary, and add them to the list.\n```python\nfor quality in ['1080', '720', '480', '380', '240', '144', 'auto']:\n    try:\n        real_url = info[quality][1][\"url\"]\n        if real_url:\n            break\n    except KeyError:\n        pass\n```\n## Step 6: Print video information\nPrint video title, size, and MIME type.\n```python\nprint_info(site_info, title, mime, size)\n```\n## Step 7: Download video\nDownload the video file using the real URL.\n```python\nif"
    ],
    "smoothed_bleu_4": 0.45146640732525845
  },
  {
    "doc_id": 2,
    "doc": {
      "id": 2,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/sina.py",
      "func_name": "sina_download",
      "original_string": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "language": "python",
      "code": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "code_tokens": [
        "def",
        "sina_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'news.sina.com.cn/zxt'",
        "in",
        "url",
        ":",
        "sina_zxt",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "return",
        "vid",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'vid=(\\d+)'",
        ")",
        "if",
        "vid",
        "is",
        "None",
        ":",
        "video_page",
        "=",
        "get_content",
        "(",
        "url",
        ")",
        "vid",
        "=",
        "hd_vid",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'hd_vid\\s*:\\s*\\'([^\\']+)\\''",
        ")",
        "if",
        "hd_vid",
        "==",
        "'0'",
        ":",
        "vids",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\''",
        ")",
        ".",
        "split",
        "(",
        "'|'",
        ")",
        "vid",
        "=",
        "vids",
        "[",
        "-",
        "1",
        "]",
        "if",
        "vid",
        "is",
        "None",
        ":",
        "vid",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'vid:\"?(\\d+)\"?'",
        ")",
        "if",
        "vid",
        ":",
        "#title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')",
        "sina_download_by_vid",
        "(",
        "vid",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "else",
        ":",
        "vkey",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'vkey\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "if",
        "vkey",
        "is",
        "None",
        ":",
        "vid",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'#(\\d+)'",
        ")",
        "sina_download_by_vid",
        "(",
        "vid",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "return",
        "title",
        "=",
        "match1",
        "(",
        "video_page",
        ",",
        "r'title\\s*:\\s*\"([^\"]+)\"'",
        ")",
        "sina_download_by_vkey",
        "(",
        "vkey",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")"
      ],
      "docstring": "Downloads Sina videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "Sina",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121"
    },
    "target": "Downloads Sina videos by URL .",
    "arguments": [
      [
        "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def sina_download_by_vkey ( vkey, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vkey, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_vid ( vid, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vid, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_www ( www, output_dir = '.', merge = True, info_only = False ) : return _sina_download_www ( www, output_dir, merge, info_only ) def _sina_download_www ( www, output_dir, merge, info_only ) : url = 'http://video.sina.com.cn/v/ %s.html' % www. replace ( '.', '' ) headers = { 'User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.1) Gecko/20100101 Firefox/10.0.1' } content = get_content ( url, headers = headers ) if 'video.sina.com"
      ]
    ],
    "filtered_resps": [
      " def sina_download_by_vkey ( vkey, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vkey, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_vid ( vid, output_dir = '.', merge = True, info_only = False ) : return sina_download_by_www ( vid, output_dir = output_dir, merge = merge, info_only = info_only ) def sina_download_by_www ( www, output_dir = '.', merge = True, info_only = False ) : return _sina_download_www ( www, output_dir, merge, info_only ) def _sina_download_www ( www, output_dir, merge, info_only ) : url = 'http://video.sina.com.cn/v/ %s.html' % www. replace ( '.', '' ) headers = { 'User-Agent', 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:10.0.1) Gecko/20100101 Firefox/10.0.1' } content = get_content ( url, headers = headers ) if 'video.sina.com"
    ],
    "smoothed_bleu_4": 0.48892631922857
  },
  {
    "doc_id": 3,
    "doc": {
      "id": 3,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "sprint",
      "original_string": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",
      "language": "python",
      "code": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text",
      "code_tokens": [
        "def",
        "sprint",
        "(",
        "text",
        ",",
        "*",
        "colors",
        ")",
        ":",
        "return",
        "\"\\33[{}m{content}\\33[{}m\"",
        ".",
        "format",
        "(",
        "\";\"",
        ".",
        "join",
        "(",
        "[",
        "str",
        "(",
        "color",
        ")",
        "for",
        "color",
        "in",
        "colors",
        "]",
        ")",
        ",",
        "RESET",
        ",",
        "content",
        "=",
        "text",
        ")",
        "if",
        "IS_ANSI_TERMINAL",
        "and",
        "colors",
        "else",
        "text"
      ],
      "docstring": "Format text with color or other effects into ANSI escaped string.",
      "docstring_tokens": [
        "Format",
        "text",
        "with",
        "color",
        "or",
        "other",
        "effects",
        "into",
        "ANSI",
        "escaped",
        "string",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62"
    },
    "target": "Format text with color or other effects into ANSI escaped string .",
    "arguments": [
      [
        "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\n[ Docs ] [ Source ] [ Tests ] [ Other Functions ] [ File ] [ Repo ]\n### def colorize()\n\nColorize text.\n\n[ Docs ] [ Source ] [ Tests ] [ Other Functions ] [ File ] [ Repo ]\n\n# Code\ndef colorize ( text, color ) :\ndef rgb ( r, g, b ) : return f \"# {int ( r \u00d7 256 ) : 02X}{int ( g \u00d7 256 ) : 02X}{int ( b \u00d7 256 ) : 02X} \"\ndef rgb255 ( r, g, b ) : return f \"${r:02X}{g:02X}{b:02X}\" def bold ( text ) :\nreturn f \"{text}\" def red ( text ) : return f \"${text} \"\ndef green ( text ) : return f \"${text} \"\ndef yellow ( text ) : return f \"${text} \"\ndef cyan ( text ) : return f \"${text} \"\ndef italic ( text ) : return f \" {text} \"\ndef magenta ( text ) : return f \"${text} \"\ndef blue ( text ) : return f \"${text} \"\ndef reset ( ) :\nreturn \"\" def rgb_to_ansi ("
      ]
    ],
    "filtered_resps": [
      "\n\n[ Docs ] [ Source ] [ Tests ] [ Other Functions ] [ File ] [ Repo ]\n### def colorize()\n\nColorize text.\n\n[ Docs ] [ Source ] [ Tests ] [ Other Functions ] [ File ] [ Repo ]\n\n# Code\ndef colorize ( text, color ) :\ndef rgb ( r, g, b ) : return f \"# {int ( r \u00d7 256 ) : 02X}{int ( g \u00d7 256 ) : 02X}{int ( b \u00d7 256 ) : 02X} \"\ndef rgb255 ( r, g, b ) : return f \"${r:02X}{g:02X}{b:02X}\" def bold ( text ) :\nreturn f \"{text}\" def red ( text ) : return f \"${text} \"\ndef green ( text ) : return f \"${text} \"\ndef yellow ( text ) : return f \"${text} \"\ndef cyan ( text ) : return f \"${text} \"\ndef italic ( text ) : return f \" {text} \"\ndef magenta ( text ) : return f \"${text} \"\ndef blue ( text ) : return f \"${text} \"\ndef reset ( ) :\nreturn \"\" def rgb_to_ansi ("
    ],
    "smoothed_bleu_4": 0.581285347685132
  },
  {
    "doc_id": 4,
    "doc": {
      "id": 4,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "print_log",
      "original_string": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")",
      "language": "python",
      "code": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")",
      "code_tokens": [
        "def",
        "print_log",
        "(",
        "text",
        ",",
        "*",
        "colors",
        ")",
        ":",
        "sys",
        ".",
        "stderr",
        ".",
        "write",
        "(",
        "sprint",
        "(",
        "\"{}: {}\"",
        ".",
        "format",
        "(",
        "script_name",
        ",",
        "text",
        ")",
        ",",
        "*",
        "colors",
        ")",
        "+",
        "\"\\n\"",
        ")"
      ],
      "docstring": "Print a log message to standard error.",
      "docstring_tokens": [
        "Print",
        "a",
        "log",
        "message",
        "to",
        "standard",
        "error",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74"
    },
    "target": "Print a log message to standard error .",
    "arguments": [
      [
        "def print_log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script_name , text ) , * colors ) + \"\\n\" )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` This function will generate output like this: ``` python :   [ 0, 1, 2, 3 ]. append ( 4 ) python :   [ 0, 1, 2, 3, 4 ] ```\n  - This function prints a formatted output of what operations a user is performing on a list. It can handle addition, deletion, insertion, and modification of lists. If the operation is an insertion or deletion, the index and value to insert or value to remove are also printed. It can handle multiple operations on the list. In this case, it simply prints out the operations in order of how they were performed. The text that is printed will change the color of the printed text based on the operation. You can add new colors by using the add_color method. For example, you can use add_color(4, 'yellow') to make any number greater than 3 be yellow. Note that there is a problem with how we handle the index for removals. We don't correctly take into account any modifications that occur before the removal. For example, if we first append a value to a list, then try to remove that value at the beginning, the index that it is removed"
      ]
    ],
    "filtered_resps": [
      " ``` This function will generate output like this: ``` python :   [ 0, 1, 2, 3 ]. append ( 4 ) python :   [ 0, 1, 2, 3, 4 ] ```\n  - This function prints a formatted output of what operations a user is performing on a list. It can handle addition, deletion, insertion, and modification of lists. If the operation is an insertion or deletion, the index and value to insert or value to remove are also printed. It can handle multiple operations on the list. In this case, it simply prints out the operations in order of how they were performed. The text that is printed will change the color of the printed text based on the operation. You can add new colors by using the add_color method. For example, you can use add_color(4, 'yellow') to make any number greater than 3 be yellow. Note that there is a problem with how we handle the index for removals. We don't correctly take into account any modifications that occur before the removal. For example, if we first append a value to a list, then try to remove that value at the beginning, the index that it is removed"
    ],
    "smoothed_bleu_4": 0.5388256314173692
  },
  {
    "doc_id": 5,
    "doc": {
      "id": 5,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "e",
      "original_string": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "language": "python",
      "code": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "code_tokens": [
        "def",
        "e",
        "(",
        "message",
        ",",
        "exit_code",
        "=",
        "None",
        ")",
        ":",
        "print_log",
        "(",
        "message",
        ",",
        "YELLOW",
        ",",
        "BOLD",
        ")",
        "if",
        "exit_code",
        "is",
        "not",
        "None",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "exit_code",
        ")"
      ],
      "docstring": "Print an error log message.",
      "docstring_tokens": [
        "Print",
        "an",
        "error",
        "log",
        "message",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92"
    },
    "target": "Print an error log message .",
    "arguments": [
      [
        "def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def i ( message ) : print_log ( message, BLUE, BOLD ) print_log = lambda f : lambda e, c, b : print ( f\" \\33[ { c } ; { b } m { e } { c } \\33[ 0m \", flush = True ) RED = \" \\033 [91m \" GREEN = \" \\033 [ 92m \" YELLOW = \" \\033 [ 93m \" BLUE = \" \\033 [ 94m \" BOLD = \" \\033 [1m \" def n ( x_1, x_2, dx ) : \"\"\" Return n values over x_1 <= x <= x_2 with delta dx. \"\"\" y = int ( ( x_2 - x_1 ) / dx ) return range ( x_1, x_2 + dx, dx ) def g ( s, t, x_3 ) : \"\"\" Returns the Gaussian function at location t with width x_3. \"\"\" return s / ( x_3 * s ** 0.5 ) * np. exp ( - ( np. log ( t ) - np. log ( s )) * ( np. log ( t ) - np. log ("
      ]
    ],
    "filtered_resps": [
      " def i ( message ) : print_log ( message, BLUE, BOLD ) print_log = lambda f : lambda e, c, b : print ( f\" \\33[ { c } ; { b } m { e } { c } \\33[ 0m \", flush = True ) RED = \" \\033 [91m \" GREEN = \" \\033 [ 92m \" YELLOW = \" \\033 [ 93m \" BLUE = \" \\033 [ 94m \" BOLD = \" \\033 [1m \" def n ( x_1, x_2, dx ) : \"\"\" Return n values over x_1 <= x <= x_2 with delta dx. \"\"\" y = int ( ( x_2 - x_1 ) / dx ) return range ( x_1, x_2 + dx, dx ) def g ( s, t, x_3 ) : \"\"\" Returns the Gaussian function at location t with width x_3. \"\"\" return s / ( x_3 * s ** 0.5 ) * np. exp ( - ( np. log ( t ) - np. log ( s )) * ( np. log ( t ) - np. log ("
    ],
    "smoothed_bleu_4": 0.5651234100161556
  },
  {
    "doc_id": 6,
    "doc": {
      "id": 6,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/log.py",
      "func_name": "wtf",
      "original_string": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "language": "python",
      "code": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)",
      "code_tokens": [
        "def",
        "wtf",
        "(",
        "message",
        ",",
        "exit_code",
        "=",
        "1",
        ")",
        ":",
        "print_log",
        "(",
        "message",
        ",",
        "RED",
        ",",
        "BOLD",
        ")",
        "if",
        "exit_code",
        "is",
        "not",
        "None",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "exit_code",
        ")"
      ],
      "docstring": "What a Terrible Failure!",
      "docstring_tokens": [
        "What",
        "a",
        "Terrible",
        "Failure!"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98"
    },
    "target": "What a Terrible Failure!",
    "arguments": [
      [
        "def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " : wtf ( 'Please, create an empty database file and add it to your local project folder under the name \"config.db\" with SQLCipher installed.', 5 ) : if not os. path. exists ( 'config.db' ) : wtf ( 'Database file does not exist!' ) : wtf ( 'Please, add a valid SQLCipher database file to the project dir.', 5 ) : try : from Crypto. PublicKey import RSA : key = RSA. generate_keypair () : with open ( 'config.db', 'wb' ) as f : f. write ( key. exportKey ( format = 'PEM' )) : print_log ( 'New database and RSA keys exported!' ) : except ImportError : wtf ( 'Missing Crypto library. Please, install it with pip install PyCrypto', 5 ) : from Crypto. PublicKey import RSA : # Get RSA from encrypted database key, decrypting with private key : with open ( 'config.db', 'rb' ) as f : encrypted_key = f. read () : private_key = RSA. import_key ( from_string ( 'PEM', encrypted_key )) : public_key = private_key. exportKey ( format ="
      ]
    ],
    "filtered_resps": [
      " : wtf ( 'Please, create an empty database file and add it to your local project folder under the name \"config.db\" with SQLCipher installed.', 5 ) : if not os. path. exists ( 'config.db' ) : wtf ( 'Database file does not exist!' ) : wtf ( 'Please, add a valid SQLCipher database file to the project dir.', 5 ) : try : from Crypto. PublicKey import RSA : key = RSA. generate_keypair () : with open ( 'config.db', 'wb' ) as f : f. write ( key. exportKey ( format = 'PEM' )) : print_log ( 'New database and RSA keys exported!' ) : except ImportError : wtf ( 'Missing Crypto library. Please, install it with pip install PyCrypto', 5 ) : from Crypto. PublicKey import RSA : # Get RSA from encrypted database key, decrypting with private key : with open ( 'config.db', 'rb' ) as f : encrypted_key = f. read () : private_key = RSA. import_key ( from_string ( 'PEM', encrypted_key )) : public_key = private_key. exportKey ( format ="
    ],
    "smoothed_bleu_4": 0.46773390193121667
  },
  {
    "doc_id": 7,
    "doc": {
      "id": 7,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/os.py",
      "func_name": "detect_os",
      "original_string": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os",
      "language": "python",
      "code": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os",
      "code_tokens": [
        "def",
        "detect_os",
        "(",
        ")",
        ":",
        "# Inspired by:",
        "# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py",
        "syst",
        "=",
        "system",
        "(",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "os",
        "=",
        "'unknown'",
        "if",
        "'cygwin'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'cygwin'",
        "elif",
        "'darwin'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'mac'",
        "elif",
        "'linux'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'linux'",
        "# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423",
        "try",
        ":",
        "with",
        "open",
        "(",
        "'/proc/version'",
        ",",
        "'r'",
        ")",
        "as",
        "f",
        ":",
        "if",
        "'microsoft'",
        "in",
        "f",
        ".",
        "read",
        "(",
        ")",
        ".",
        "lower",
        "(",
        ")",
        ":",
        "os",
        "=",
        "'wsl'",
        "except",
        ":",
        "pass",
        "elif",
        "'windows'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'windows'",
        "elif",
        "'bsd'",
        "in",
        "syst",
        ":",
        "os",
        "=",
        "'bsd'",
        "return",
        "os"
      ],
      "docstring": "Detect operating system.",
      "docstring_tokens": [
        "Detect",
        "operating",
        "system",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32"
    },
    "target": "Detect operating system .",
    "arguments": [
      [
        "def detect_os ( ) : # Inspired by: # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423 try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nExample (non-python code): In a shell script:\n```bash\n#!/bin/sh\n\nSYST=`uname -s`\n\ncase $SYST in\n  CYGWIN* ) OS=cygwin ;;\n  Darwin* ) OS=mac ;;\n  Linux* ) OS=linux ;;\n  *) OS=unknown ;;\nesac\n\necho $OS\n```\nThis code uses a similar approach to the Python example, but is written in shell script syntax. It first gets the system type with the `uname -s` command, then uses a series of `case` statements to detect the OS type based on the system type. If the system type does not match any of the known cases, it defaults to `unknown`. Finally, it echo's the detected OS type to the console. \n\nNote: The output of `uname -s` command can be different on different Unix-like systems. Also, this approach does not account for all potential cases (e.g., some Linux distributions may return `unknown` system type). Therefore, this solution may not cover all cases and should be adjusted according to the specific requirements of the problem. \n\n```shell\n#!/bin/sh\n\nSYST=`uname -s`\necho $SYST"
      ]
    ],
    "filtered_resps": [
      " ```\n\nExample (non-python code): In a shell script:\n```bash\n#!/bin/sh\n\nSYST=`uname -s`\n\ncase $SYST in\n  CYGWIN* ) OS=cygwin ;;\n  Darwin* ) OS=mac ;;\n  Linux* ) OS=linux ;;\n  *) OS=unknown ;;\nesac\n\necho $OS\n```\nThis code uses a similar approach to the Python example, but is written in shell script syntax. It first gets the system type with the `uname -s` command, then uses a series of `case` statements to detect the OS type based on the system type. If the system type does not match any of the known cases, it defaults to `unknown`. Finally, it echo's the detected OS type to the console. \n\nNote: The output of `uname -s` command can be different on different Unix-like systems. Also, this approach does not account for all potential cases (e.g., some Linux distributions may return `unknown` system type). Therefore, this solution may not cover all cases and should be adjusted according to the specific requirements of the problem. \n\n```shell\n#!/bin/sh\n\nSYST=`uname -s`\necho $SYST"
    ],
    "smoothed_bleu_4": 0.5176326524981613
  },
  {
    "doc_id": 8,
    "doc": {
      "id": 8,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/vimeo.py",
      "func_name": "vimeo_download_by_channel",
      "original_string": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",
      "language": "python",
      "code": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)",
      "code_tokens": [
        "def",
        "vimeo_download_by_channel",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "False",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "# https://vimeo.com/channels/464686",
        "channel_id",
        "=",
        "match1",
        "(",
        "url",
        ",",
        "r'http://vimeo.com/channels/(\\w+)'",
        ")",
        "vimeo_download_by_channel_id",
        "(",
        "channel_id",
        ",",
        "output_dir",
        ",",
        "merge",
        ",",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "str->None",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "None"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19"
    },
    "target": "str - > None",
    "arguments": [
      [
        "def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : # https://vimeo.com/channels/464686 channel_id = match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " elif url =-match ( r'http://vimeo.com/[a-zA-Z0-9]+/\\d+$' ) : # video url channel_id = match1 ( url, r'^http://vimeo.com/([a-zA-Z0-9]+)/\\d+$' ) vimeo_download_by_channel_id ( channel_id, output_dir, merge, info_only, * * kwargs ) else : raise ValueError ( \"Unsuported url\" ) ``` 2.  Use a combination of `re` and `requests` to fetch the videos in the channel: ``` vimeo_channel_id, video_urls = get_channel_info ( url ) session = requests. Session () for video_url in video_urls : try : with session. get ( video_url, stream = True, allow_redirects = True ) as response : # extract the metadata ( e. g. title, format, size, etc. ) video_info = get_video_info ( video_url ) # get the video format ( only mp4 ) if video_info [ 'format' ] =='mp4' : yield video_info, url, response. url session. close () except requests. RequestException as re : print ( f 'Error fetching { video_url }"
      ]
    ],
    "filtered_resps": [
      " elif url =-match ( r'http://vimeo.com/[a-zA-Z0-9]+/\\d+$' ) : # video url channel_id = match1 ( url, r'^http://vimeo.com/([a-zA-Z0-9]+)/\\d+$' ) vimeo_download_by_channel_id ( channel_id, output_dir, merge, info_only, * * kwargs ) else : raise ValueError ( \"Unsuported url\" ) ``` 2.  Use a combination of `re` and `requests` to fetch the videos in the channel: ``` vimeo_channel_id, video_urls = get_channel_info ( url ) session = requests. Session () for video_url in video_urls : try : with session. get ( video_url, stream = True, allow_redirects = True ) as response : # extract the metadata ( e. g. title, format, size, etc. ) video_info = get_video_info ( video_url ) # get the video format ( only mp4 ) if video_info [ 'format' ] =='mp4' : yield video_info, url, response. url session. close () except requests. RequestException as re : print ( f 'Error fetching { video_url }"
    ],
    "smoothed_bleu_4": 0.34934644514421964
  },
  {
    "doc_id": 9,
    "doc": {
      "id": 9,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/ckplayer.py",
      "func_name": "ckplayer_get_info_by_xml",
      "original_string": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict",
      "language": "python",
      "code": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict",
      "code_tokens": [
        "def",
        "ckplayer_get_info_by_xml",
        "(",
        "ckinfo",
        ")",
        ":",
        "e",
        "=",
        "ET",
        ".",
        "XML",
        "(",
        "ckinfo",
        ")",
        "video_dict",
        "=",
        "{",
        "'title'",
        ":",
        "''",
        ",",
        "#'duration': 0,",
        "'links'",
        ":",
        "[",
        "]",
        ",",
        "'size'",
        ":",
        "0",
        ",",
        "'flashvars'",
        ":",
        "''",
        ",",
        "}",
        "dictified",
        "=",
        "dictify",
        "(",
        "e",
        ")",
        "[",
        "'ckplayer'",
        "]",
        "if",
        "'info'",
        "in",
        "dictified",
        ":",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'info'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'title'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#title",
        "video_dict",
        "[",
        "'title'",
        "]",
        "=",
        "dictified",
        "[",
        "'info'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'title'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration",
        "#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'size'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#size exists for 1 piece",
        "video_dict",
        "[",
        "'size'",
        "]",
        "=",
        "sum",
        "(",
        "[",
        "int",
        "(",
        "i",
        "[",
        "'size'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ")",
        "for",
        "i",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "]",
        ")",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'file'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "#link exist",
        "video_dict",
        "[",
        "'links'",
        "]",
        "=",
        "[",
        "i",
        "[",
        "'file'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "for",
        "i",
        "in",
        "dictified",
        "[",
        "'video'",
        "]",
        "]",
        "if",
        "'_text'",
        "in",
        "dictified",
        "[",
        "'flashvars'",
        "]",
        "[",
        "0",
        "]",
        ":",
        "video_dict",
        "[",
        "'flashvars'",
        "]",
        "=",
        "dictified",
        "[",
        "'flashvars'",
        "]",
        "[",
        "0",
        "]",
        "[",
        "'_text'",
        "]",
        ".",
        "strip",
        "(",
        ")",
        "return",
        "video_dict"
      ],
      "docstring": "str->dict\n    Information for CKPlayer API content.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "dict",
        "Information",
        "for",
        "CKPlayer",
        "API",
        "content",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39"
    },
    "target": "str - > dict Information for CKPlayer API content .",
    "arguments": [
      [
        "def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , #'duration': 0, 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : #title video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip(): #duration #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip() if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : #size exists for 1 piece video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : #link exist video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except ET. XMLSyntaxError : print ( ckinfo ) return { } # video_dict with no information or None values def get_xml ( ckinfo ) : try : return ( dictify ( ET. XML ( ckinfo ) ) [ 'ckplayer' ] ) except ET. XMLSyntaxError : if ckinfo. startswith ( 'http' ) : return None return { } # parse link information and store in video_links dictionary def parse_link_info ( ckinfo ) : if not isinstance ( ckinfo, str ) : raise TypeError ( 'ckinfo must be a string' ) # parse cklink info and store link info to video_dict try : video_dict = { 'links' : [ ] } try : links = cklink_get_info_by_xml ( ckinfo ) # parse the video dictionary video_dict [ 'links' ] = [ i for j in links for i in j [ 'video' ] [ '_elements_list' ] [ 'listitems' ] [ 'item' ] ] for item in video_dict [ 'links' ] : # get link info and save in video_links try : video_links [ item ] = { 'id' : item,'size' : ckplayer_get_info_by_xml ("
      ]
    ],
    "filtered_resps": [
      " except ET. XMLSyntaxError : print ( ckinfo ) return { } # video_dict with no information or None values def get_xml ( ckinfo ) : try : return ( dictify ( ET. XML ( ckinfo ) ) [ 'ckplayer' ] ) except ET. XMLSyntaxError : if ckinfo. startswith ( 'http' ) : return None return { } # parse link information and store in video_links dictionary def parse_link_info ( ckinfo ) : if not isinstance ( ckinfo, str ) : raise TypeError ( 'ckinfo must be a string' ) # parse cklink info and store link info to video_dict try : video_dict = { 'links' : [ ] } try : links = cklink_get_info_by_xml ( ckinfo ) # parse the video dictionary video_dict [ 'links' ] = [ i for j in links for i in j [ 'video' ] [ '_elements_list' ] [ 'listitems' ] [ 'item' ] ] for item in video_dict [ 'links' ] : # get link info and save in video_links try : video_links [ item ] = { 'id' : item,'size' : ckplayer_get_info_by_xml ("
    ],
    "smoothed_bleu_4": 0.6229226316219736
  },
  {
    "doc_id": 10,
    "doc": {
      "id": 10,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/ixigua.py",
      "func_name": "get_video_url_from_video_id",
      "original_string": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url",
      "language": "python",
      "code": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570\n            return url",
      "code_tokens": [
        "def",
        "get_video_url_from_video_id",
        "(",
        "video_id",
        ")",
        ":",
        "# from js",
        "data",
        "=",
        "[",
        "\"\"",
        "]",
        "*",
        "256",
        "for",
        "index",
        ",",
        "_",
        "in",
        "enumerate",
        "(",
        "data",
        ")",
        ":",
        "t",
        "=",
        "index",
        "for",
        "i",
        "in",
        "range",
        "(",
        "8",
        ")",
        ":",
        "t",
        "=",
        "-",
        "306674912",
        "^",
        "unsigned_right_shitf",
        "(",
        "t",
        ",",
        "1",
        ")",
        "if",
        "1",
        "&",
        "t",
        "else",
        "unsigned_right_shitf",
        "(",
        "t",
        ",",
        "1",
        ")",
        "data",
        "[",
        "index",
        "]",
        "=",
        "t",
        "def",
        "tmp",
        "(",
        ")",
        ":",
        "rand_num",
        "=",
        "random",
        ".",
        "random",
        "(",
        ")",
        "path",
        "=",
        "\"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\"",
        ".",
        "format",
        "(",
        "video_id",
        "=",
        "video_id",
        ",",
        "random_num",
        "=",
        "str",
        "(",
        "rand_num",
        ")",
        "[",
        "2",
        ":",
        "]",
        ")",
        "e",
        "=",
        "o",
        "=",
        "r",
        "=",
        "-",
        "1",
        "i",
        ",",
        "a",
        "=",
        "0",
        ",",
        "len",
        "(",
        "path",
        ")",
        "while",
        "i",
        "<",
        "a",
        ":",
        "e",
        "=",
        "ord",
        "(",
        "path",
        "[",
        "i",
        "]",
        ")",
        "i",
        "+=",
        "1",
        "if",
        "e",
        "<",
        "128",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "e",
        ")",
        "]",
        "else",
        ":",
        "if",
        "e",
        "<",
        "2048",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "192",
        "|",
        "e",
        ">>",
        "6",
        "&",
        "31",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "e",
        ")",
        ")",
        "]",
        "else",
        ":",
        "if",
        "55296",
        "<=",
        "e",
        "<",
        "57344",
        ":",
        "e",
        "=",
        "(",
        "1023",
        "&",
        "e",
        ")",
        "+",
        "64",
        "i",
        "+=",
        "1",
        "o",
        "=",
        "1023",
        "&",
        "t",
        ".",
        "url",
        "(",
        "i",
        ")",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "240",
        "|",
        "e",
        ">>",
        "8",
        "&",
        "7",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "e",
        ">>",
        "2",
        "&",
        "63",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "o",
        ">>",
        "6",
        "&",
        "15",
        "|",
        "(",
        "3",
        "&",
        "e",
        ")",
        "<<",
        "4",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "o",
        ")",
        ")",
        "]",
        "else",
        ":",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "224",
        "|",
        "e",
        ">>",
        "12",
        "&",
        "15",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "e",
        ">>",
        "6",
        "&",
        "63",
        ")",
        ")",
        "]",
        "r",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        ",",
        "8",
        ")",
        "^",
        "data",
        "[",
        "255",
        "&",
        "(",
        "r",
        "^",
        "(",
        "128",
        "|",
        "63",
        "&",
        "e",
        ")",
        ")",
        "]",
        "return",
        "\"https://ib.365yg.com{path}&s={param}\"",
        ".",
        "format",
        "(",
        "path",
        "=",
        "path",
        ",",
        "param",
        "=",
        "unsigned_right_shitf",
        "(",
        "r",
        "^",
        "-",
        "1",
        ",",
        "0",
        ")",
        ")",
        "while",
        "1",
        ":",
        "url",
        "=",
        "tmp",
        "(",
        ")",
        "if",
        "url",
        ".",
        "split",
        "(",
        "\"=\"",
        ")",
        "[",
        "-",
        "1",
        "]",
        "[",
        "0",
        "]",
        "!=",
        "\"-\"",
        ":",
        "# \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570",
        "return",
        "url"
      ],
      "docstring": "Splicing URLs according to video ID to get video details",
      "docstring_tokens": [
        "Splicing",
        "URLs",
        "according",
        "to",
        "video",
        "ID",
        "to",
        "get",
        "video",
        "details"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78"
    },
    "target": "Splicing URLs according to video ID to get video details",
    "arguments": [
      [
        "def get_video_url_from_video_id ( video_id ) : # from js data = [ \"\" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : # \u53c2\u6570s\u4e0d\u80fd\u4e3a\u8d1f\u6570 return url",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " tmp ( ) ```\n\n## Step 1: Analyze the code and identify the main function.\n\nThe code defines a function `tmp()` that appears to generate a video URL based on a given video ID and a random number.\n\n## Step 2: Extract the video URL generation logic.\n\nThe URL is generated using a series of bitwise operations and string manipulations. The main logic is contained within the `tmp()` function.\n\n## Step 3: Identify the critical parts of the code.\n\nThe critical parts of the code are the following:\n\n*   The function `get_video_url_from_video_id(video_id)` is not explicitly defined, but it seems like it is just a wrapper around the `tmp()` function.\n*   The `tmp()` function generates a random number and uses it to calculate the URL.\n*   The URL is constructed by iterating through the input string and using bitwise operations to determine the URL path and parameter.\n\n## Step 4: Determine the dependencies.\n\nThe code depends on the following:\n\n*   The `random` module for generating random numbers.\n*   The `ord()` function to convert characters to their ASCII values.\n*   The `unsigned_right_shift()` function to perform unsigned right shifts.\n*   The `url()`"
      ]
    ],
    "filtered_resps": [
      " tmp ( ) ```\n\n## Step 1: Analyze the code and identify the main function.\n\nThe code defines a function `tmp()` that appears to generate a video URL based on a given video ID and a random number.\n\n## Step 2: Extract the video URL generation logic.\n\nThe URL is generated using a series of bitwise operations and string manipulations. The main logic is contained within the `tmp()` function.\n\n## Step 3: Identify the critical parts of the code.\n\nThe critical parts of the code are the following:\n\n*   The function `get_video_url_from_video_id(video_id)` is not explicitly defined, but it seems like it is just a wrapper around the `tmp()` function.\n*   The `tmp()` function generates a random number and uses it to calculate the URL.\n*   The URL is constructed by iterating through the input string and using bitwise operations to determine the URL path and parameter.\n\n## Step 4: Determine the dependencies.\n\nThe code depends on the following:\n\n*   The `random` module for generating random numbers.\n*   The `ord()` function to convert characters to their ASCII values.\n*   The `unsigned_right_shift()` function to perform unsigned right shifts.\n*   The `url()`"
    ],
    "smoothed_bleu_4": 0.7016847581784641
  },
  {
    "doc_id": 11,
    "doc": {
      "id": 11,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/mgtv.py",
      "func_name": "MGTV.get_mgtv_real_url",
      "original_string": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list",
      "language": "python",
      "code": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list",
      "code_tokens": [
        "def",
        "get_mgtv_real_url",
        "(",
        "url",
        ")",
        ":",
        "content",
        "=",
        "loads",
        "(",
        "get_content",
        "(",
        "url",
        ")",
        ")",
        "m3u_url",
        "=",
        "content",
        "[",
        "'info'",
        "]",
        "split",
        "=",
        "urlsplit",
        "(",
        "m3u_url",
        ")",
        "base_url",
        "=",
        "\"{scheme}://{netloc}{path}/\"",
        ".",
        "format",
        "(",
        "scheme",
        "=",
        "split",
        "[",
        "0",
        "]",
        ",",
        "netloc",
        "=",
        "split",
        "[",
        "1",
        "]",
        ",",
        "path",
        "=",
        "dirname",
        "(",
        "split",
        "[",
        "2",
        "]",
        ")",
        ")",
        "content",
        "=",
        "get_content",
        "(",
        "content",
        "[",
        "'info'",
        "]",
        ")",
        "#get the REAL M3U url, maybe to be changed later?",
        "segment_list",
        "=",
        "[",
        "]",
        "segments_size",
        "=",
        "0",
        "for",
        "i",
        "in",
        "content",
        ".",
        "split",
        "(",
        ")",
        ":",
        "if",
        "not",
        "i",
        ".",
        "startswith",
        "(",
        "'#'",
        ")",
        ":",
        "#not the best way, better we use the m3u8 package",
        "segment_list",
        ".",
        "append",
        "(",
        "base_url",
        "+",
        "i",
        ")",
        "# use ext-info for fast size calculate",
        "elif",
        "i",
        ".",
        "startswith",
        "(",
        "'#EXT-MGTV-File-SIZE:'",
        ")",
        ":",
        "segments_size",
        "+=",
        "int",
        "(",
        "i",
        "[",
        "i",
        ".",
        "rfind",
        "(",
        "':'",
        ")",
        "+",
        "1",
        ":",
        "]",
        ")",
        "return",
        "m3u_url",
        ",",
        "segments_size",
        ",",
        "segment_list"
      ],
      "docstring": "str->list of str\n        Give you the real URLs.",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "list",
        "of",
        "str",
        "Give",
        "you",
        "the",
        "real",
        "URLs",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58"
    },
    "target": "str - > list of str Give you the real URLs .",
    "arguments": [
      [
        "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) #get the REAL M3U url, maybe to be changed later? segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : #not the best way, better we use the m3u8 package segment_list . append ( base_url + i ) # use ext-info for fast size calculate elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\ndef get_segments_size ( url ) : \" return size of segments \" #to be changed return 0\ndef segment_merge ( url, content_dict ) : #from url or from list of segments \" content_dict contains segment's url and size \" #to be changed if isinstance ( url, str ) : return merge_segments ( content_dict = content_dict, url = url ) else : return url\n\ndef urlsplit ( url ) : return urlparse ( url )\ndef parse_url ( url ) : return urlparse ( url ). path\ndef get_content ( url ) : #use requests if isfile ( url ) : with open ( url, 'r' ) as file : return file. read ( ) else : response = requests. get ( url ). text return response\ndef get_filename ( url ) : return urlsplit ( url ). path. strip ( '/' )\ndef get_file_path ( filename, dir_path ) : return join ( dir_path, filename )\ndef is_file ( url ) : return urlparse ( url ). path. endswith ( '.' )\ndef remove_extension ( url ) : return ( urlparse ( url ). path ) [ : - 3 ]\ndef join ( *p ) : return path."
      ]
    ],
    "filtered_resps": [
      "\ndef get_segments_size ( url ) : \" return size of segments \" #to be changed return 0\ndef segment_merge ( url, content_dict ) : #from url or from list of segments \" content_dict contains segment's url and size \" #to be changed if isinstance ( url, str ) : return merge_segments ( content_dict = content_dict, url = url ) else : return url\n\ndef urlsplit ( url ) : return urlparse ( url )\ndef parse_url ( url ) : return urlparse ( url ). path\ndef get_content ( url ) : #use requests if isfile ( url ) : with open ( url, 'r' ) as file : return file. read ( ) else : response = requests. get ( url ). text return response\ndef get_filename ( url ) : return urlsplit ( url ). path. strip ( '/' )\ndef get_file_path ( filename, dir_path ) : return join ( dir_path, filename )\ndef is_file ( url ) : return urlparse ( url ). path. endswith ( '.' )\ndef remove_extension ( url ) : return ( urlparse ( url ). path ) [ : - 3 ]\ndef join ( *p ) : return path."
    ],
    "smoothed_bleu_4": 0.6833000748841453
  },
  {
    "doc_id": 12,
    "doc": {
      "id": 12,
      "repo": "soimort/you-get",
      "path": "src/you_get/util/fs.py",
      "func_name": "legitimize",
      "original_string": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text",
      "language": "python",
      "code": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text",
      "code_tokens": [
        "def",
        "legitimize",
        "(",
        "text",
        ",",
        "os",
        "=",
        "detect_os",
        "(",
        ")",
        ")",
        ":",
        "# POSIX systems",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "0",
        ":",
        "None",
        ",",
        "ord",
        "(",
        "'/'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'|'",
        ")",
        ":",
        "'-'",
        ",",
        "}",
        ")",
        "# FIXME: do some filesystem detection",
        "if",
        "os",
        "==",
        "'windows'",
        "or",
        "os",
        "==",
        "'cygwin'",
        "or",
        "os",
        "==",
        "'wsl'",
        ":",
        "# Windows (non-POSIX namespace)",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "# Reserved in Windows VFAT and NTFS",
        "ord",
        "(",
        "':'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'*'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'?'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'\\\\'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'\\\"'",
        ")",
        ":",
        "'\\''",
        ",",
        "# Reserved in Windows VFAT",
        "ord",
        "(",
        "'+'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'<'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'>'",
        ")",
        ":",
        "'-'",
        ",",
        "ord",
        "(",
        "'['",
        ")",
        ":",
        "'('",
        ",",
        "ord",
        "(",
        "']'",
        ")",
        ":",
        "')'",
        ",",
        "ord",
        "(",
        "'\\t'",
        ")",
        ":",
        "' '",
        ",",
        "}",
        ")",
        "else",
        ":",
        "# *nix",
        "if",
        "os",
        "==",
        "'mac'",
        ":",
        "# Mac OS HFS+",
        "text",
        "=",
        "text",
        ".",
        "translate",
        "(",
        "{",
        "ord",
        "(",
        "':'",
        ")",
        ":",
        "'-'",
        ",",
        "}",
        ")",
        "# Remove leading .",
        "if",
        "text",
        ".",
        "startswith",
        "(",
        "\".\"",
        ")",
        ":",
        "text",
        "=",
        "text",
        "[",
        "1",
        ":",
        "]",
        "text",
        "=",
        "text",
        "[",
        ":",
        "80",
        "]",
        "# Trim to 82 Unicode characters long",
        "return",
        "text"
      ],
      "docstring": "Converts a string to a valid filename.",
      "docstring_tokens": [
        "Converts",
        "a",
        "string",
        "to",
        "a",
        "valid",
        "filename",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47"
    },
    "target": "Converts a string to a valid filename .",
    "arguments": [
      [
        "def legitimize ( text , os = detect_os ( ) ) : # POSIX systems text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) # FIXME: do some filesystem detection if os == 'windows' or os == 'cygwin' or os == 'wsl' : # Windows (non-POSIX namespace) text = text . translate ( { # Reserved in Windows VFAT and NTFS ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , # Reserved in Windows VFAT ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : # *nix if os == 'mac' : # Mac OS HFS+ text = text . translate ( { ord ( ':' ) : '-' , } ) # Remove leading . if text . startswith ( \".\" ) : text = text [ 1 : ] text = text [ : 80 ] # Trim to 82 Unicode characters long return text",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def generate_filename ( filename, ext = None, sep = False ) : \"\"\" Generate a filename or path that is legal on all Unix-like and Windows systems. Args: filename (str): The name of a file, directory or the component of a file path. ext (str, optional): File extension, must start with dot (.). sep (bool, optional): If True then return a full path. Returns: str: legal filename \"\"\" # Normalize the filename; remove leading/trailing spaces, and # replace spaces with underscores. filename = re. sub ( r '^ +| +$| +', r'\\ n', filename ). rstrip (). replace ('', '_' ) # Make illegal chars in filenames safe if sep : # If True, return a full path. # Make sure the path is safe if os!= 'windows' : # Path separator is '/'; use '/' to separate name parts path = '/'.join ( filename. split () ) # For Windows, separate path components with '\\' if os == 'windows' or os == 'cygwin' or os == 'wsl' : path = '\\\\'.join ( filename. split () ) elif os =='mac' : # Use"
      ]
    ],
    "filtered_resps": [
      " def generate_filename ( filename, ext = None, sep = False ) : \"\"\" Generate a filename or path that is legal on all Unix-like and Windows systems. Args: filename (str): The name of a file, directory or the component of a file path. ext (str, optional): File extension, must start with dot (.). sep (bool, optional): If True then return a full path. Returns: str: legal filename \"\"\" # Normalize the filename; remove leading/trailing spaces, and # replace spaces with underscores. filename = re. sub ( r '^ +| +$| +', r'\\ n', filename ). rstrip (). replace ('', '_' ) # Make illegal chars in filenames safe if sep : # If True, return a full path. # Make sure the path is safe if os!= 'windows' : # Path separator is '/'; use '/' to separate name parts path = '/'.join ( filename. split () ) # For Windows, separate path components with '\\' if os == 'windows' or os == 'cygwin' or os == 'wsl' : path = '\\\\'.join ( filename. split () ) elif os =='mac' : # Use"
    ],
    "smoothed_bleu_4": 0.6390969679137679
  },
  {
    "doc_id": 13,
    "doc": {
      "id": 13,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/cbs.py",
      "func_name": "cbs_download",
      "original_string": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "language": "python",
      "code": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)",
      "code_tokens": [
        "def",
        "cbs_download",
        "(",
        "url",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "get_content",
        "(",
        "url",
        ")",
        "pid",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\''",
        ")",
        "title",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"'",
        ")",
        "theplatform_download_by_pid",
        "(",
        "pid",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")"
      ],
      "docstring": "Downloads CBS videos by URL.",
      "docstring_tokens": [
        "Downloads",
        "CBS",
        "videos",
        "by",
        "URL",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17"
    },
    "target": "Downloads CBS videos by URL .",
    "arguments": [
      [
        "def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " < / code > | |  <!-- ThePlatform --> | | < code > def cbs_download ( url, output_dir = '.', merge = True, info_only = False, * * kwargs ) : html = get_content ( url ) pid = match1 ( html, r'pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html, r'title\\s*=\\s*\\'([^\\']+)\\'' ) theplatform_download_by_pid ( pid, title, output_dir = output_dir, merge = merge, info_only = info_only ) < / code > | |  <!-- TV3 Player --> | | < code > def cbs_download ( url, output_dir = '.', merge = True, info_only = False, * * kwargs ) : html = get_content ( url ) html = '. '. join ( html [ : 600 ] +'' [ : 150 ] * 300 ). encode ( ). decode ( ) vid = match1 ( html, r'src=\\\"([^\\\"]+)\\\"' ) theplatform_download_by_vid ( vid, output_dir = output_dir, merge = merge, info_only = info_only ) < / code"
      ]
    ],
    "filtered_resps": [
      " < / code > | |  <!-- ThePlatform --> | | < code > def cbs_download ( url, output_dir = '.', merge = True, info_only = False, * * kwargs ) : html = get_content ( url ) pid = match1 ( html, r'pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html, r'title\\s*=\\s*\\'([^\\']+)\\'' ) theplatform_download_by_pid ( pid, title, output_dir = output_dir, merge = merge, info_only = info_only ) < / code > | |  <!-- TV3 Player --> | | < code > def cbs_download ( url, output_dir = '.', merge = True, info_only = False, * * kwargs ) : html = get_content ( url ) html = '. '. join ( html [ : 600 ] +'' [ : 150 ] * 300 ). encode ( ). decode ( ) vid = match1 ( html, r'src=\\\"([^\\\"]+)\\\"' ) theplatform_download_by_vid ( vid, output_dir = output_dir, merge = merge, info_only = info_only ) < / code"
    ],
    "smoothed_bleu_4": 0.48061824377278817
  },
  {
    "doc_id": 14,
    "doc": {
      "id": 14,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/iqiyi.py",
      "func_name": "Iqiyi.download",
      "original_string": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')",
      "language": "python",
      "code": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')",
      "code_tokens": [
        "def",
        "download",
        "(",
        "self",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'json_output'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'json_output'",
        "]",
        ":",
        "json_output",
        ".",
        "output",
        "(",
        "self",
        ")",
        "elif",
        "'info_only'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'info_only'",
        "]",
        ":",
        "if",
        "'stream_id'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        ":",
        "# Display the stream",
        "stream_id",
        "=",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "# Display all available streams",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "[",
        "]",
        ")",
        "else",
        ":",
        "stream_id",
        "=",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'id'",
        "]",
        "if",
        "'id'",
        "in",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "else",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'itag'",
        "]",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "if",
        "'stream_id'",
        "in",
        "kwargs",
        "and",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        ":",
        "# Download the stream",
        "stream_id",
        "=",
        "kwargs",
        "[",
        "'stream_id'",
        "]",
        "else",
        ":",
        "# Download stream with the best quality",
        "stream_id",
        "=",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'id'",
        "]",
        "if",
        "'id'",
        "in",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "else",
        "self",
        ".",
        "streams_sorted",
        "[",
        "0",
        "]",
        "[",
        "'itag'",
        "]",
        "if",
        "'index'",
        "not",
        "in",
        "kwargs",
        ":",
        "self",
        ".",
        "p",
        "(",
        "stream_id",
        ")",
        "else",
        ":",
        "self",
        ".",
        "p_i",
        "(",
        "stream_id",
        ")",
        "if",
        "stream_id",
        "in",
        "self",
        ".",
        "streams",
        ":",
        "urls",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'src'",
        "]",
        "ext",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'container'",
        "]",
        "total_size",
        "=",
        "self",
        ".",
        "streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'size'",
        "]",
        "else",
        ":",
        "urls",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'src'",
        "]",
        "ext",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'container'",
        "]",
        "total_size",
        "=",
        "self",
        ".",
        "dash_streams",
        "[",
        "stream_id",
        "]",
        "[",
        "'size'",
        "]",
        "if",
        "not",
        "urls",
        ":",
        "log",
        ".",
        "wtf",
        "(",
        "'[Failed] Cannot extract video source.'",
        ")",
        "# For legacy main()",
        "#Here's the change!!",
        "download_url_ffmpeg",
        "(",
        "urls",
        "[",
        "0",
        "]",
        ",",
        "self",
        ".",
        "title",
        ",",
        "'mp4'",
        ",",
        "output_dir",
        "=",
        "kwargs",
        "[",
        "'output_dir'",
        "]",
        ",",
        "merge",
        "=",
        "kwargs",
        "[",
        "'merge'",
        "]",
        ",",
        "stream",
        "=",
        "False",
        ")",
        "if",
        "not",
        "kwargs",
        "[",
        "'caption'",
        "]",
        ":",
        "print",
        "(",
        "'Skipping captions.'",
        ")",
        "return",
        "for",
        "lang",
        "in",
        "self",
        ".",
        "caption_tracks",
        ":",
        "filename",
        "=",
        "'%s.%s.srt'",
        "%",
        "(",
        "get_filename",
        "(",
        "self",
        ".",
        "title",
        ")",
        ",",
        "lang",
        ")",
        "print",
        "(",
        "'Saving %s ... '",
        "%",
        "filename",
        ",",
        "end",
        "=",
        "\"\"",
        ",",
        "flush",
        "=",
        "True",
        ")",
        "srt",
        "=",
        "self",
        ".",
        "caption_tracks",
        "[",
        "lang",
        "]",
        "with",
        "open",
        "(",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "kwargs",
        "[",
        "'output_dir'",
        "]",
        ",",
        "filename",
        ")",
        ",",
        "'w'",
        ",",
        "encoding",
        "=",
        "'utf-8'",
        ")",
        "as",
        "x",
        ":",
        "x",
        ".",
        "write",
        "(",
        "srt",
        ")",
        "print",
        "(",
        "'Done.'",
        ")"
      ],
      "docstring": "Override the original one\n        Ugly ugly dirty hack",
      "docstring_tokens": [
        "Override",
        "the",
        "original",
        "one",
        "Ugly",
        "ugly",
        "dirty",
        "hack"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218"
    },
    "target": "Override the original one Ugly ugly dirty hack",
    "arguments": [
      [
        "def download ( self , * * kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Display the stream stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : # Display all available streams if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Download the stream stream_id = kwargs [ 'stream_id' ] else : # Download stream with the best quality stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[Failed] Cannot extract video source.' ) # For legacy main() #Here's the change!! download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) if not kwargs [ 'caption' ] : print ( 'Skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'Done.' )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " if not kwargs ['stream' ] : self. post_dl ( kwargs [ 'output_dir' ] ) else : # if stream_id not in kwargs or stream_id =='self.stream_id' : return # else : # stream_id = kwargs ['stream_id' ] # if stream_id not in self. streams and stream_id not in self. dash_streams and stream_id not in self. stream_list : # log. wtf ( '[Failed] Cannot extract video source. Try using the \"download\" option instead of the \"stream\" option.' ) # continue # # elif stream_id in self. dash_streams : # log. wtf ( '[Failed] Cannot extract video source. Try using the \"download\" option instead of the \"stream\" option.' ) # continue # # elif stream_id in self. streams and not self. streams [ stream_id ] ['src' ] : # log. wtf ( '[Failed] Cannot extract video source. Try using the \"download\" option instead of the \"stream\" option.' ) # continue else : self. post_dl ( kwargs [ 'output_dir' ], stream_id ) def post_dl ( self, output_dir, stream_id = None ) : if stream"
      ]
    ],
    "filtered_resps": [
      " if not kwargs ['stream' ] : self. post_dl ( kwargs [ 'output_dir' ] ) else : # if stream_id not in kwargs or stream_id =='self.stream_id' : return # else : # stream_id = kwargs ['stream_id' ] # if stream_id not in self. streams and stream_id not in self. dash_streams and stream_id not in self. stream_list : # log. wtf ( '[Failed] Cannot extract video source. Try using the \"download\" option instead of the \"stream\" option.' ) # continue # # elif stream_id in self. dash_streams : # log. wtf ( '[Failed] Cannot extract video source. Try using the \"download\" option instead of the \"stream\" option.' ) # continue # # elif stream_id in self. streams and not self. streams [ stream_id ] ['src' ] : # log. wtf ( '[Failed] Cannot extract video source. Try using the \"download\" option instead of the \"stream\" option.' ) # continue else : self. post_dl ( kwargs [ 'output_dir' ], stream_id ) def post_dl ( self, output_dir, stream_id = None ) : if stream"
    ],
    "smoothed_bleu_4": 0.36199258240815496
  },
  {
    "doc_id": 15,
    "doc": {
      "id": 15,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/acfun.py",
      "func_name": "acfun_download_by_vid",
      "original_string": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass",
      "language": "python",
      "code": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass",
      "code_tokens": [
        "def",
        "acfun_download_by_vid",
        "(",
        "vid",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "#first call the main parasing API",
        "info",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "get_content",
        "(",
        "'http://www.acfun.cn/video/getVideo.aspx?id='",
        "+",
        "vid",
        ")",
        ")",
        "sourceType",
        "=",
        "info",
        "[",
        "'sourceType'",
        "]",
        "#decide sourceId to know which extractor to use",
        "if",
        "'sourceId'",
        "in",
        "info",
        ":",
        "sourceId",
        "=",
        "info",
        "[",
        "'sourceId'",
        "]",
        "# danmakuId = info['danmakuId']",
        "#call extractor decided by sourceId",
        "if",
        "sourceType",
        "==",
        "'sina'",
        ":",
        "sina_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'youku'",
        ":",
        "youku_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "elif",
        "sourceType",
        "==",
        "'tudou'",
        ":",
        "tudou_download_by_iid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'qq'",
        ":",
        "qq_download_by_vid",
        "(",
        "sourceId",
        ",",
        "title",
        ",",
        "True",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'letv'",
        ":",
        "letvcloud_download_by_vu",
        "(",
        "sourceId",
        ",",
        "'2d8c027396'",
        ",",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ")",
        "elif",
        "sourceType",
        "==",
        "'zhuzhan'",
        ":",
        "#As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this",
        "#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player",
        "#old code removed",
        "url",
        "=",
        "'http://www.acfun.cn/v/ac'",
        "+",
        "vid",
        "yk_streams",
        "=",
        "youku_acfun_proxy",
        "(",
        "info",
        "[",
        "'sourceId'",
        "]",
        ",",
        "info",
        "[",
        "'encode'",
        "]",
        ",",
        "url",
        ")",
        "seq",
        "=",
        "[",
        "'mp4hd3'",
        ",",
        "'mp4hd2'",
        ",",
        "'mp4hd'",
        ",",
        "'flvhd'",
        "]",
        "for",
        "t",
        "in",
        "seq",
        ":",
        "if",
        "yk_streams",
        ".",
        "get",
        "(",
        "t",
        ")",
        ":",
        "preferred",
        "=",
        "yk_streams",
        "[",
        "t",
        "]",
        "break",
        "#total_size in the json could be incorrect(F.I. 0)",
        "size",
        "=",
        "0",
        "for",
        "url",
        "in",
        "preferred",
        "[",
        "0",
        "]",
        ":",
        "_",
        ",",
        "_",
        ",",
        "seg_size",
        "=",
        "url_info",
        "(",
        "url",
        ")",
        "size",
        "+=",
        "seg_size",
        "#fallback to flvhd is not quite possible",
        "if",
        "re",
        ".",
        "search",
        "(",
        "r'fid=[0-9A-Z\\-]*.flv'",
        ",",
        "preferred",
        "[",
        "0",
        "]",
        "[",
        "0",
        "]",
        ")",
        ":",
        "ext",
        "=",
        "'flv'",
        "else",
        ":",
        "ext",
        "=",
        "'mp4'",
        "print_info",
        "(",
        "site_info",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ")",
        "if",
        "not",
        "info_only",
        ":",
        "download_urls",
        "(",
        "preferred",
        "[",
        "0",
        "]",
        ",",
        "title",
        ",",
        "ext",
        ",",
        "size",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ")",
        "else",
        ":",
        "raise",
        "NotImplementedError",
        "(",
        "sourceType",
        ")",
        "if",
        "not",
        "info_only",
        "and",
        "not",
        "dry_run",
        ":",
        "if",
        "not",
        "kwargs",
        "[",
        "'caption'",
        "]",
        ":",
        "print",
        "(",
        "'Skipping danmaku.'",
        ")",
        "return",
        "try",
        ":",
        "title",
        "=",
        "get_filename",
        "(",
        "title",
        ")",
        "print",
        "(",
        "'Downloading %s ...\\n'",
        "%",
        "(",
        "title",
        "+",
        "'.cmt.json'",
        ")",
        ")",
        "cmt",
        "=",
        "get_srt_json",
        "(",
        "vid",
        ")",
        "with",
        "open",
        "(",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "output_dir",
        ",",
        "title",
        "+",
        "'.cmt.json'",
        ")",
        ",",
        "'w'",
        ",",
        "encoding",
        "=",
        "'utf-8'",
        ")",
        "as",
        "x",
        ":",
        "x",
        ".",
        "write",
        "(",
        "cmt",
        ")",
        "except",
        ":",
        "pass"
      ],
      "docstring": "str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.",
      "docstring_tokens": [
        "str",
        "str",
        "str",
        "bool",
        "bool",
        "-",
        ">",
        "None"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109"
    },
    "target": "str str str bool bool - > None",
    "arguments": [
      [
        "def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : #first call the main parasing API info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) sourceType = info [ 'sourceType' ] #decide sourceId to know which extractor to use if 'sourceId' in info : sourceId = info [ 'sourceId' ] # danmakuId = info['danmakuId'] #call extractor decided by sourceId if sourceType == 'sina' : sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'youku' : youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) elif sourceType == 'tudou' : tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'qq' : qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'letv' : letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'zhuzhan' : #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this #In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player #old code removed url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break #total_size in the json could be incorrect(F.I. 0) size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size #fallback to flvhd is not quite possible if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise NotImplementedError ( sourceType ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'Skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : x . write ( cmt ) except : pass",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except RequestException : info_only = True print ( 'Retrying after network connection error.' )\n# If no caption is downloaded when merge == True, the function prints a message to the user\n# This message is a hint to add the argument `caption` to download the caption\nif not info_only and ( os. path. join ( output_dir, title + '.cmt.json' ) if output_dir else title + '.cmt.json' ). exists ( ) and ( not kwargs [ 'caption' ] and merge ) :\nprint ( 'No danmaku file is downloaded. Try adding `-c' (or `--caption` in the CLI) to download the danmaku file. For details, see https://acfun.readthedocs.io/en/latest/cli.html#danmaku.' )\n\n# When `info_only` is `False` and the function was not interrupted, it downloads the video if `merge` is True\n# The video is not downloaded when `merge` is False\n\nif not info_only and not dry_run and merge :\nif not acfunvid_exists ( vid, dry_run = False ) :\nget_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid"
      ]
    ],
    "filtered_resps": [
      " except RequestException : info_only = True print ( 'Retrying after network connection error.' )\n# If no caption is downloaded when merge == True, the function prints a message to the user\n# This message is a hint to add the argument `caption` to download the caption\nif not info_only and ( os. path. join ( output_dir, title + '.cmt.json' ) if output_dir else title + '.cmt.json' ). exists ( ) and ( not kwargs [ 'caption' ] and merge ) :\nprint ( 'No danmaku file is downloaded. Try adding `-c' (or `--caption` in the CLI) to download the danmaku file. For details, see https://acfun.readthedocs.io/en/latest/cli.html#danmaku.' )\n\n# When `info_only` is `False` and the function was not interrupted, it downloads the video if `merge` is True\n# The video is not downloaded when `merge` is False\n\nif not info_only and not dry_run and merge :\nif not acfunvid_exists ( vid, dry_run = False ) :\nget_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid"
    ],
    "smoothed_bleu_4": 0.37843138237970025
  },
  {
    "doc_id": 16,
    "doc": {
      "id": 16,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "matchall",
      "original_string": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret",
      "language": "python",
      "code": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret",
      "code_tokens": [
        "def",
        "matchall",
        "(",
        "text",
        ",",
        "patterns",
        ")",
        ":",
        "ret",
        "=",
        "[",
        "]",
        "for",
        "pattern",
        "in",
        "patterns",
        ":",
        "match",
        "=",
        "re",
        ".",
        "findall",
        "(",
        "pattern",
        ",",
        "text",
        ")",
        "ret",
        "+=",
        "match",
        "return",
        "ret"
      ],
      "docstring": "Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.",
      "docstring_tokens": [
        "Scans",
        "through",
        "a",
        "string",
        "for",
        "substrings",
        "matched",
        "some",
        "patterns",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268"
    },
    "target": "Scans through a string for substrings matched some patterns .",
    "arguments": [
      [
        "def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret += match return ret",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n# Use regular expression to find the dates: match = findall(r'\\d{4}-\\d{2}-\\d{2}.*?(\\d{4})-\\d{2}-\\d{2}', text) dates = [x[0] for x in match] dates # This line is added after the match and date are found # Output: ['2020', '2015']\n```\n\nIn this corrected code, the list comprehension is used to extract the year that we want. `findall` will return all non-overlapping matches of the regular expression in the string as a list of strings, where each match is a 2-tuple. We use a for loop and list comprehension to extract the specific date we want.\n\nHere's the corrected code with explanation and example use case:\n\n\n```python\nimport re\ndef matchall ( text, patterns ) : \n    # This function will return a list of lists where each sublist contains \n    #    all non-overlapping matches of the regular expression in the string \n    #    as a list of strings, where each match is a 2-tuple.\n    ret = [ ]\n    for pattern in patterns : \n         match = re.findall"
      ]
    ],
    "filtered_resps": [
      "\n# Use regular expression to find the dates: match = findall(r'\\d{4}-\\d{2}-\\d{2}.*?(\\d{4})-\\d{2}-\\d{2}', text) dates = [x[0] for x in match] dates # This line is added after the match and date are found # Output: ['2020', '2015']\n```\n\nIn this corrected code, the list comprehension is used to extract the year that we want. `findall` will return all non-overlapping matches of the regular expression in the string as a list of strings, where each match is a 2-tuple. We use a for loop and list comprehension to extract the specific date we want.\n\nHere's the corrected code with explanation and example use case:\n\n\n```python\nimport re\ndef matchall ( text, patterns ) : \n    # This function will return a list of lists where each sublist contains \n    #    all non-overlapping matches of the regular expression in the string \n    #    as a list of strings, where each match is a 2-tuple.\n    ret = [ ]\n    for pattern in patterns : \n         match = re.findall"
    ],
    "smoothed_bleu_4": 0.6072516822446649
  },
  {
    "doc_id": 17,
    "doc": {
      "id": 17,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "parse_query_param",
      "original_string": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None",
      "language": "python",
      "code": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None",
      "code_tokens": [
        "def",
        "parse_query_param",
        "(",
        "url",
        ",",
        "param",
        ")",
        ":",
        "try",
        ":",
        "return",
        "parse",
        ".",
        "parse_qs",
        "(",
        "parse",
        ".",
        "urlparse",
        "(",
        "url",
        ")",
        ".",
        "query",
        ")",
        "[",
        "param",
        "]",
        "[",
        "0",
        "]",
        "except",
        ":",
        "return",
        "None"
      ],
      "docstring": "Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.",
      "docstring_tokens": [
        "Parses",
        "the",
        "query",
        "string",
        "of",
        "a",
        "URL",
        "and",
        "returns",
        "the",
        "value",
        "of",
        "a",
        "parameter",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299"
    },
    "target": "Parses the query string of a URL and returns the value of a parameter .",
    "arguments": [
      [
        "def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\nThis code takes a URL as input and attempts to extract a query parameter. If the parameter exists in the query string, it returns the value; otherwise, it returns None. You can use the returned value as needed in your application.\n\nHere\u2019s a step-by-step explanation of the code:\n\n1. `try`: This section of the code contains the code that might raise an error. In this case, it attempts to parse the query string in the URL using `parse.parse_qs()`.\n2. `return parse.parse_qs(parse.urlparse(url).query)[param][0]`: If the code inside the `try` block runs without errors, it attempts to extract the value of the specified query parameter and return it.\n3. `except`: This keyword is used to handle exceptions, which are runtime errors that occur during execution.\n4. `return None`: If any exception occurs while executing the code in the `try` block, this line catches the exception and returns None.\n\nYou can use this function like this:\n```python\nurl = \"https://example.com?name=John&age=30\"\nparam = \"name\"\nprint(parse_query_param(url, param))  # Output: John\n\nurl = \"https"
      ]
    ],
    "filtered_resps": [
      "\n```\nThis code takes a URL as input and attempts to extract a query parameter. If the parameter exists in the query string, it returns the value; otherwise, it returns None. You can use the returned value as needed in your application.\n\nHere\u2019s a step-by-step explanation of the code:\n\n1. `try`: This section of the code contains the code that might raise an error. In this case, it attempts to parse the query string in the URL using `parse.parse_qs()`.\n2. `return parse.parse_qs(parse.urlparse(url).query)[param][0]`: If the code inside the `try` block runs without errors, it attempts to extract the value of the specified query parameter and return it.\n3. `except`: This keyword is used to handle exceptions, which are runtime errors that occur during execution.\n4. `return None`: If any exception occurs while executing the code in the `try` block, this line catches the exception and returns None.\n\nYou can use this function like this:\n```python\nurl = \"https://example.com?name=John&age=30\"\nparam = \"name\"\nprint(parse_query_param(url, param))  # Output: John\n\nurl = \"https"
    ],
    "smoothed_bleu_4": 1.7276787227812511
  },
  {
    "doc_id": 18,
    "doc": {
      "id": 18,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "get_content",
      "original_string": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data",
      "language": "python",
      "code": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data",
      "code_tokens": [
        "def",
        "get_content",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "{",
        "}",
        ",",
        "decoded",
        "=",
        "True",
        ")",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'get_content: %s'",
        "%",
        "url",
        ")",
        "req",
        "=",
        "request",
        ".",
        "Request",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "if",
        "cookies",
        ":",
        "cookies",
        ".",
        "add_cookie_header",
        "(",
        "req",
        ")",
        "req",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "req",
        ".",
        "unredirected_hdrs",
        ")",
        "response",
        "=",
        "urlopen_with_retry",
        "(",
        "req",
        ")",
        "data",
        "=",
        "response",
        ".",
        "read",
        "(",
        ")",
        "# Handle HTTP compression for gzip and deflate (zlib)",
        "content_encoding",
        "=",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Encoding'",
        ")",
        "if",
        "content_encoding",
        "==",
        "'gzip'",
        ":",
        "data",
        "=",
        "ungzip",
        "(",
        "data",
        ")",
        "elif",
        "content_encoding",
        "==",
        "'deflate'",
        ":",
        "data",
        "=",
        "undeflate",
        "(",
        "data",
        ")",
        "# Decode the response body",
        "if",
        "decoded",
        ":",
        "charset",
        "=",
        "match1",
        "(",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Type'",
        ",",
        "''",
        ")",
        ",",
        "r'charset=([\\w-]+)'",
        ")",
        "if",
        "charset",
        "is",
        "not",
        "None",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "charset",
        ",",
        "'ignore'",
        ")",
        "else",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ",",
        "'ignore'",
        ")",
        "return",
        "data"
      ],
      "docstring": "Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
      "docstring_tokens": [
        "Gets",
        "the",
        "content",
        "of",
        "a",
        "URL",
        "via",
        "sending",
        "a",
        "HTTP",
        "GET",
        "request",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454"
    },
    "target": "Gets the content of a URL via sending a HTTP GET request .",
    "arguments": [
      [
        "def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ( logging. exception ( 'urllib2 error %s' % e ), e ) # This is a copy from: http://code.activestate.com/recipes/279857/ def match1 ( pattern, s, pos = 0, end = None ) : from itertools import islice def match_any ( pattern, s ) : s = s [ pos : ] if end is None : end = len ( s ) result = match_any ( pattern, s ) if not result : return None else : return result [ 0 ] def match_one ( pattern, s ) : matches = match_any ( pattern, s ) return matches and matches [ 0 ] return match_one ( re. compile ( r\"(\\W% s )\" % pattern ), s ) def get_json ( url, headers = { }, decoded = True, return_list = False, ** kwargs ) : # This function uses the json module for Python 2.6 # But for python 2.5 it uses the simplejson module and logging. debug ( 'get_content: % s' % url ) req = request. Request ( url, headers = headers ) if cookies : cookies. add_cookie_header ( req ) req. headers"
      ]
    ],
    "filtered_resps": [
      " ( logging. exception ( 'urllib2 error %s' % e ), e ) # This is a copy from: http://code.activestate.com/recipes/279857/ def match1 ( pattern, s, pos = 0, end = None ) : from itertools import islice def match_any ( pattern, s ) : s = s [ pos : ] if end is None : end = len ( s ) result = match_any ( pattern, s ) if not result : return None else : return result [ 0 ] def match_one ( pattern, s ) : matches = match_any ( pattern, s ) return matches and matches [ 0 ] return match_one ( re. compile ( r\"(\\W% s )\" % pattern ), s ) def get_json ( url, headers = { }, decoded = True, return_list = False, ** kwargs ) : # This function uses the json module for Python 2.6 # But for python 2.5 it uses the simplejson module and logging. debug ( 'get_content: % s' % url ) req = request. Request ( url, headers = headers ) if cookies : cookies. add_cookie_header ( req ) req. headers"
    ],
    "smoothed_bleu_4": 0.7866315228531848
  },
  {
    "doc_id": 19,
    "doc": {
      "id": 19,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "post_content",
      "original_string": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data",
      "language": "python",
      "code": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data",
      "code_tokens": [
        "def",
        "post_content",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "{",
        "}",
        ",",
        "post_data",
        "=",
        "{",
        "}",
        ",",
        "decoded",
        "=",
        "True",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "kwargs",
        ".",
        "get",
        "(",
        "'post_data_raw'",
        ")",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'post_content: %s\\npost_data_raw: %s'",
        "%",
        "(",
        "url",
        ",",
        "kwargs",
        "[",
        "'post_data_raw'",
        "]",
        ")",
        ")",
        "else",
        ":",
        "logging",
        ".",
        "debug",
        "(",
        "'post_content: %s\\npost_data: %s'",
        "%",
        "(",
        "url",
        ",",
        "post_data",
        ")",
        ")",
        "req",
        "=",
        "request",
        ".",
        "Request",
        "(",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "if",
        "cookies",
        ":",
        "cookies",
        ".",
        "add_cookie_header",
        "(",
        "req",
        ")",
        "req",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "req",
        ".",
        "unredirected_hdrs",
        ")",
        "if",
        "kwargs",
        ".",
        "get",
        "(",
        "'post_data_raw'",
        ")",
        ":",
        "post_data_enc",
        "=",
        "bytes",
        "(",
        "kwargs",
        "[",
        "'post_data_raw'",
        "]",
        ",",
        "'utf-8'",
        ")",
        "else",
        ":",
        "post_data_enc",
        "=",
        "bytes",
        "(",
        "parse",
        ".",
        "urlencode",
        "(",
        "post_data",
        ")",
        ",",
        "'utf-8'",
        ")",
        "response",
        "=",
        "urlopen_with_retry",
        "(",
        "req",
        ",",
        "data",
        "=",
        "post_data_enc",
        ")",
        "data",
        "=",
        "response",
        ".",
        "read",
        "(",
        ")",
        "# Handle HTTP compression for gzip and deflate (zlib)",
        "content_encoding",
        "=",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Encoding'",
        ")",
        "if",
        "content_encoding",
        "==",
        "'gzip'",
        ":",
        "data",
        "=",
        "ungzip",
        "(",
        "data",
        ")",
        "elif",
        "content_encoding",
        "==",
        "'deflate'",
        ":",
        "data",
        "=",
        "undeflate",
        "(",
        "data",
        ")",
        "# Decode the response body",
        "if",
        "decoded",
        ":",
        "charset",
        "=",
        "match1",
        "(",
        "response",
        ".",
        "getheader",
        "(",
        "'Content-Type'",
        ")",
        ",",
        "r'charset=([\\w-]+)'",
        ")",
        "if",
        "charset",
        "is",
        "not",
        "None",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "charset",
        ")",
        "else",
        ":",
        "data",
        "=",
        "data",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "return",
        "data"
      ],
      "docstring": "Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.",
      "docstring_tokens": [
        "Post",
        "the",
        "content",
        "of",
        "a",
        "URL",
        "via",
        "sending",
        "a",
        "HTTP",
        "POST",
        "request",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501"
    },
    "target": "Post the content of a URL via sending a HTTP POST request .",
    "arguments": [
      [
        "def post_content ( url , headers = { } , post_data = { } , decoded = True , * * kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_html_content ( url, user_agent = default_user_agent, cookies = None, * * kwargs ) : #... ```\n\nNote that we had to remove the `return` statement at the end of `post_content` as it was not following the `*` in `*args`. We also fixed the `if` condition inside the `if post_data_raw:` block to use a single `logging.debug` call. This is because we want to print both `url` and `kwargs['post_data_raw']` in the same log statement. Additionally, we extracted the `content_encoding` logic into a separate line for better readability. Let's review the changes we made:\n\n1.  Removed the `return` statement at the end of `post_content` to fix the issue with the `*` in `*args`.\n2.  Simplified the `if` condition inside the `if post_data_raw:` block by combining the two `logging.debug` calls into one.\n3.  Extracted the `content_encoding` logic into a separate line for better readability.\n\nWith these changes, the function should now work as expected.\n\n**Please let me know if you need further clarification or if you'd like me to review"
      ]
    ],
    "filtered_resps": [
      " def get_html_content ( url, user_agent = default_user_agent, cookies = None, * * kwargs ) : #... ```\n\nNote that we had to remove the `return` statement at the end of `post_content` as it was not following the `*` in `*args`. We also fixed the `if` condition inside the `if post_data_raw:` block to use a single `logging.debug` call. This is because we want to print both `url` and `kwargs['post_data_raw']` in the same log statement. Additionally, we extracted the `content_encoding` logic into a separate line for better readability. Let's review the changes we made:\n\n1.  Removed the `return` statement at the end of `post_content` to fix the issue with the `*` in `*args`.\n2.  Simplified the `if` condition inside the `if post_data_raw:` block by combining the two `logging.debug` calls into one.\n3.  Extracted the `content_encoding` logic into a separate line for better readability.\n\nWith these changes, the function should now work as expected.\n\n**Please let me know if you need further clarification or if you'd like me to review"
    ],
    "smoothed_bleu_4": 0.6338732990778437
  },
  {
    "doc_id": 20,
    "doc": {
      "id": 20,
      "repo": "soimort/you-get",
      "path": "src/you_get/common.py",
      "func_name": "parse_host",
      "original_string": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
      "language": "python",
      "code": "def parse_host(host):\n    \"\"\"Parses host name and port number from a string.\n    \"\"\"\n    if re.match(r'^(\\d+)$', host) is not None:\n        return (\"0.0.0.0\", int(host))\n    if re.match(r'^(\\w+)://', host) is None:\n        host = \"//\" + host\n    o = parse.urlparse(host)\n    hostname = o.hostname or \"0.0.0.0\"\n    port = o.port or 0\n    return (hostname, port)",
      "code_tokens": [
        "def",
        "parse_host",
        "(",
        "host",
        ")",
        ":",
        "if",
        "re",
        ".",
        "match",
        "(",
        "r'^(\\d+)$'",
        ",",
        "host",
        ")",
        "is",
        "not",
        "None",
        ":",
        "return",
        "(",
        "\"0.0.0.0\"",
        ",",
        "int",
        "(",
        "host",
        ")",
        ")",
        "if",
        "re",
        ".",
        "match",
        "(",
        "r'^(\\w+)://'",
        ",",
        "host",
        ")",
        "is",
        "None",
        ":",
        "host",
        "=",
        "\"//\"",
        "+",
        "host",
        "o",
        "=",
        "parse",
        ".",
        "urlparse",
        "(",
        "host",
        ")",
        "hostname",
        "=",
        "o",
        ".",
        "hostname",
        "or",
        "\"0.0.0.0\"",
        "port",
        "=",
        "o",
        ".",
        "port",
        "or",
        "0",
        "return",
        "(",
        "hostname",
        ",",
        "port",
        ")"
      ],
      "docstring": "Parses host name and port number from a string.",
      "docstring_tokens": [
        "Parses",
        "host",
        "name",
        "and",
        "port",
        "number",
        "from",
        "a",
        "string",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1216-L1226"
    },
    "target": "Parses host name and port number from a string .",
    "arguments": [
      [
        "def parse_host ( host ) : if re . match ( r'^(\\d+)$' , host ) is not None : return ( \"0.0.0.0\" , int ( host ) ) if re . match ( r'^(\\w+)://' , host ) is None : host = \"//\" + host o = parse . urlparse ( host ) hostname = o . hostname or \"0.0.0.0\" port = o . port or 0 return ( hostname , port )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n    - @blueyed\n    - 2010-04-12T06:47:44Z\n\n  - ``` ###   import   base64 import   re import   urlparse ```\n\n## Step 1: Extract port from the host string.\nIf the host string consists only of digits, we consider it as a port number and return a tuple with \"0.0.0.0\" as the hostname and the port number as an integer.\n\n## Step 2: Check if the host string starts with a scheme (like \"http://\" or \"https://\").\nWe check if the host string starts with a scheme using a regular expression. If it does, we let the urlparse function handle the parsing. If it doesn't, we add a scheme to the host string to make urlparse work correctly.\n\n## Step 3: Parse the host string using urlparse.\nWe use the urlparse function from the parse module to break the host string into its components.\n\n## Step 4: Extract the hostname and port from the parsed URL.\nWe take the hostname from the parsed URL. If it's empty, we default to \"0.0.0.0\". We also take the port from the parsed URL. If it's"
      ]
    ],
    "filtered_resps": [
      " ```\n    - @blueyed\n    - 2010-04-12T06:47:44Z\n\n  - ``` ###   import   base64 import   re import   urlparse ```\n\n## Step 1: Extract port from the host string.\nIf the host string consists only of digits, we consider it as a port number and return a tuple with \"0.0.0.0\" as the hostname and the port number as an integer.\n\n## Step 2: Check if the host string starts with a scheme (like \"http://\" or \"https://\").\nWe check if the host string starts with a scheme using a regular expression. If it does, we let the urlparse function handle the parsing. If it doesn't, we add a scheme to the host string to make urlparse work correctly.\n\n## Step 3: Parse the host string using urlparse.\nWe use the urlparse function from the parse module to break the host string into its components.\n\n## Step 4: Extract the hostname and port from the parsed URL.\nWe take the hostname from the parsed URL. If it's empty, we default to \"0.0.0.0\". We also take the port from the parsed URL. If it's"
    ],
    "smoothed_bleu_4": 0.9504204993170806
  },
  {
    "doc_id": 21,
    "doc": {
      "id": 21,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/showroom.py",
      "func_name": "showroom_get_roomid_by_room_url_key",
      "original_string": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')\n    assert roomid\n    return roomid",
      "language": "python",
      "code": "def showroom_get_roomid_by_room_url_key(room_url_key):\n    \"\"\"str->str\"\"\"\n    fake_headers_mobile = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Charset': 'UTF-8,*;q=0.5',\n        'Accept-Encoding': 'gzip,deflate,sdch',\n        'Accept-Language': 'en-US,en;q=0.8',\n        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'\n    }\n    webpage_url = 'https://www.showroom-live.com/' + room_url_key\n    html = get_content(webpage_url, headers = fake_headers_mobile)\n    roomid = match1(html, r'room\\?room_id\\=(\\d+)')\n    assert roomid\n    return roomid",
      "code_tokens": [
        "def",
        "showroom_get_roomid_by_room_url_key",
        "(",
        "room_url_key",
        ")",
        ":",
        "fake_headers_mobile",
        "=",
        "{",
        "'Accept'",
        ":",
        "'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'",
        ",",
        "'Accept-Charset'",
        ":",
        "'UTF-8,*;q=0.5'",
        ",",
        "'Accept-Encoding'",
        ":",
        "'gzip,deflate,sdch'",
        ",",
        "'Accept-Language'",
        ":",
        "'en-US,en;q=0.8'",
        ",",
        "'User-Agent'",
        ":",
        "'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'",
        "}",
        "webpage_url",
        "=",
        "'https://www.showroom-live.com/'",
        "+",
        "room_url_key",
        "html",
        "=",
        "get_content",
        "(",
        "webpage_url",
        ",",
        "headers",
        "=",
        "fake_headers_mobile",
        ")",
        "roomid",
        "=",
        "match1",
        "(",
        "html",
        ",",
        "r'room\\?room_id\\=(\\d+)'",
        ")",
        "assert",
        "roomid",
        "return",
        "roomid"
      ],
      "docstring": "str->str",
      "docstring_tokens": [
        "str",
        "-",
        ">",
        "str"
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L11-L24"
    },
    "target": "str - > str",
    "arguments": [
      [
        "def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) assert roomid return roomid",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\ndef fake_headers_mobile : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Charset' : 'UTF-8,*;q=0.5', 'Accept-Encoding' : 'gzip,deflate,sdch', 'Accept-Language' : 'en-US,en;q=0.8', 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' }\ndef get_content ( url, headers ) : url = 'https://' + url. replace ( 'http://', '' ). replace ( 'https://', '' ) headers [ 'Host' ] = url. split ( '/' ) [ 2 ] headers [ 'Referer' ] = url return requests. get ( url, headers = headers, allow_redirects = True ). text\ndef match1 ( text, pattern ) : return re. search ( pattern, text ). group ( 1"
      ]
    ],
    "filtered_resps": [
      "\ndef fake_headers_mobile : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Charset' : 'UTF-8,*;q=0.5', 'Accept-Encoding' : 'gzip,deflate,sdch', 'Accept-Language' : 'en-US,en;q=0.8', 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' }\ndef get_content ( url, headers ) : url = 'https://' + url. replace ( 'http://', '' ). replace ( 'https://', '' ) headers [ 'Host' ] = url. split ( '/' ) [ 2 ] headers [ 'Referer' ] = url return requests. get ( url, headers = headers, allow_redirects = True ). text\ndef match1 ( text, pattern ) : return re. search ( pattern, text ). group ( 1"
    ],
    "smoothed_bleu_4": 0.36866531861680635
  },
  {
    "doc_id": 22,
    "doc": {
      "id": 22,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "_wanmen_get_title_by_json_topic_part",
      "original_string": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",
      "language": "python",
      "code": "def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):\n    \"\"\"JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.\"\"\"\n\n    return '_'.join([json_content[0]['name'],\n                    json_content[0]['Topics'][tIndex]['name'],\n                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])",
      "code_tokens": [
        "def",
        "_wanmen_get_title_by_json_topic_part",
        "(",
        "json_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        ":",
        "return",
        "'_'",
        ".",
        "join",
        "(",
        "[",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'name'",
        "]",
        ",",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'name'",
        "]",
        ",",
        "json_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'Parts'",
        "]",
        "[",
        "pIndex",
        "]",
        "[",
        "'name'",
        "]",
        "]",
        ")"
      ],
      "docstring": "JSON, int, int, int->str\n    \n    Get a proper title with courseid+topicID+partID.",
      "docstring_tokens": [
        "JSON",
        "int",
        "int",
        "int",
        "-",
        ">",
        "str",
        "Get",
        "a",
        "proper",
        "title",
        "with",
        "courseid",
        "+",
        "topicID",
        "+",
        "partID",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L18-L25"
    },
    "target": "JSON int int int - > str Get a proper title with courseid + topicID + partID .",
    "arguments": [
      [
        "def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : return '_' . join ( [ json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " )\ndef _wanmen_get_title_by_json_topic_part ( json_content, tIndex, pIndex ) : return '_'. join ( [ json_content [ 0 ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'parts' ] [ pIndex ] [ 'name' ] ] ) )\n```\n\nFixing the bug, the corrected code is:\n```python\ndef _wanmen_get_title_by_json_topic_part ( json_content, tIndex, pIndex ) :\n    return '_'. join ( [ json_content [ 0 ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'parts' ] [ pIndex ] [ 'name' ] ] ) )\n```\n\nExplanation: The keys in the dictionary were changed from 'Topic' to 'topic' (case insensitive) in the JSON string, but the code was still accessing them with the original capital 'T'. The corrected code changes the key access"
      ]
    ],
    "filtered_resps": [
      " )\ndef _wanmen_get_title_by_json_topic_part ( json_content, tIndex, pIndex ) : return '_'. join ( [ json_content [ 0 ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'parts' ] [ pIndex ] [ 'name' ] ] ) )\n```\n\nFixing the bug, the corrected code is:\n```python\ndef _wanmen_get_title_by_json_topic_part ( json_content, tIndex, pIndex ) :\n    return '_'. join ( [ json_content [ 0 ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'name' ], json_content [ 0 ] [ 'topics' ] [ tIndex ] [ 'parts' ] [ pIndex ] [ 'name' ] ] ) )\n```\n\nExplanation: The keys in the dictionary were changed from 'Topic' to 'topic' (case insensitive) in the JSON string, but the code was still accessing them with the original capital 'T'. The corrected code changes the key access"
    ],
    "smoothed_bleu_4": 0.581285347685132
  },
  {
    "doc_id": 23,
    "doc": {
      "id": 23,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "wanmen_download_by_course",
      "original_string": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)",
      "language": "python",
      "code": "def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.\"\"\"\n\n    for tIndex in range(len(json_api_content[0]['Topics'])):\n        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):\n            wanmen_download_by_course_topic_part(json_api_content,\n                                                 tIndex,\n                                                 pIndex,\n                                                 output_dir=output_dir,\n                                                 merge=merge,\n                                                 info_only=info_only,\n                                                 **kwargs)",
      "code_tokens": [
        "def",
        "wanmen_download_by_course",
        "(",
        "json_api_content",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "for",
        "tIndex",
        "in",
        "range",
        "(",
        "len",
        "(",
        "json_api_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        ")",
        ")",
        ":",
        "for",
        "pIndex",
        "in",
        "range",
        "(",
        "len",
        "(",
        "json_api_content",
        "[",
        "0",
        "]",
        "[",
        "'Topics'",
        "]",
        "[",
        "tIndex",
        "]",
        "[",
        "'Parts'",
        "]",
        ")",
        ")",
        ":",
        "wanmen_download_by_course_topic_part",
        "(",
        "json_api_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "int->None\n    \n    Download a WHOLE course.\n    Reuse the API call to save time.",
      "docstring_tokens": [
        "int",
        "-",
        ">",
        "None",
        "Download",
        "a",
        "WHOLE",
        "course",
        ".",
        "Reuse",
        "the",
        "API",
        "call",
        "to",
        "save",
        "time",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L37-L51"
    },
    "target": "int - > None Download a WHOLE course . Reuse the API call to save time .",
    "arguments": [
      [
        "def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : for pIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def wanmen_download_by_course_topic_part ( json_api_content, tIndex, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : print ( f 'Topic { tIndex + 1 } / { len ( json_api_content [ 0 ] [ \"Topics\" ] ) } : { json_api_content [ 0 ] [ \"Topics\" ] [ tIndex ] [ \"name\" ] }, Part { pIndex + 1 } / { len ( json_api_content [ 0 ] [ \"Topics\" ] [ tIndex ] [ \"Parts\" ] ) }') for resourceIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'Resources' ] ) ) : resource = json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'Resources' ] [ resourceIndex ]. get ( 'data', { } ) ; resourceFile = get_file_name ( resource, ** kwargs ) ; file_path = get_file_path ( resourceFile, output_dir, info_only, ** kwargs )"
      ]
    ],
    "filtered_resps": [
      " def wanmen_download_by_course_topic_part ( json_api_content, tIndex, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : print ( f 'Topic { tIndex + 1 } / { len ( json_api_content [ 0 ] [ \"Topics\" ] ) } : { json_api_content [ 0 ] [ \"Topics\" ] [ tIndex ] [ \"name\" ] }, Part { pIndex + 1 } / { len ( json_api_content [ 0 ] [ \"Topics\" ] [ tIndex ] [ \"Parts\" ] ) }') for resourceIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'Resources' ] ) ) : resource = json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'Resources' ] [ resourceIndex ]. get ( 'data', { } ) ; resourceFile = get_file_name ( resource, ** kwargs ) ; file_path = get_file_path ( resourceFile, output_dir, info_only, ** kwargs )"
    ],
    "smoothed_bleu_4": 0.581285347685132
  },
  {
    "doc_id": 24,
    "doc": {
      "id": 24,
      "repo": "soimort/you-get",
      "path": "src/you_get/extractors/wanmen.py",
      "func_name": "wanmen_download_by_course_topic_part",
      "original_string": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",
      "language": "python",
      "code": "def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"int, int, int->None\n    \n    Download ONE PART of the course.\"\"\"\n\n    html = json_api_content\n\n    title = _wanmen_get_title_by_json_topic_part(html, \n                                                  tIndex, \n                                                  pIndex)\n\n    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,\n                                                      tIndex, \n                                                     pIndex)\n\n    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)",
      "code_tokens": [
        "def",
        "wanmen_download_by_course_topic_part",
        "(",
        "json_api_content",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ",",
        "output_dir",
        "=",
        "'.'",
        ",",
        "merge",
        "=",
        "True",
        ",",
        "info_only",
        "=",
        "False",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "html",
        "=",
        "json_api_content",
        "title",
        "=",
        "_wanmen_get_title_by_json_topic_part",
        "(",
        "html",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        "bokeccID",
        "=",
        "_wanmen_get_boke_id_by_json_topic_part",
        "(",
        "html",
        ",",
        "tIndex",
        ",",
        "pIndex",
        ")",
        "bokecc_download_by_id",
        "(",
        "vid",
        "=",
        "bokeccID",
        ",",
        "title",
        "=",
        "title",
        ",",
        "output_dir",
        "=",
        "output_dir",
        ",",
        "merge",
        "=",
        "merge",
        ",",
        "info_only",
        "=",
        "info_only",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "int, int, int->None\n    \n    Download ONE PART of the course.",
      "docstring_tokens": [
        "int",
        "int",
        "int",
        "-",
        ">",
        "None",
        "Download",
        "ONE",
        "PART",
        "of",
        "the",
        "course",
        "."
      ],
      "sha": "b746ac01c9f39de94cac2d56f665285b0523b974",
      "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L69-L84"
    },
    "target": "int int int - > None Download ONE PART of the course .",
    "arguments": [
      [
        "def wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_topic_part ( html , tIndex , pIndex ) bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , tIndex , pIndex ) bokecc_download_by_id ( vid = bokeccID , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return None\ndef wanmen_download_by_course_part ( json_api_content, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_course_part ( html, pIndex ) bokeccID = _wanmen_get_boke_id_by_json_course_part ( html, pIndex ) bokecc_download_by_id ( vid = bokeccID, title = title, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs ) return None\ndef wanmen_download ( json_api_content, output_dir = '.', merge = True, info_only = False, * * kwargs ) : try : # course index html = json_api_content. json () html = json_api_content. text [ - ( html. soup. find ('script'). find ('id'='course-list- json'). parent. parent. i. text ). count ( '_' ) :] # course title & ID title = json_api_content. text [ - ( json_api_content. soup. find ('div ', {'class':'course-title'} )."
      ]
    ],
    "filtered_resps": [
      " return None\ndef wanmen_download_by_course_part ( json_api_content, pIndex, output_dir = '.', merge = True, info_only = False, * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_course_part ( html, pIndex ) bokeccID = _wanmen_get_boke_id_by_json_course_part ( html, pIndex ) bokecc_download_by_id ( vid = bokeccID, title = title, output_dir = output_dir, merge = merge, info_only = info_only, * * kwargs ) return None\ndef wanmen_download ( json_api_content, output_dir = '.', merge = True, info_only = False, * * kwargs ) : try : # course index html = json_api_content. json () html = json_api_content. text [ - ( html. soup. find ('script'). find ('id'='course-list- json'). parent. parent. i. text ). count ( '_' ) :] # course title & ID title = json_api_content. text [ - ( json_api_content. soup. find ('div ', {'class':'course-title'} )."
    ],
    "smoothed_bleu_4": 0.5486735138998247
  },
  {
    "doc_id": 25,
    "doc": {
      "id": 25,
      "repo": "apache/airflow",
      "path": "airflow/executors/base_executor.py",
      "func_name": "BaseExecutor.has_task",
      "original_string": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
      "language": "python",
      "code": "def has_task(self, task_instance):\n        \"\"\"\n        Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor\n        \"\"\"\n        if task_instance.key in self.queued_tasks or task_instance.key in self.running:\n            return True",
      "code_tokens": [
        "def",
        "has_task",
        "(",
        "self",
        ",",
        "task_instance",
        ")",
        ":",
        "if",
        "task_instance",
        ".",
        "key",
        "in",
        "self",
        ".",
        "queued_tasks",
        "or",
        "task_instance",
        ".",
        "key",
        "in",
        "self",
        ".",
        "running",
        ":",
        "return",
        "True"
      ],
      "docstring": "Checks if a task is either queued or running in this executor\n\n        :param task_instance: TaskInstance\n        :return: True if the task is known to this executor",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "task",
        "is",
        "either",
        "queued",
        "or",
        "running",
        "in",
        "this",
        "executor"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L97-L105"
    },
    "target": "Checks if a task is either queued or running in this executor",
    "arguments": [
      [
        "def has_task ( self , task_instance ) : if task_instance . key in self . queued_tasks or task_instance . key in self . running : return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return False # This method would return all tasks in the # queue as a list ``` TaskInstance ``` objects. def get_tasks_in_queue ( self ): task_list = [ self. queued_tasks [ key ] for key in self. queued_tasks. keys ()] return task_list ``` TaskInstance ``` objects. def get_tasks_running ( self ): task_list = [ self. running_tasks [ key ] for key in self. running_tasks. keys ()] return task_list ```\n  - ``` self. running_tasks [ task_key ]. status = Status. DONE and status  =  self. running_tasks. pop ( task_key ) and status  =  self. _status_callback ( task_key,   Status. DONE ``` The `self._status_callback` method is not defined. This method is used to handle the status of a task after it has been marked as DONE.\n  - ``` self. running_tasks [ task_key ]. status = Status. CANCELLED and self. running_tasks. pop ( task_key ) and self. _status_callback ( task_key,   Status. CANCELLED ) ``` This line cancels the task with the task key `task_key` and sets its status to CANCELLED. Then, it"
      ]
    ],
    "filtered_resps": [
      " return False # This method would return all tasks in the # queue as a list ``` TaskInstance ``` objects. def get_tasks_in_queue ( self ): task_list = [ self. queued_tasks [ key ] for key in self. queued_tasks. keys ()] return task_list ``` TaskInstance ``` objects. def get_tasks_running ( self ): task_list = [ self. running_tasks [ key ] for key in self. running_tasks. keys ()] return task_list ```\n  - ``` self. running_tasks [ task_key ]. status = Status. DONE and status  =  self. running_tasks. pop ( task_key ) and status  =  self. _status_callback ( task_key,   Status. DONE ``` The `self._status_callback` method is not defined. This method is used to handle the status of a task after it has been marked as DONE.\n  - ``` self. running_tasks [ task_key ]. status = Status. CANCELLED and self. running_tasks. pop ( task_key ) and self. _status_callback ( task_key,   Status. CANCELLED ) ``` This line cancels the task with the task key `task_key` and sets its status to CANCELLED. Then, it"
    ],
    "smoothed_bleu_4": 0.6853303011358302
  },
  {
    "doc_id": 26,
    "doc": {
      "id": 26,
      "repo": "apache/airflow",
      "path": "airflow/executors/base_executor.py",
      "func_name": "BaseExecutor.get_event_buffer",
      "original_string": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events",
      "language": "python",
      "code": "def get_event_buffer(self, dag_ids=None):\n        \"\"\"\n        Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events\n        \"\"\"\n        cleared_events = dict()\n        if dag_ids is None:\n            cleared_events = self.event_buffer\n            self.event_buffer = dict()\n        else:\n            for key in list(self.event_buffer.keys()):\n                dag_id, _, _, _ = key\n                if dag_id in dag_ids:\n                    cleared_events[key] = self.event_buffer.pop(key)\n\n        return cleared_events",
      "code_tokens": [
        "def",
        "get_event_buffer",
        "(",
        "self",
        ",",
        "dag_ids",
        "=",
        "None",
        ")",
        ":",
        "cleared_events",
        "=",
        "dict",
        "(",
        ")",
        "if",
        "dag_ids",
        "is",
        "None",
        ":",
        "cleared_events",
        "=",
        "self",
        ".",
        "event_buffer",
        "self",
        ".",
        "event_buffer",
        "=",
        "dict",
        "(",
        ")",
        "else",
        ":",
        "for",
        "key",
        "in",
        "list",
        "(",
        "self",
        ".",
        "event_buffer",
        ".",
        "keys",
        "(",
        ")",
        ")",
        ":",
        "dag_id",
        ",",
        "_",
        ",",
        "_",
        ",",
        "_",
        "=",
        "key",
        "if",
        "dag_id",
        "in",
        "dag_ids",
        ":",
        "cleared_events",
        "[",
        "key",
        "]",
        "=",
        "self",
        ".",
        "event_buffer",
        ".",
        "pop",
        "(",
        "key",
        ")",
        "return",
        "cleared_events"
      ],
      "docstring": "Returns and flush the event buffer. In case dag_ids is specified\n        it will only return and flush events for the given dag_ids. Otherwise\n        it returns and flushes all\n\n        :param dag_ids: to dag_ids to return events for, if None returns all\n        :return: a dict of events",
      "docstring_tokens": [
        "Returns",
        "and",
        "flush",
        "the",
        "event",
        "buffer",
        ".",
        "In",
        "case",
        "dag_ids",
        "is",
        "specified",
        "it",
        "will",
        "only",
        "return",
        "and",
        "flush",
        "events",
        "for",
        "the",
        "given",
        "dag_ids",
        ".",
        "Otherwise",
        "it",
        "returns",
        "and",
        "flushes",
        "all"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L160-L179"
    },
    "target": "Returns and flush the event buffer . In case dag_ids is specified it will only return and flush events for the given dag_ids . Otherwise it returns and flushes all",
    "arguments": [
      [
        "def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\nget the cleared events between the DAGs\n- dag_ids \u2013 filter the cleared events by these DAGs\n\u2013 return a dictionary containing all cleared events between DAGs, otherwise return None\nget the events associated with the specified DAG\n- dag_id \u2013 get the events (including cleared ones) for this DAG\n\u2013 return a dictionary containing all events for the DAG\nget the event buffer of DAG\n- dag_id \u2013 get the event buffer of this DAG\n\u2013 return the dictionary containing all events for the DAG\nget the current events of a DAG\n- dag_id \u2013 get the list of current running jobs\n- \u2013 return a dictionary containing all current running jobs of the DAG, otherwise return None\nget the current running jobs of a DAG\n- dag_id \u2013 get the list of current running jobs\n- \u2013 return a dictionary containing all current running jobs of the DAG, otherwise return None\nget the event buffer of DAG\n- dag_id \u2013 get the event buffer of this DAG\n\u2013 return the dictionary containing all events for the DAG\nget the current running jobs of a DAG\n- dag_id \u2013 get the list of current running jobs\n- \u2013 return a dictionary containing all current running jobs of the DAG, otherwise return None"
      ]
    ],
    "filtered_resps": [
      "\nget the cleared events between the DAGs\n- dag_ids \u2013 filter the cleared events by these DAGs\n\u2013 return a dictionary containing all cleared events between DAGs, otherwise return None\nget the events associated with the specified DAG\n- dag_id \u2013 get the events (including cleared ones) for this DAG\n\u2013 return a dictionary containing all events for the DAG\nget the event buffer of DAG\n- dag_id \u2013 get the event buffer of this DAG\n\u2013 return the dictionary containing all events for the DAG\nget the current events of a DAG\n- dag_id \u2013 get the list of current running jobs\n- \u2013 return a dictionary containing all current running jobs of the DAG, otherwise return None\nget the current running jobs of a DAG\n- dag_id \u2013 get the list of current running jobs\n- \u2013 return a dictionary containing all current running jobs of the DAG, otherwise return None\nget the event buffer of DAG\n- dag_id \u2013 get the event buffer of this DAG\n\u2013 return the dictionary containing all events for the DAG\nget the current running jobs of a DAG\n- dag_id \u2013 get the list of current running jobs\n- \u2013 return a dictionary containing all current running jobs of the DAG, otherwise return None"
    ],
    "smoothed_bleu_4": 2.015381917447841
  },
  {
    "doc_id": 27,
    "doc": {
      "id": 27,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/snowflake_hook.py",
      "func_name": "SnowflakeHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Returns a snowflake.connection object\n        \"\"\"\n        conn_config = self._get_conn_params()\n        conn = snowflake.connector.connect(**conn_config)\n        return conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn_config",
        "=",
        "self",
        ".",
        "_get_conn_params",
        "(",
        ")",
        "conn",
        "=",
        "snowflake",
        ".",
        "connector",
        ".",
        "connect",
        "(",
        "*",
        "*",
        "conn_config",
        ")",
        "return",
        "conn"
      ],
      "docstring": "Returns a snowflake.connection object",
      "docstring_tokens": [
        "Returns",
        "a",
        "snowflake",
        ".",
        "connection",
        "object"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L107-L113"
    },
    "target": "Returns a snowflake . connection object",
    "arguments": [
      [
        "def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " conn_config\nconn = self. get_conn ( )\nWe use this connection to query the warehouse, and fetch the results. You can see the `query` method is used, just the same as we used it above. But this time, it returns a _cursor object that is wrapped in a `SnowflakeDataFetcher`.\n```\ncursor   =   conn. query ( \"SELECT * FROM warehouse\" )\nfetcher   =   SnowflakeDataFetcher ( cursor,   conn )\nrows   =   fetcher. fetchall ( )\nfetcher. finish ( )\n```\nThe reason we need to wrap the cursor in a SnowflakeDataFetcher object is that the Snowflake Python driver provides a cursor object that can only be used within the connection it was acquired from. In order to support using the results outside the connection (which is where PySpark would run the `fetchall` call), we need to wrap the cursor in a proxy class, that will forward calls to the original cursor and the connection.\n```\nclass SnowflakeDataFetcher ( object ) :\ndef   __init__ ( self,   cursor,   conn ) :\nself. _cursor   =   cursor\nself. _conn   =   conn\nself."
      ]
    ],
    "filtered_resps": [
      " conn_config\nconn = self. get_conn ( )\nWe use this connection to query the warehouse, and fetch the results. You can see the `query` method is used, just the same as we used it above. But this time, it returns a _cursor object that is wrapped in a `SnowflakeDataFetcher`.\n```\ncursor   =   conn. query ( \"SELECT * FROM warehouse\" )\nfetcher   =   SnowflakeDataFetcher ( cursor,   conn )\nrows   =   fetcher. fetchall ( )\nfetcher. finish ( )\n```\nThe reason we need to wrap the cursor in a SnowflakeDataFetcher object is that the Snowflake Python driver provides a cursor object that can only be used within the connection it was acquired from. In order to support using the results outside the connection (which is where PySpark would run the `fetchall` call), we need to wrap the cursor in a proxy class, that will forward calls to the original cursor and the connection.\n```\nclass SnowflakeDataFetcher ( object ) :\ndef   __init__ ( self,   cursor,   conn ) :\nself. _cursor   =   cursor\nself. _conn   =   conn\nself."
    ],
    "smoothed_bleu_4": 0.8374455283766328
  },
  {
    "doc_id": 28,
    "doc": {
      "id": 28,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/snowflake_hook.py",
      "func_name": "SnowflakeHook._get_aws_credentials",
      "original_string": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key",
      "language": "python",
      "code": "def _get_aws_credentials(self):\n        \"\"\"\n        returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements\n        \"\"\"\n        if self.snowflake_conn_id:\n            connection_object = self.get_connection(self.snowflake_conn_id)\n            if 'aws_secret_access_key' in connection_object.extra_dejson:\n                aws_access_key_id = connection_object.extra_dejson.get(\n                    'aws_access_key_id')\n                aws_secret_access_key = connection_object.extra_dejson.get(\n                    'aws_secret_access_key')\n        return aws_access_key_id, aws_secret_access_key",
      "code_tokens": [
        "def",
        "_get_aws_credentials",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "snowflake_conn_id",
        ":",
        "connection_object",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "snowflake_conn_id",
        ")",
        "if",
        "'aws_secret_access_key'",
        "in",
        "connection_object",
        ".",
        "extra_dejson",
        ":",
        "aws_access_key_id",
        "=",
        "connection_object",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'aws_access_key_id'",
        ")",
        "aws_secret_access_key",
        "=",
        "connection_object",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'aws_secret_access_key'",
        ")",
        "return",
        "aws_access_key_id",
        ",",
        "aws_secret_access_key"
      ],
      "docstring": "returns aws_access_key_id, aws_secret_access_key\n        from extra\n\n        intended to be used by external import and export statements",
      "docstring_tokens": [
        "returns",
        "aws_access_key_id",
        "aws_secret_access_key",
        "from",
        "extra"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L115-L129"
    },
    "target": "returns aws_access_key_id aws_secret_access_key from extra",
    "arguments": [
      [
        "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ", self. region_name, self. bucket_name + \"/\" + self. s3_key else : raise AirflowException ( \"You must configure aws credentials using 'aws_secret_access_key' and 'aws_access_key_id' in airbnb-airflow-config.json\" ) ```\n  - File \"/usr/local/airflow/airflow/providers/amazon/aws/hooks/aws_hooks.py\", Lines 35 in _init_credentials\n  - Function: _get_credentials_from_airflow\n  - Arguments: self\n\nReturn Value: A dict containing AWS access key ID, secret access key, region name and bucket key. The dict will be used to initialize AWS credentials on the EC2 instance.\n\n### Step 3: Define the AWS hook in Airflow\n\nTo add the AWS hook to Airflow, create a new file my_snowflake_airflow_plugin/handlers/snowflakes3to_s3.py with the following content:\n\n```\n from airflow import settings\n from airflow.hooks.base import Hook\n from providers.amazon.aws.operators import aws_hook\n from. import my_snowflake_airflow_plugin\n from. import aws_credentials\n from airflow import settings\n\nclass SnowflakeS3ToS3(Hook):\n     def __init__("
      ]
    ],
    "filtered_resps": [
      ", self. region_name, self. bucket_name + \"/\" + self. s3_key else : raise AirflowException ( \"You must configure aws credentials using 'aws_secret_access_key' and 'aws_access_key_id' in airbnb-airflow-config.json\" ) ```\n  - File \"/usr/local/airflow/airflow/providers/amazon/aws/hooks/aws_hooks.py\", Lines 35 in _init_credentials\n  - Function: _get_credentials_from_airflow\n  - Arguments: self\n\nReturn Value: A dict containing AWS access key ID, secret access key, region name and bucket key. The dict will be used to initialize AWS credentials on the EC2 instance.\n\n### Step 3: Define the AWS hook in Airflow\n\nTo add the AWS hook to Airflow, create a new file my_snowflake_airflow_plugin/handlers/snowflakes3to_s3.py with the following content:\n\n```\n from airflow import settings\n from airflow.hooks.base import Hook\n from providers.amazon.aws.operators import aws_hook\n from. import my_snowflake_airflow_plugin\n from. import aws_credentials\n from airflow import settings\n\nclass SnowflakeS3ToS3(Hook):\n     def __init__("
    ],
    "smoothed_bleu_4": 4.654470988681577
  },
  {
    "doc_id": 29,
    "doc": {
      "id": 29,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/grpc_hook.py",
      "func_name": "GrpcHook._get_field",
      "original_string": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
      "language": "python",
      "code": "def _get_field(self, field_name, default=None):\n        \"\"\"\n        Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.\n        \"\"\"\n        full_field_name = 'extra__grpc__{}'.format(field_name)\n        if full_field_name in self.extras:\n            return self.extras[full_field_name]\n        else:\n            return default",
      "code_tokens": [
        "def",
        "_get_field",
        "(",
        "self",
        ",",
        "field_name",
        ",",
        "default",
        "=",
        "None",
        ")",
        ":",
        "full_field_name",
        "=",
        "'extra__grpc__{}'",
        ".",
        "format",
        "(",
        "field_name",
        ")",
        "if",
        "full_field_name",
        "in",
        "self",
        ".",
        "extras",
        ":",
        "return",
        "self",
        ".",
        "extras",
        "[",
        "full_field_name",
        "]",
        "else",
        ":",
        "return",
        "default"
      ],
      "docstring": "Fetches a field from extras, and returns it. This is some Airflow\n        magic. The grpc hook type adds custom UI elements\n        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n        They get formatted as shown below.",
      "docstring_tokens": [
        "Fetches",
        "a",
        "field",
        "from",
        "extras",
        "and",
        "returns",
        "it",
        ".",
        "This",
        "is",
        "some",
        "Airflow",
        "magic",
        ".",
        "The",
        "grpc",
        "hook",
        "type",
        "adds",
        "custom",
        "UI",
        "elements",
        "to",
        "the",
        "hook",
        "page",
        "which",
        "allow",
        "admins",
        "to",
        "specify",
        "scopes",
        "credential",
        "pem",
        "files",
        "etc",
        ".",
        "They",
        "get",
        "formatted",
        "as",
        "shown",
        "below",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/grpc_hook.py#L112-L123"
    },
    "target": "Fetches a field from extras and returns it . This is some Airflow magic . The grpc hook type adds custom UI elements to the hook page which allow admins to specify scopes credential pem files etc . They get formatted as shown below .",
    "arguments": [
      [
        "def _get_field ( self , field_name , default = None ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\n### Step 3: Update the `get_extras()` method to include the new `full_field_name` variable\n\nHere's how you can modify the `get_extras()` method to include the `full_field_name` variable:\n\n```\ndef   get_extras ( self ) :\n     \"\"\"Return all stored gRPC attributes.\"\"\"\n     full_field_name   =   'extra__grpc__{}'\n     extras   =   {}\n     keys   =   self. extra_keys ()\n     for   key   in   keys :\n         value   =   self. extra ( key,   default = None )\n         full_key_name   =   full_field_name. format ( key )\n         extras [ full_key_name ]   =   value\n     return   extras\n```\n\n### Step 4: Update the `get_field()` method to use the `full_field_name` variable\n\nHere's how you can modify the `get_field()` method to use the `full_field_name` variable:\n\n```\ndef   get_field ( self,   field_name,   default = None ) :\n     \"\"\"Return the value for the given gRPC attribute name.\"\"\"\n     full_field_name   =   'extra__grpc__{}'. format"
      ]
    ],
    "filtered_resps": [
      " ```\n\n### Step 3: Update the `get_extras()` method to include the new `full_field_name` variable\n\nHere's how you can modify the `get_extras()` method to include the `full_field_name` variable:\n\n```\ndef   get_extras ( self ) :\n     \"\"\"Return all stored gRPC attributes.\"\"\"\n     full_field_name   =   'extra__grpc__{}'\n     extras   =   {}\n     keys   =   self. extra_keys ()\n     for   key   in   keys :\n         value   =   self. extra ( key,   default = None )\n         full_key_name   =   full_field_name. format ( key )\n         extras [ full_key_name ]   =   value\n     return   extras\n```\n\n### Step 4: Update the `get_field()` method to use the `full_field_name` variable\n\nHere's how you can modify the `get_field()` method to use the `full_field_name` variable:\n\n```\ndef   get_field ( self,   field_name,   default = None ) :\n     \"\"\"Return the value for the given gRPC attribute name.\"\"\"\n     full_field_name   =   'extra__grpc__{}'. format"
    ],
    "smoothed_bleu_4": 0.6990492968198697
  },
  {
    "doc_id": 30,
    "doc": {
      "id": 30,
      "repo": "apache/airflow",
      "path": "airflow/hooks/postgres_hook.py",
      "func_name": "PostgresHook.copy_expert",
      "original_string": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()",
      "language": "python",
      "code": "def copy_expert(self, sql, filename, open=open):\n        \"\"\"\n        Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.\n        \"\"\"\n        if not os.path.isfile(filename):\n            with open(filename, 'w'):\n                pass\n\n        with open(filename, 'r+') as f:\n            with closing(self.get_conn()) as conn:\n                with closing(conn.cursor()) as cur:\n                    cur.copy_expert(sql, f)\n                    f.truncate(f.tell())\n                    conn.commit()",
      "code_tokens": [
        "def",
        "copy_expert",
        "(",
        "self",
        ",",
        "sql",
        ",",
        "filename",
        ",",
        "open",
        "=",
        "open",
        ")",
        ":",
        "if",
        "not",
        "os",
        ".",
        "path",
        ".",
        "isfile",
        "(",
        "filename",
        ")",
        ":",
        "with",
        "open",
        "(",
        "filename",
        ",",
        "'w'",
        ")",
        ":",
        "pass",
        "with",
        "open",
        "(",
        "filename",
        ",",
        "'r+'",
        ")",
        "as",
        "f",
        ":",
        "with",
        "closing",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ")",
        "as",
        "conn",
        ":",
        "with",
        "closing",
        "(",
        "conn",
        ".",
        "cursor",
        "(",
        ")",
        ")",
        "as",
        "cur",
        ":",
        "cur",
        ".",
        "copy_expert",
        "(",
        "sql",
        ",",
        "f",
        ")",
        "f",
        ".",
        "truncate",
        "(",
        "f",
        ".",
        "tell",
        "(",
        ")",
        ")",
        "conn",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Executes SQL using psycopg2 copy_expert method.\n        Necessary to execute COPY command without access to a superuser.\n\n        Note: if this method is called with a \"COPY FROM\" statement and\n        the specified input file does not exist, it creates an empty\n        file and no data is loaded, but the operation succeeds.\n        So if users want to be aware when the input file does not exist,\n        they have to check its existence by themselves.",
      "docstring_tokens": [
        "Executes",
        "SQL",
        "using",
        "psycopg2",
        "copy_expert",
        "method",
        ".",
        "Necessary",
        "to",
        "execute",
        "COPY",
        "command",
        "without",
        "access",
        "to",
        "a",
        "superuser",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L63-L83"
    },
    "target": "Executes SQL using psycopg2 copy_expert method . Necessary to execute COPY command without access to a superuser .",
    "arguments": [
      [
        "def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur . copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. set_conn ( False )\n```\n\nThe vulnerability is caused by the `copy_expert` method of the PostgreSQL cursor object not handling file descriptors in the correct way, which can lead to a file descriptor leak when transferring data to a file.\n\nTo exploit this vulnerability, an attacker would need to be able to create a file that is in the same directory as the file being copied to, and to have access to the database credentials. They could then use the `copy_expert` method to create a file descriptor leak, which they could use to read sensitive data from the system.\n\nHere's an example of how an attacker might exploit this vulnerability:\n\n```\n# Create a file in the same directory as the file being copied to\nfp = open(\"evil_file.txt\", \"w\")\n\n# Connect to the database and create a cursor object\nconn = psycopg2.connect(dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\", host=\"myhost\", port=5432)\ncur = conn.cursor()\n\n# Create a copy of the original file to a new file\ncur.copy_expert(\"COPY table_name TO LOCAL '/path/to/original/file.txt'\", fp)\n\n# Close the file to create a file descriptor leak\nfp"
      ]
    ],
    "filtered_resps": [
      " self. set_conn ( False )\n```\n\nThe vulnerability is caused by the `copy_expert` method of the PostgreSQL cursor object not handling file descriptors in the correct way, which can lead to a file descriptor leak when transferring data to a file.\n\nTo exploit this vulnerability, an attacker would need to be able to create a file that is in the same directory as the file being copied to, and to have access to the database credentials. They could then use the `copy_expert` method to create a file descriptor leak, which they could use to read sensitive data from the system.\n\nHere's an example of how an attacker might exploit this vulnerability:\n\n```\n# Create a file in the same directory as the file being copied to\nfp = open(\"evil_file.txt\", \"w\")\n\n# Connect to the database and create a cursor object\nconn = psycopg2.connect(dbname=\"mydatabase\", user=\"myuser\", password=\"mypassword\", host=\"myhost\", port=5432)\ncur = conn.cursor()\n\n# Create a copy of the original file to a new file\ncur.copy_expert(\"COPY table_name TO LOCAL '/path/to/original/file.txt'\", fp)\n\n# Close the file to create a file descriptor leak\nfp"
    ],
    "smoothed_bleu_4": 1.2292540870741455
  },
  {
    "doc_id": 31,
    "doc": {
      "id": 31,
      "repo": "apache/airflow",
      "path": "airflow/hooks/postgres_hook.py",
      "func_name": "PostgresHook.bulk_dump",
      "original_string": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)",
      "language": "python",
      "code": "def bulk_dump(self, table, tmp_file):\n        \"\"\"\n        Dumps a database table into a tab-delimited file\n        \"\"\"\n        self.copy_expert(\"COPY {table} TO STDOUT\".format(table=table), tmp_file)",
      "code_tokens": [
        "def",
        "bulk_dump",
        "(",
        "self",
        ",",
        "table",
        ",",
        "tmp_file",
        ")",
        ":",
        "self",
        ".",
        "copy_expert",
        "(",
        "\"COPY {table} TO STDOUT\"",
        ".",
        "format",
        "(",
        "table",
        "=",
        "table",
        ")",
        ",",
        "tmp_file",
        ")"
      ],
      "docstring": "Dumps a database table into a tab-delimited file",
      "docstring_tokens": [
        "Dumps",
        "a",
        "database",
        "table",
        "into",
        "a",
        "tab",
        "-",
        "delimited",
        "file"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L91-L95"
    },
    "target": "Dumps a database table into a tab - delimited file",
    "arguments": [
      [
        "def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( \"COPY {table} TO STDOUT\" . format ( table = table ) , tmp_file )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ;\nBulk dump table to file: Dump a whole table without reading it row by row.\n```\n# Bulk dump table to file\nself.bulk_dump(table, 'file_to_dumper.py')```\n\n### 3.3.2. `copy_expert` Method\n--------------------------------\n\nThe `copy_expert` method allows you to execute a command as an expert by providing the command as a string.\n\n#### Example\n-----------\n\n```python\n# Execute expert command\nself.copy_expert(\"COPY table_name FROM '/path/to/input/file.csv' WITH CSV HEADER;\", 'file_to_orm.py')\n# Execute expert command with error handling\nself.copy_expert(\"COPY table_name FROM '/path/to/input/file.csv' WITH CSV HEADER;\", 'file_to_orm.py', verbose=2)\n```\n\n### 3.3.3. `execute` Method\n---------------------------\n\nThe `execute` method allows you to execute an SQL command.\n\n#### Example\n-----------\n\n```python\n# Execute SQL command\nself.execute(\"CREATE TABLE test AS (SELECT * FROM table_name) WITH NO DATA;\")\n# Execute SQL command with error handling\nself.execute(\"CREATE TABLE test AS (SELECT * FROM table_name) WITH NO DATA;\", verbose=2)\n"
      ]
    ],
    "filtered_resps": [
      " ;\nBulk dump table to file: Dump a whole table without reading it row by row.\n```\n# Bulk dump table to file\nself.bulk_dump(table, 'file_to_dumper.py')```\n\n### 3.3.2. `copy_expert` Method\n--------------------------------\n\nThe `copy_expert` method allows you to execute a command as an expert by providing the command as a string.\n\n#### Example\n-----------\n\n```python\n# Execute expert command\nself.copy_expert(\"COPY table_name FROM '/path/to/input/file.csv' WITH CSV HEADER;\", 'file_to_orm.py')\n# Execute expert command with error handling\nself.copy_expert(\"COPY table_name FROM '/path/to/input/file.csv' WITH CSV HEADER;\", 'file_to_orm.py', verbose=2)\n```\n\n### 3.3.3. `execute` Method\n---------------------------\n\nThe `execute` method allows you to execute an SQL command.\n\n#### Example\n-----------\n\n```python\n# Execute SQL command\nself.execute(\"CREATE TABLE test AS (SELECT * FROM table_name) WITH NO DATA;\")\n# Execute SQL command with error handling\nself.execute(\"CREATE TABLE test AS (SELECT * FROM table_name) WITH NO DATA;\", verbose=2)\n"
    ],
    "smoothed_bleu_4": 0.40387644394079253
  },
  {
    "doc_id": 32,
    "doc": {
      "id": 32,
      "repo": "apache/airflow",
      "path": "airflow/contrib/operators/file_to_gcs.py",
      "func_name": "FileToGoogleCloudStorageOperator.execute",
      "original_string": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
      "language": "python",
      "code": "def execute(self, context):\n        \"\"\"\n        Uploads the file to Google cloud storage\n        \"\"\"\n        hook = GoogleCloudStorageHook(\n            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,\n            delegate_to=self.delegate_to)\n\n        hook.upload(\n            bucket_name=self.bucket,\n            object_name=self.dst,\n            mime_type=self.mime_type,\n            filename=self.src,\n            gzip=self.gzip,\n        )",
      "code_tokens": [
        "def",
        "execute",
        "(",
        "self",
        ",",
        "context",
        ")",
        ":",
        "hook",
        "=",
        "GoogleCloudStorageHook",
        "(",
        "google_cloud_storage_conn_id",
        "=",
        "self",
        ".",
        "google_cloud_storage_conn_id",
        ",",
        "delegate_to",
        "=",
        "self",
        ".",
        "delegate_to",
        ")",
        "hook",
        ".",
        "upload",
        "(",
        "bucket_name",
        "=",
        "self",
        ".",
        "bucket",
        ",",
        "object_name",
        "=",
        "self",
        ".",
        "dst",
        ",",
        "mime_type",
        "=",
        "self",
        ".",
        "mime_type",
        ",",
        "filename",
        "=",
        "self",
        ".",
        "src",
        ",",
        "gzip",
        "=",
        "self",
        ".",
        "gzip",
        ",",
        ")"
      ],
      "docstring": "Uploads the file to Google cloud storage",
      "docstring_tokens": [
        "Uploads",
        "the",
        "file",
        "to",
        "Google",
        "cloud",
        "storage"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_gcs.py#L68-L82"
    },
    "target": "Uploads the file to Google cloud storage",
    "arguments": [
      [
        "def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " raise   FileExistsError ( f \" The file  { self. src }  already exists in cloud storage:  \"\n                     f \" { self. dst } \" )\n\ndef   pre_execute ( self, context ) :\n     file_size_bytes = getsize ( self. src )\n     if   file_size_bytes   >   self. max_size_bytes :\n         raise   FileSizeExceededError ( f \" File  { self. src }  has size  \"\n                                     f \" { file_size_bytes }B  which is bigger \"\n                                     f \"than the configured  { self. max_size_bytes } \"\n                                     f \" which exceeds the limit of \"\n                                     f \" { self. max_size_bytes } B \" )\n\ndef   post_execute ( self, callback, context, ** kwargs ) :  # pragma: no cover\n     hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self. google_cloud_storage_conn_id,\n                                   delegate_to = self. delegate_to )\n     object_id,   object_exists = hook. list_objects ( bucket_name = self. bucket,\n                                                 object_name = self. dst )\n     context. get ( 'ti_xcom' ). push ( xcom_value = object_id )\n\n     if  "
      ]
    ],
    "filtered_resps": [
      " raise   FileExistsError ( f \" The file  { self. src }  already exists in cloud storage:  \"\n                     f \" { self. dst } \" )\n\ndef   pre_execute ( self, context ) :\n     file_size_bytes = getsize ( self. src )\n     if   file_size_bytes   >   self. max_size_bytes :\n         raise   FileSizeExceededError ( f \" File  { self. src }  has size  \"\n                                     f \" { file_size_bytes }B  which is bigger \"\n                                     f \"than the configured  { self. max_size_bytes } \"\n                                     f \" which exceeds the limit of \"\n                                     f \" { self. max_size_bytes } B \" )\n\ndef   post_execute ( self, callback, context, ** kwargs ) :  # pragma: no cover\n     hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self. google_cloud_storage_conn_id,\n                                   delegate_to = self. delegate_to )\n     object_id,   object_exists = hook. list_objects ( bucket_name = self. bucket,\n                                                 object_name = self. dst )\n     context. get ( 'ti_xcom' ). push ( xcom_value = object_id )\n\n     if  "
    ],
    "smoothed_bleu_4": 0.8645454908873125
  },
  {
    "doc_id": 33,
    "doc": {
      "id": 33,
      "repo": "apache/airflow",
      "path": "airflow/macros/hive.py",
      "func_name": "max_partition",
      "original_string": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)",
      "language": "python",
      "code": "def max_partition(\n        table, schema=\"default\", field=None, filter_map=None,\n        metastore_conn_id='metastore_default'):\n    \"\"\"\n    Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'\n    \"\"\"\n    from airflow.hooks.hive_hooks import HiveMetastoreHook\n    if '.' in table:\n        schema, table = table.split('.')\n    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)\n    return hh.max_partition(\n        schema=schema, table_name=table, field=field, filter_map=filter_map)",
      "code_tokens": [
        "def",
        "max_partition",
        "(",
        "table",
        ",",
        "schema",
        "=",
        "\"default\"",
        ",",
        "field",
        "=",
        "None",
        ",",
        "filter_map",
        "=",
        "None",
        ",",
        "metastore_conn_id",
        "=",
        "'metastore_default'",
        ")",
        ":",
        "from",
        "airflow",
        ".",
        "hooks",
        ".",
        "hive_hooks",
        "import",
        "HiveMetastoreHook",
        "if",
        "'.'",
        "in",
        "table",
        ":",
        "schema",
        ",",
        "table",
        "=",
        "table",
        ".",
        "split",
        "(",
        "'.'",
        ")",
        "hh",
        "=",
        "HiveMetastoreHook",
        "(",
        "metastore_conn_id",
        "=",
        "metastore_conn_id",
        ")",
        "return",
        "hh",
        ".",
        "max_partition",
        "(",
        "schema",
        "=",
        "schema",
        ",",
        "table_name",
        "=",
        "table",
        ",",
        "field",
        "=",
        "field",
        ",",
        "filter_map",
        "=",
        "filter_map",
        ")"
      ],
      "docstring": "Gets the max partition for a table.\n\n    :param schema: The hive schema the table lives in\n    :type schema: str\n    :param table: The hive table you are interested in, supports the dot\n        notation as in \"my_database.my_table\", if a dot is found,\n        the schema param is disregarded\n    :type table: str\n    :param metastore_conn_id: The hive connection you are interested in.\n        If your default is set you don't need to use this parameter.\n    :type metastore_conn_id: str\n    :param filter_map: partition_key:partition_value map used for partition filtering,\n                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n                       Only partitions matching all partition_key:partition_value\n                       pairs will be considered as candidates of max partition.\n    :type filter_map: map\n    :param field: the field to get the max value from. If there's only\n        one partition field, this will be inferred\n    :type field: str\n\n    >>> max_partition('airflow.static_babynames_partitioned')\n    '2015-01-01'",
      "docstring_tokens": [
        "Gets",
        "the",
        "max",
        "partition",
        "for",
        "a",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L23-L55"
    },
    "target": "Gets the max partition for a table .",
    "arguments": [
      [
        "def max_partition ( table , schema = \"default\" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _load_partition ( table, schema, partition_path, hive_table_info ) : # If it's the whole table, load all of the rows. if table == \"ALL_TABLES\" : table_info = hive_table_info [ hive_table_info. index_of ( table ) ] # If partition_path is a single partition, we load by partition partition_key, partition_val = partition_path. rsplit ( '_', 1 ) partition_col = [ c for c in table_info. columns if c. lower ( ) == partition_key. lower ()][ 0 ] return load ( table_info, partition_col, partition_val ) # If partition_path is all partitions, we load by partition # The Hive metastore doesn't support this, so we load # all partitions and then filter. return load_all ( table_info, partition_col = None ) Load one or more partitions of a Hive table to XCom. Returns a dictionary with the XCom key, table information and the # of rows loaded. If partition_path is None or \"\", the entire table is loaded. if isinstance ( partition_path, str ) and partition_path == \"\" : partition_list = all_partitions ( table, schema ) else : partition_list = [ partition_path ] for partition"
      ]
    ],
    "filtered_resps": [
      " def _load_partition ( table, schema, partition_path, hive_table_info ) : # If it's the whole table, load all of the rows. if table == \"ALL_TABLES\" : table_info = hive_table_info [ hive_table_info. index_of ( table ) ] # If partition_path is a single partition, we load by partition partition_key, partition_val = partition_path. rsplit ( '_', 1 ) partition_col = [ c for c in table_info. columns if c. lower ( ) == partition_key. lower ()][ 0 ] return load ( table_info, partition_col, partition_val ) # If partition_path is all partitions, we load by partition # The Hive metastore doesn't support this, so we load # all partitions and then filter. return load_all ( table_info, partition_col = None ) Load one or more partitions of a Hive table to XCom. Returns a dictionary with the XCom key, table information and the # of rows loaded. If partition_path is None or \"\", the entire table is loaded. if isinstance ( partition_path, str ) and partition_path == \"\" : partition_list = all_partitions ( table, schema ) else : partition_list = [ partition_path ] for partition"
    ],
    "smoothed_bleu_4": 0.5467567343986371
  },
  {
    "doc_id": 34,
    "doc": {
      "id": 34,
      "repo": "apache/airflow",
      "path": "airflow/hooks/mysql_hook.py",
      "func_name": "MySqlHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Returns a mysql connection object\n        \"\"\"\n        conn = self.get_connection(self.mysql_conn_id)\n        conn_config = {\n            \"user\": conn.login,\n            \"passwd\": conn.password or '',\n            \"host\": conn.host or 'localhost',\n            \"db\": self.schema or conn.schema or ''\n        }\n\n        if not conn.port:\n            conn_config[\"port\"] = 3306\n        else:\n            conn_config[\"port\"] = int(conn.port)\n\n        if conn.extra_dejson.get('charset', False):\n            conn_config[\"charset\"] = conn.extra_dejson[\"charset\"]\n            if (conn_config[\"charset\"]).lower() == 'utf8' or\\\n                    (conn_config[\"charset\"]).lower() == 'utf-8':\n                conn_config[\"use_unicode\"] = True\n        if conn.extra_dejson.get('cursor', False):\n            if (conn.extra_dejson[\"cursor\"]).lower() == 'sscursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'dictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.DictCursor\n            elif (conn.extra_dejson[\"cursor\"]).lower() == 'ssdictcursor':\n                conn_config[\"cursorclass\"] = MySQLdb.cursors.SSDictCursor\n        local_infile = conn.extra_dejson.get('local_infile', False)\n        if conn.extra_dejson.get('ssl', False):\n            # SSL parameter for MySQL has to be a dictionary and in case\n            # of extra/dejson we can get string if extra is passed via\n            # URL parameters\n            dejson_ssl = conn.extra_dejson['ssl']\n            if isinstance(dejson_ssl, six.string_types):\n                dejson_ssl = json.loads(dejson_ssl)\n            conn_config['ssl'] = dejson_ssl\n        if conn.extra_dejson.get('unix_socket'):\n            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']\n        if local_infile:\n            conn_config[\"local_infile\"] = 1\n        conn = MySQLdb.connect(**conn_config)\n        return conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "mysql_conn_id",
        ")",
        "conn_config",
        "=",
        "{",
        "\"user\"",
        ":",
        "conn",
        ".",
        "login",
        ",",
        "\"passwd\"",
        ":",
        "conn",
        ".",
        "password",
        "or",
        "''",
        ",",
        "\"host\"",
        ":",
        "conn",
        ".",
        "host",
        "or",
        "'localhost'",
        ",",
        "\"db\"",
        ":",
        "self",
        ".",
        "schema",
        "or",
        "conn",
        ".",
        "schema",
        "or",
        "''",
        "}",
        "if",
        "not",
        "conn",
        ".",
        "port",
        ":",
        "conn_config",
        "[",
        "\"port\"",
        "]",
        "=",
        "3306",
        "else",
        ":",
        "conn_config",
        "[",
        "\"port\"",
        "]",
        "=",
        "int",
        "(",
        "conn",
        ".",
        "port",
        ")",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'charset'",
        ",",
        "False",
        ")",
        ":",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"charset\"",
        "]",
        "if",
        "(",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'utf8'",
        "or",
        "(",
        "conn_config",
        "[",
        "\"charset\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'utf-8'",
        ":",
        "conn_config",
        "[",
        "\"use_unicode\"",
        "]",
        "=",
        "True",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'cursor'",
        ",",
        "False",
        ")",
        ":",
        "if",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'sscursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "SSCursor",
        "elif",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'dictcursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "DictCursor",
        "elif",
        "(",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "\"cursor\"",
        "]",
        ")",
        ".",
        "lower",
        "(",
        ")",
        "==",
        "'ssdictcursor'",
        ":",
        "conn_config",
        "[",
        "\"cursorclass\"",
        "]",
        "=",
        "MySQLdb",
        ".",
        "cursors",
        ".",
        "SSDictCursor",
        "local_infile",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'local_infile'",
        ",",
        "False",
        ")",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'ssl'",
        ",",
        "False",
        ")",
        ":",
        "# SSL parameter for MySQL has to be a dictionary and in case",
        "# of extra/dejson we can get string if extra is passed via",
        "# URL parameters",
        "dejson_ssl",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "'ssl'",
        "]",
        "if",
        "isinstance",
        "(",
        "dejson_ssl",
        ",",
        "six",
        ".",
        "string_types",
        ")",
        ":",
        "dejson_ssl",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "dejson_ssl",
        ")",
        "conn_config",
        "[",
        "'ssl'",
        "]",
        "=",
        "dejson_ssl",
        "if",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'unix_socket'",
        ")",
        ":",
        "conn_config",
        "[",
        "'unix_socket'",
        "]",
        "=",
        "conn",
        ".",
        "extra_dejson",
        "[",
        "'unix_socket'",
        "]",
        "if",
        "local_infile",
        ":",
        "conn_config",
        "[",
        "\"local_infile\"",
        "]",
        "=",
        "1",
        "conn",
        "=",
        "MySQLdb",
        ".",
        "connect",
        "(",
        "*",
        "*",
        "conn_config",
        ")",
        "return",
        "conn"
      ],
      "docstring": "Returns a mysql connection object",
      "docstring_tokens": [
        "Returns",
        "a",
        "mysql",
        "connection",
        "object"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L62-L105"
    },
    "target": "Returns a mysql connection object",
    "arguments": [
      [
        "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn_config [ \"port\" ] = 3306 else : conn_config [ \"port\" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn_config [ \"use_unicode\" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : # SSL parameter for MySQL has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # URL parameters dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ \"local_infile\" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` As you can see, the function returns a MySQLdb object, which you should then use to execute your queries. For example, in a function: ``` def execute_query ( self, query ) : self. conn = get_conn ( self ) try : result = self. conn. cursor ( ). fetchall ( ) self. conn. commit ( ) except Exception as e : self. conn. rollback ( ) raise return result ``` You can replace the `execute_query` function with your actual SQL query. Keep in mind that database connection and query execution should be properly handled in your application to avoid issues. This example assumes you're using the MySQLdb library, which is not the most modern way to interact with MySQL databases. Consider using a more modern library like PyMySQL. This library also provides an implementation of the MySQL protocol. It supports SSL/TLS encryption via the `ssl` parameter when creating a connection. For example: ``` conn = pymysql. connect ( host = 'localhost', user = 'user', password = 'password', ssl = {'ca' : '/var/lib/mysql/ca.pem' } ) ``` \n\nPlease note that you need to use MySQLdb library that provides a MySQL interface. You may need to"
      ]
    ],
    "filtered_resps": [
      " ``` As you can see, the function returns a MySQLdb object, which you should then use to execute your queries. For example, in a function: ``` def execute_query ( self, query ) : self. conn = get_conn ( self ) try : result = self. conn. cursor ( ). fetchall ( ) self. conn. commit ( ) except Exception as e : self. conn. rollback ( ) raise return result ``` You can replace the `execute_query` function with your actual SQL query. Keep in mind that database connection and query execution should be properly handled in your application to avoid issues. This example assumes you're using the MySQLdb library, which is not the most modern way to interact with MySQL databases. Consider using a more modern library like PyMySQL. This library also provides an implementation of the MySQL protocol. It supports SSL/TLS encryption via the `ssl` parameter when creating a connection. For example: ``` conn = pymysql. connect ( host = 'localhost', user = 'user', password = 'password', ssl = {'ca' : '/var/lib/mysql/ca.pem' } ) ``` \n\nPlease note that you need to use MySQLdb library that provides a MySQL interface. You may need to"
    ],
    "smoothed_bleu_4": 0.7447490518059487
  },
  {
    "doc_id": 35,
    "doc": {
      "id": 35,
      "repo": "apache/airflow",
      "path": "airflow/bin/cli.py",
      "func_name": "task_state",
      "original_string": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
      "language": "python",
      "code": "def task_state(args):\n    \"\"\"\n    Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success\n    \"\"\"\n    dag = get_dag(args)\n    task = dag.get_task(task_id=args.task_id)\n    ti = TaskInstance(task, args.execution_date)\n    print(ti.current_state())",
      "code_tokens": [
        "def",
        "task_state",
        "(",
        "args",
        ")",
        ":",
        "dag",
        "=",
        "get_dag",
        "(",
        "args",
        ")",
        "task",
        "=",
        "dag",
        ".",
        "get_task",
        "(",
        "task_id",
        "=",
        "args",
        ".",
        "task_id",
        ")",
        "ti",
        "=",
        "TaskInstance",
        "(",
        "task",
        ",",
        "args",
        ".",
        "execution_date",
        ")",
        "print",
        "(",
        "ti",
        ".",
        "current_state",
        "(",
        ")",
        ")"
      ],
      "docstring": "Returns the state of a TaskInstance at the command line.\n    >>> airflow task_state tutorial sleep 2015-01-01\n    success",
      "docstring_tokens": [
        "Returns",
        "the",
        "state",
        "of",
        "a",
        "TaskInstance",
        "at",
        "the",
        "command",
        "line",
        ".",
        ">>>",
        "airflow",
        "task_state",
        "tutorial",
        "sleep",
        "2015",
        "-",
        "01",
        "-",
        "01",
        "success"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L554-L563"
    },
    "target": "Returns the state of a TaskInstance at the command line . >>> airflow task_state tutorial sleep 2015 - 01 - 01 success",
    "arguments": [
      [
        "def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args . execution_date ) print ( ti . current_state ( ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\nIn this task, we define a single argument `task_id` and use it to retrieve a `TaskInstance` for the given task ID and DAG run's execution date. Then we use the `current_state()` method to print the status of the `TaskInstance` to the console.\n\nNote: In real-world task code, you might want to handle errors and exceptions more robustly.\n\nTo run this task, you will need to provide the `task_id` argument using the `celery` command line tool:\n```\ncelery -A myapp worker -Q myqueue -l info\ncelery -A myapp tasks.task_state --task_id=<task_id> --execution_date=<execution_date> --loglevel=info\n```\nJust replace `<task_id>` with the ID of the task you want to check and `<execution_date>` with the desired execution date. The execution date should be in `YYYY-MM-DD HH:MM:SS` format.\n\nYou can run the task with any valid `execution_date`, including the current date.\n\nKeep in mind that this task relies on the `airflow` and `celery` libraries being installed and properly configured. Make sure you have a valid airflow database and a"
      ]
    ],
    "filtered_resps": [
      " ```\nIn this task, we define a single argument `task_id` and use it to retrieve a `TaskInstance` for the given task ID and DAG run's execution date. Then we use the `current_state()` method to print the status of the `TaskInstance` to the console.\n\nNote: In real-world task code, you might want to handle errors and exceptions more robustly.\n\nTo run this task, you will need to provide the `task_id` argument using the `celery` command line tool:\n```\ncelery -A myapp worker -Q myqueue -l info\ncelery -A myapp tasks.task_state --task_id=<task_id> --execution_date=<execution_date> --loglevel=info\n```\nJust replace `<task_id>` with the ID of the task you want to check and `<execution_date>` with the desired execution date. The execution date should be in `YYYY-MM-DD HH:MM:SS` format.\n\nYou can run the task with any valid `execution_date`, including the current date.\n\nKeep in mind that this task relies on the `airflow` and `celery` libraries being installed and properly configured. Make sure you have a valid airflow database and a"
    ],
    "smoothed_bleu_4": 1.2962089072832965
  },
  {
    "doc_id": 36,
    "doc": {
      "id": 36,
      "repo": "apache/airflow",
      "path": "airflow/bin/cli.py",
      "func_name": "restart_workers",
      "original_string": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)",
      "language": "python",
      "code": "def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):\n    \"\"\"\n    Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.\n    \"\"\"\n\n    def wait_until_true(fn, timeout=0):\n        \"\"\"\n        Sleeps until fn is true\n        \"\"\"\n        t = time.time()\n        while not fn():\n            if 0 < timeout <= time.time() - t:\n                raise AirflowWebServerTimeout(\n                    \"No response from gunicorn master within {0} seconds\"\n                    .format(timeout))\n            time.sleep(0.1)\n\n    def start_refresh(gunicorn_master_proc):\n        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')\n        log.debug('%s doing a refresh of %s workers', state, batch_size)\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        excess = 0\n        for _ in range(batch_size):\n            gunicorn_master_proc.send_signal(signal.SIGTTIN)\n            excess += 1\n            wait_until_true(lambda: num_workers_expected + excess ==\n                            get_num_workers_running(gunicorn_master_proc),\n                            master_timeout)\n\n    try:\n        wait_until_true(lambda: num_workers_expected ==\n                        get_num_workers_running(gunicorn_master_proc),\n                        master_timeout)\n        while True:\n            num_workers_running = get_num_workers_running(gunicorn_master_proc)\n            num_ready_workers_running = \\\n                get_num_ready_workers_running(gunicorn_master_proc)\n\n            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)\n\n            # Whenever some workers are not ready, wait until all workers are ready\n            if num_ready_workers_running < num_workers_running:\n                log.debug('%s some workers are starting up, waiting...', state)\n                sys.stdout.flush()\n                time.sleep(1)\n\n            # Kill a worker gracefully by asking gunicorn to reduce number of workers\n            elif num_workers_running > num_workers_expected:\n                excess = num_workers_running - num_workers_expected\n                log.debug('%s killing %s workers', state, excess)\n\n                for _ in range(excess):\n                    gunicorn_master_proc.send_signal(signal.SIGTTOU)\n                    excess -= 1\n                    wait_until_true(lambda: num_workers_expected + excess ==\n                                    get_num_workers_running(gunicorn_master_proc),\n                                    master_timeout)\n\n            # Start a new worker by asking gunicorn to increase number of workers\n            elif num_workers_running == num_workers_expected:\n                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')\n                log.debug(\n                    '%s sleeping for %ss starting doing a refresh...',\n                    state, refresh_interval\n                )\n                time.sleep(refresh_interval)\n                start_refresh(gunicorn_master_proc)\n\n            else:\n                # num_ready_workers_running == num_workers_running < num_workers_expected\n                log.error((\n                    \"%s some workers seem to have died and gunicorn\"\n                    \"did not restart them as expected\"\n                ), state)\n                time.sleep(10)\n                if len(\n                    psutil.Process(gunicorn_master_proc.pid).children()\n                ) < num_workers_expected:\n                    start_refresh(gunicorn_master_proc)\n    except (AirflowWebServerTimeout, OSError) as err:\n        log.error(err)\n        log.error(\"Shutting down webserver\")\n        try:\n            gunicorn_master_proc.terminate()\n            gunicorn_master_proc.wait()\n        finally:\n            sys.exit(1)",
      "code_tokens": [
        "def",
        "restart_workers",
        "(",
        "gunicorn_master_proc",
        ",",
        "num_workers_expected",
        ",",
        "master_timeout",
        ")",
        ":",
        "def",
        "wait_until_true",
        "(",
        "fn",
        ",",
        "timeout",
        "=",
        "0",
        ")",
        ":",
        "\"\"\"\n        Sleeps until fn is true\n        \"\"\"",
        "t",
        "=",
        "time",
        ".",
        "time",
        "(",
        ")",
        "while",
        "not",
        "fn",
        "(",
        ")",
        ":",
        "if",
        "0",
        "<",
        "timeout",
        "<=",
        "time",
        ".",
        "time",
        "(",
        ")",
        "-",
        "t",
        ":",
        "raise",
        "AirflowWebServerTimeout",
        "(",
        "\"No response from gunicorn master within {0} seconds\"",
        ".",
        "format",
        "(",
        "timeout",
        ")",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "0.1",
        ")",
        "def",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        ":",
        "batch_size",
        "=",
        "conf",
        ".",
        "getint",
        "(",
        "'webserver'",
        ",",
        "'worker_refresh_batch_size'",
        ")",
        "log",
        ".",
        "debug",
        "(",
        "'%s doing a refresh of %s workers'",
        ",",
        "state",
        ",",
        "batch_size",
        ")",
        "sys",
        ".",
        "stdout",
        ".",
        "flush",
        "(",
        ")",
        "sys",
        ".",
        "stderr",
        ".",
        "flush",
        "(",
        ")",
        "excess",
        "=",
        "0",
        "for",
        "_",
        "in",
        "range",
        "(",
        "batch_size",
        ")",
        ":",
        "gunicorn_master_proc",
        ".",
        "send_signal",
        "(",
        "signal",
        ".",
        "SIGTTIN",
        ")",
        "excess",
        "+=",
        "1",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "+",
        "excess",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "try",
        ":",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "while",
        "True",
        ":",
        "num_workers_running",
        "=",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        "num_ready_workers_running",
        "=",
        "get_num_ready_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        "state",
        "=",
        "'[{0} / {1}]'",
        ".",
        "format",
        "(",
        "num_ready_workers_running",
        ",",
        "num_workers_running",
        ")",
        "# Whenever some workers are not ready, wait until all workers are ready",
        "if",
        "num_ready_workers_running",
        "<",
        "num_workers_running",
        ":",
        "log",
        ".",
        "debug",
        "(",
        "'%s some workers are starting up, waiting...'",
        ",",
        "state",
        ")",
        "sys",
        ".",
        "stdout",
        ".",
        "flush",
        "(",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "1",
        ")",
        "# Kill a worker gracefully by asking gunicorn to reduce number of workers",
        "elif",
        "num_workers_running",
        ">",
        "num_workers_expected",
        ":",
        "excess",
        "=",
        "num_workers_running",
        "-",
        "num_workers_expected",
        "log",
        ".",
        "debug",
        "(",
        "'%s killing %s workers'",
        ",",
        "state",
        ",",
        "excess",
        ")",
        "for",
        "_",
        "in",
        "range",
        "(",
        "excess",
        ")",
        ":",
        "gunicorn_master_proc",
        ".",
        "send_signal",
        "(",
        "signal",
        ".",
        "SIGTTOU",
        ")",
        "excess",
        "-=",
        "1",
        "wait_until_true",
        "(",
        "lambda",
        ":",
        "num_workers_expected",
        "+",
        "excess",
        "==",
        "get_num_workers_running",
        "(",
        "gunicorn_master_proc",
        ")",
        ",",
        "master_timeout",
        ")",
        "# Start a new worker by asking gunicorn to increase number of workers",
        "elif",
        "num_workers_running",
        "==",
        "num_workers_expected",
        ":",
        "refresh_interval",
        "=",
        "conf",
        ".",
        "getint",
        "(",
        "'webserver'",
        ",",
        "'worker_refresh_interval'",
        ")",
        "log",
        ".",
        "debug",
        "(",
        "'%s sleeping for %ss starting doing a refresh...'",
        ",",
        "state",
        ",",
        "refresh_interval",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "refresh_interval",
        ")",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        "else",
        ":",
        "# num_ready_workers_running == num_workers_running < num_workers_expected",
        "log",
        ".",
        "error",
        "(",
        "(",
        "\"%s some workers seem to have died and gunicorn\"",
        "\"did not restart them as expected\"",
        ")",
        ",",
        "state",
        ")",
        "time",
        ".",
        "sleep",
        "(",
        "10",
        ")",
        "if",
        "len",
        "(",
        "psutil",
        ".",
        "Process",
        "(",
        "gunicorn_master_proc",
        ".",
        "pid",
        ")",
        ".",
        "children",
        "(",
        ")",
        ")",
        "<",
        "num_workers_expected",
        ":",
        "start_refresh",
        "(",
        "gunicorn_master_proc",
        ")",
        "except",
        "(",
        "AirflowWebServerTimeout",
        ",",
        "OSError",
        ")",
        "as",
        "err",
        ":",
        "log",
        ".",
        "error",
        "(",
        "err",
        ")",
        "log",
        ".",
        "error",
        "(",
        "\"Shutting down webserver\"",
        ")",
        "try",
        ":",
        "gunicorn_master_proc",
        ".",
        "terminate",
        "(",
        ")",
        "gunicorn_master_proc",
        ".",
        "wait",
        "(",
        ")",
        "finally",
        ":",
        "sys",
        ".",
        "exit",
        "(",
        "1",
        ")"
      ],
      "docstring": "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n    restarting workers occasionally.\n    Each iteration of the loop traverses one edge of this state transition\n    diagram, where each state (node) represents\n    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n    The horizontal transition at ? happens after the new worker parses all the\n    dags (so it could take a while!)\n       V \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    [n / n] \u2500\u2500TTIN\u2500\u2500> [ [n, n+bs) / n + bs ]  \u2500\u2500\u2500\u2500?\u2500\u2500\u2500> [n + bs / n + bs] \u2500\u2500TTOU\u2500\u2518\n       ^                          ^\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500 [ [0, n) / n ] <\u2500\u2500\u2500 start\n    We change the number of workers by sending TTIN and TTOU to the gunicorn\n    master process, which increases and decreases the number of child workers\n    respectively. Gunicorn guarantees that on TTOU workers are terminated\n    gracefully and that the oldest worker is terminated.",
      "docstring_tokens": [
        "Runs",
        "forever",
        "monitoring",
        "the",
        "child",
        "processes",
        "of"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L763-L868"
    },
    "target": "Runs forever monitoring the child processes of",
    "arguments": [
      [
        "def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : \"\"\" Sleeps until fn is true \"\"\" t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . SIGTTIN ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while True : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) # Whenever some workers are not ready, wait until all workers are ready if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) # Kill a worker gracefully by asking gunicorn to reduce number of workers elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . SIGTTOU ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) # Start a new worker by asking gunicorn to increase number of workers elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : # num_ready_workers_running == num_workers_running < num_workers_expected log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) time . sleep ( 10 ) if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( AirflowWebServerTimeout , OSError ) as err : log . error ( err ) log . error ( \"Shutting down webserver\" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def launch_workers : if state =='starting' : log. info ( \"'{0}' is launching workers now.\". format ( state ) ) log. debug ('starting {0} worker(s)'. format ( conf. getint ( 'webserver', 'num_workers' ) ) ) # Ensure the worker number and batch size are compatible with each other num_workers = conf. getint ( 'webserver', 'num_workers' ) batch_size = conf. getint ( 'webserver', 'worker_refresh_batch_size' ) if batch_size > num_workers : log. error ( \"batch_size={0} is larger than num_workers={1}, \" \"adjusting batch_size\". format ( batch_size, num_workers ) ) batch_size = num_workers elif batch_size < 1 : raise AirflowWebServerConfigError ( \"worker_refresh_batch_size={0} < 1 which is invalid\". format ( batch_size ) ) # This needs to be atomic in order for the SIGPWR signal to work process. _pickle_protocol_version = 2 # If the webserver is the master process, let it handle worker management if is_leader : state ='master' gunicorn_master_proc"
      ]
    ],
    "filtered_resps": [
      " def launch_workers : if state =='starting' : log. info ( \"'{0}' is launching workers now.\". format ( state ) ) log. debug ('starting {0} worker(s)'. format ( conf. getint ( 'webserver', 'num_workers' ) ) ) # Ensure the worker number and batch size are compatible with each other num_workers = conf. getint ( 'webserver', 'num_workers' ) batch_size = conf. getint ( 'webserver', 'worker_refresh_batch_size' ) if batch_size > num_workers : log. error ( \"batch_size={0} is larger than num_workers={1}, \" \"adjusting batch_size\". format ( batch_size, num_workers ) ) batch_size = num_workers elif batch_size < 1 : raise AirflowWebServerConfigError ( \"worker_refresh_batch_size={0} < 1 which is invalid\". format ( batch_size ) ) # This needs to be atomic in order for the SIGPWR signal to work process. _pickle_protocol_version = 2 # If the webserver is the master process, let it handle worker management if is_leader : state ='master' gunicorn_master_proc"
    ],
    "smoothed_bleu_4": 0.3741832807730007
  },
  {
    "doc_id": 37,
    "doc": {
      "id": 37,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_translate_hook.py",
      "func_name": "CloudTranslateHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client\n        \"\"\"\n        if not self._client:\n            self._client = Client(credentials=self._get_credentials())\n        return self._client",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "_client",
        ":",
        "self",
        ".",
        "_client",
        "=",
        "Client",
        "(",
        "credentials",
        "=",
        "self",
        ".",
        "_get_credentials",
        "(",
        ")",
        ")",
        "return",
        "self",
        ".",
        "_client"
      ],
      "docstring": "Retrieves connection to Cloud Translate\n\n        :return: Google Cloud Translate client object.\n        :rtype: Client",
      "docstring_tokens": [
        "Retrieves",
        "connection",
        "to",
        "Cloud",
        "Translate"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L34-L43"
    },
    "target": "Retrieves connection to Cloud Translate",
    "arguments": [
      [
        "def get_conn ( self ) : if not self . _client : self . _client = Client ( credentials = self . _get_credentials ( ) ) return self . _client",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nThis docstring describes a method that creates a connection to a service using the provided client and credentials. It explains that this method is part of a class that provides functionality for interacting with services, and it indicates that it raises an exception when a `Client` instance is missing. It also includes type hints and a blank line to enhance readability.\n\nHowever, this code snippet does not define which service the `Client` instance is connecting to, which is crucial information for understanding the context of the method. To address this, you can add an additional line to the docstring to specify the service. Here's an updated version of the docstring and the method:\n\n```python\nimport logging\n\nclass BaseService :\n    \"\"\"\n    Base class for services that provides functionality to interact with\n    a remote service using AWS SDK.\n\n    Attributes :\n        __client : AWS SDK client instance\n        __credentials : AWS SDK credentials\n\n    Methods :\n        get_conn() : Connects to the service using the client and credentials\n    \"\"\"\n\n    def __init__ ( self, service_name ) :\n        \"\"\"\n        Initializes the service with a specified service name.\n\n        Args :\n            service_name : Name of the service (e.g.,'s3', 'd"
      ]
    ],
    "filtered_resps": [
      " ```\n\nThis docstring describes a method that creates a connection to a service using the provided client and credentials. It explains that this method is part of a class that provides functionality for interacting with services, and it indicates that it raises an exception when a `Client` instance is missing. It also includes type hints and a blank line to enhance readability.\n\nHowever, this code snippet does not define which service the `Client` instance is connecting to, which is crucial information for understanding the context of the method. To address this, you can add an additional line to the docstring to specify the service. Here's an updated version of the docstring and the method:\n\n```python\nimport logging\n\nclass BaseService :\n    \"\"\"\n    Base class for services that provides functionality to interact with\n    a remote service using AWS SDK.\n\n    Attributes :\n        __client : AWS SDK client instance\n        __credentials : AWS SDK credentials\n\n    Methods :\n        get_conn() : Connects to the service using the client and credentials\n    \"\"\"\n\n    def __init__ ( self, service_name ) :\n        \"\"\"\n        Initializes the service with a specified service name.\n\n        Args :\n            service_name : Name of the service (e.g.,'s3', 'd"
    ],
    "smoothed_bleu_4": 0.5766449149223869
  },
  {
    "doc_id": 38,
    "doc": {
      "id": 38,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_translate_hook.py",
      "func_name": "CloudTranslateHook.translate",
      "original_string": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )",
      "language": "python",
      "code": "def translate(\n        self, values, target_language, format_=None, source_language=None, model=None\n    ):\n        \"\"\"Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.\n        \"\"\"\n        client = self.get_conn()\n\n        return client.translate(\n            values=values,\n            target_language=target_language,\n            format_=format_,\n            source_language=source_language,\n            model=model,\n        )",
      "code_tokens": [
        "def",
        "translate",
        "(",
        "self",
        ",",
        "values",
        ",",
        "target_language",
        ",",
        "format_",
        "=",
        "None",
        ",",
        "source_language",
        "=",
        "None",
        ",",
        "model",
        "=",
        "None",
        ")",
        ":",
        "client",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        "return",
        "client",
        ".",
        "translate",
        "(",
        "values",
        "=",
        "values",
        ",",
        "target_language",
        "=",
        "target_language",
        ",",
        "format_",
        "=",
        "format_",
        ",",
        "source_language",
        "=",
        "source_language",
        ",",
        "model",
        "=",
        "model",
        ",",
        ")"
      ],
      "docstring": "Translate a string or list of strings.\n\n        See https://cloud.google.com/translate/docs/translating-text\n\n        :type values: str or list\n        :param values: String or list of strings to translate.\n\n        :type target_language: str\n        :param target_language: The language to translate results into. This\n                                is required by the API and defaults to\n                                the target language of the current instance.\n\n        :type format_: str\n        :param format_: (Optional) One of ``text`` or ``html``, to specify\n                        if the input text is plain text or HTML.\n\n        :type source_language: str or None\n        :param source_language: (Optional) The language of the text to\n                                be translated.\n\n        :type model: str or None\n        :param model: (Optional) The model used to translate the text, such\n                      as ``'base'`` or ``'nmt'``.\n\n        :rtype: str or list\n        :returns: A list of dictionaries for each queried value. Each\n                  dictionary typically contains three keys (though not\n                  all will be present in all cases)\n\n                  * ``detectedSourceLanguage``: The detected language (as an\n                    ISO 639-1 language code) of the text.\n                  * ``translatedText``: The translation of the text into the\n                    target language.\n                  * ``input``: The corresponding input value.\n                  * ``model``: The model used to translate the text.\n\n                  If only a single value is passed, then only a single\n                  dictionary will be returned.\n        :raises: :class:`~exceptions.ValueError` if the number of\n                 values and translations differ.",
      "docstring_tokens": [
        "Translate",
        "a",
        "string",
        "or",
        "list",
        "of",
        "strings",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L45-L97"
    },
    "target": "Translate a string or list of strings .",
    "arguments": [
      [
        "def translate ( self , values , target_language , format_ = None , source_language = None , model = None ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ = format_ , source_language = source_language , model = model , )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " >>> from google.cloud import translate_v2 as translate >>> client. translate ( values = [ \"Hello. I'm an English speaker\" ], target_language = \"es\", source_language = \"en\", model = \"es-en\" ) { \"translatedText\" : \"Hola. Soy un hablante de ingl\u0413\u00ads\" } translate. Client ``` ``` >>> from google.cloud import translate_v2 as translate >>> client = translate. Client () client. get_conn ( ) >>> client = translate. Client ( client = None ) >>> client. get_conn () Traceback ( most recent call last ):... AttributeError :'TranslateClient'object has no attribute '_client ``` ``` >>> from google.cloud.translate_v2 import Client >>> client = Client ( ) client. get_conn ( ) >>> client = Client ( client = None ) >>> client. get_conn () Traceback ( most recent call last ):... AttributeError :'TranslateClient'object has no attribute '_client ```\n    - update_model ( name, file_path, metadata=None, api_version=None ) source\n    - Updates the specified machine translation model. Note that this method will not perform any validation on the model data. ``` >>> from google.cloud import translate_v2 as translate >>>"
      ]
    ],
    "filtered_resps": [
      " >>> from google.cloud import translate_v2 as translate >>> client. translate ( values = [ \"Hello. I'm an English speaker\" ], target_language = \"es\", source_language = \"en\", model = \"es-en\" ) { \"translatedText\" : \"Hola. Soy un hablante de ingl\u0413\u00ads\" } translate. Client ``` ``` >>> from google.cloud import translate_v2 as translate >>> client = translate. Client () client. get_conn ( ) >>> client = translate. Client ( client = None ) >>> client. get_conn () Traceback ( most recent call last ):... AttributeError :'TranslateClient'object has no attribute '_client ``` ``` >>> from google.cloud.translate_v2 import Client >>> client = Client ( ) client. get_conn ( ) >>> client = Client ( client = None ) >>> client. get_conn () Traceback ( most recent call last ):... AttributeError :'TranslateClient'object has no attribute '_client ```\n    - update_model ( name, file_path, metadata=None, api_version=None ) source\n    - Updates the specified machine translation model. Note that this method will not perform any validation on the model data. ``` >>> from google.cloud import translate_v2 as translate >>>"
    ],
    "smoothed_bleu_4": 0.3908665686434188
  },
  {
    "doc_id": 39,
    "doc": {
      "id": 39,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.get_instance",
      "original_string": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_instance(self, instance, project_id=None):\n        \"\"\"\n        Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().instances().get(\n            project=project_id,\n            instance=instance\n        ).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "get",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Retrieves a resource containing information about a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL instance resource.\n        :rtype: dict",
      "docstring_tokens": [
        "Retrieves",
        "a",
        "resource",
        "containing",
        "information",
        "about",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L97-L112"
    },
    "target": "Retrieves a resource containing information about a Cloud SQL instance .",
    "arguments": [
      [
        "def get_instance ( self , instance , project_id = None ) : return self . get_conn ( ) . instances ( ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " get_conn : return self. _connect ( ) _connect : return self. conn. _connection ( ) conn : return self. _connect ( )\nIn the example above, the\u00a0 `get_instance` \u00a0method is the entry point for getting an instance. It first calls the\u00a0 `get_conn` \u00a0method to create a connection object. The\u00a0 `get_conn` \u00a0method attempts to connect to the cloud using the\u00a0 `_connect` \u00a0method, which is the actual method that creates the connection. The\u00a0 `_connect` \u00a0method creates a connection instance and returns it. The connection object is then used to get the instance using ` instances().get(project=project_id, instance=instance).execute() `. The `num_retries` parameter is set to the number of retries specified in the class.\n\nIn terms of testing, your use case could be testing the behavior of the `get_instance` method under different conditions, such as:\n\n1. Valid instance return: Make sure that when a valid instance is provided, the method returns the instance successfully.\n\n2. Instance not found: Test that when an instance does not exist in a project, a suitable error is raised or a default is returned.\n\n3."
      ]
    ],
    "filtered_resps": [
      " get_conn : return self. _connect ( ) _connect : return self. conn. _connection ( ) conn : return self. _connect ( )\nIn the example above, the\u00a0 `get_instance` \u00a0method is the entry point for getting an instance. It first calls the\u00a0 `get_conn` \u00a0method to create a connection object. The\u00a0 `get_conn` \u00a0method attempts to connect to the cloud using the\u00a0 `_connect` \u00a0method, which is the actual method that creates the connection. The\u00a0 `_connect` \u00a0method creates a connection instance and returns it. The connection object is then used to get the instance using ` instances().get(project=project_id, instance=instance).execute() `. The `num_retries` parameter is set to the number of retries specified in the class.\n\nIn terms of testing, your use case could be testing the behavior of the `get_instance` method under different conditions, such as:\n\n1. Valid instance return: Make sure that when a valid instance is provided, the method returns the instance successfully.\n\n2. Instance not found: Test that when an instance does not exist in a project, a suitable error is raised or a default is returned.\n\n3."
    ],
    "smoothed_bleu_4": 0.7163300463119544
  },
  {
    "doc_id": 40,
    "doc": {
      "id": 40,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.create_instance",
      "original_string": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def create_instance(self, body, project_id=None):\n        \"\"\"\n        Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().insert(\n            project=project_id,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_instance",
        "(",
        "self",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "insert",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new Cloud SQL instance.\n\n        :param body: Body required by the Cloud SQL insert API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L115-L133"
    },
    "target": "Creates a new Cloud SQL instance .",
    "arguments": [
      [
        "def create_instance ( self , body , project_id = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return self. get_conn ( ). instances ( ). get ( self. _instance_name ) def _wait_for_operation_to_complete ( self, project_id, operation_name ) : timeout = 20 # 10 * num_retries num_times_to_wait = 2 if hasattr ( self. _options, 'timeout' ) else 1 for i in range ( num_times_to_wait ) : if self. _has_completed_operation ( project_id, operation_name ) : return time. sleep ( timeout ) self. _increment_retry ( ) def _has_completed_operation ( self, project_id, operation_name ) : # TODO: Add a timeout for this function operation_ref = self. get_conn ( ). operations ( ). get ( operation_name ) status = operation_ref. get_status ( ) if status ['state' ]!= 'DONE' : return False return True def _increment_retry ( self ) : if hasattr ( self. _options, 'timeout' ) : self. _options. timeout -= 1 def get_conn ( self ) : # Get a connection based on the specified options connection = googleapiclient. discovery. build ( 'compute', 'v1', credentials = self. access )"
      ]
    ],
    "filtered_resps": [
      " return self. get_conn ( ). instances ( ). get ( self. _instance_name ) def _wait_for_operation_to_complete ( self, project_id, operation_name ) : timeout = 20 # 10 * num_retries num_times_to_wait = 2 if hasattr ( self. _options, 'timeout' ) else 1 for i in range ( num_times_to_wait ) : if self. _has_completed_operation ( project_id, operation_name ) : return time. sleep ( timeout ) self. _increment_retry ( ) def _has_completed_operation ( self, project_id, operation_name ) : # TODO: Add a timeout for this function operation_ref = self. get_conn ( ). operations ( ). get ( operation_name ) status = operation_ref. get_status ( ) if status ['state' ]!= 'DONE' : return False return True def _increment_retry ( self ) : if hasattr ( self. _options, 'timeout' ) : self. _options. timeout -= 1 def get_conn ( self ) : # Get a connection based on the specified options connection = googleapiclient. discovery. build ( 'compute', 'v1', credentials = self. access )"
    ],
    "smoothed_bleu_4": 0.48340867685721445
  },
  {
    "doc_id": 41,
    "doc": {
      "id": 41,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.patch_instance",
      "original_string": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def patch_instance(self, body, instance, project_id=None):\n        \"\"\"\n        Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().patch(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "patch_instance",
        "(",
        "self",
        ",",
        "body",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates settings of a Cloud SQL instance.\n\n        Caution: This is not a partial update, so you must include values for\n        all the settings that you want to retain.\n\n        :param body: Body required by the Cloud SQL patch API, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n        :type body: dict\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "settings",
        "of",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L136-L160"
    },
    "target": "Updates settings of a Cloud SQL instance .",
    "arguments": [
      [
        "def patch_instance ( self , body , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n[FILEPATH] novaclient/v2/api.py [/FILEPATH]\n\"\"\"\nThis module provides the `compute_v2\\Api` class which is a subclass of\n:class:`novaclient.base.ClientBase`.\n\"\"\"\nimport warnings\n\nfrom novaclient import api_versions, base, common\nfrom novaclient.v1 import api as v1\n\nAPI_VERSIONS = [\n    v1.v1,\n    api_versions.V200\n]\n\n\nclass ComputeV2Client(base.ClientBase):\n    \"\"\"Client for the new OpenStack Compute v2 API.\n\n    This is the current API, as of the OpenStack Essex release.\n    \"\"\"\n\n    api_versions = API_VERSIONS\n\n    user_agent = \"novaclient/%(version)s\"\n\n    def __init__(self, **kwargs):\n        try:\n            super(ComputeV2Client, self).__init__(**kwargs)\n            if common.NovaVersion not in kwargs:\n                kwargs[\"novaversion\"] = \"2.0\"\n        except TypeError:\n            # Old way of instantiating the client\n            warnings.warn(\"Passing a `novaversion` kwarg is deprecated.\",\n                          DeprecationWarning)\n\n    def disable_rollover_retries ( self ) :\n        self. num"
      ]
    ],
    "filtered_resps": [
      " return response\n[FILEPATH] novaclient/v2/api.py [/FILEPATH]\n\"\"\"\nThis module provides the `compute_v2\\Api` class which is a subclass of\n:class:`novaclient.base.ClientBase`.\n\"\"\"\nimport warnings\n\nfrom novaclient import api_versions, base, common\nfrom novaclient.v1 import api as v1\n\nAPI_VERSIONS = [\n    v1.v1,\n    api_versions.V200\n]\n\n\nclass ComputeV2Client(base.ClientBase):\n    \"\"\"Client for the new OpenStack Compute v2 API.\n\n    This is the current API, as of the OpenStack Essex release.\n    \"\"\"\n\n    api_versions = API_VERSIONS\n\n    user_agent = \"novaclient/%(version)s\"\n\n    def __init__(self, **kwargs):\n        try:\n            super(ComputeV2Client, self).__init__(**kwargs)\n            if common.NovaVersion not in kwargs:\n                kwargs[\"novaversion\"] = \"2.0\"\n        except TypeError:\n            # Old way of instantiating the client\n            warnings.warn(\"Passing a `novaversion` kwarg is deprecated.\",\n                          DeprecationWarning)\n\n    def disable_rollover_retries ( self ) :\n        self. num"
    ],
    "smoothed_bleu_4": 0.5618280756251459
  },
  {
    "doc_id": 42,
    "doc": {
      "id": 42,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.delete_instance",
      "original_string": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def delete_instance(self, instance, project_id=None):\n        \"\"\"\n        Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().instances().delete(\n            project=project_id,\n            instance=instance,\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "delete_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "delete",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Deletes a Cloud SQL instance.\n\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :param instance: Cloud SQL instance ID. This does not include the project ID.\n        :type instance: str\n        :return: None",
      "docstring_tokens": [
        "Deletes",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L163-L180"
    },
    "target": "Deletes a Cloud SQL instance .",
    "arguments": [
      [
        "def delete_instance ( self , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _wait_for_operation_to_complete ( self, project_id, operation_name ) : ``` ``` operation = self. get_conn ( ). get_operation ( project = project_id, name = operation_name, ) operation. wait_for_status ( ) self. _raise_for_status ( operation [ \"metadata\" ] [\"items\" ] [ 0 ][ \"status\" ] ) ``` ```\n    - def enable_instance ( self, instance, project_id=None, zone=None ) \u00b6\n    - Enables an existing instance. Note: This operation can be used to enable an instance that has been disabled due to low disk usage.\n      - Parameters\n      -\n        - instance ( str ) \u2013 The instance reference (e.g., projects/[PROJECT_ID]/zones/[ZONE]/instances/[INSTANCE_NAME]) that identifies the instance to update.\n        - project_id ( str ) \u2013 This argument is deprecated in favor of zone. To enable an instance in a region, the project ID is not required and the zone will specify the region name. To enable an instance in a zone, the project ID and zone must be specified.\n        - zone ( str ) \u2013 The zone in which to enable the instance. If not provided, the project ID must be specified.\n\n     "
      ]
    ],
    "filtered_resps": [
      " def _wait_for_operation_to_complete ( self, project_id, operation_name ) : ``` ``` operation = self. get_conn ( ). get_operation ( project = project_id, name = operation_name, ) operation. wait_for_status ( ) self. _raise_for_status ( operation [ \"metadata\" ] [\"items\" ] [ 0 ][ \"status\" ] ) ``` ```\n    - def enable_instance ( self, instance, project_id=None, zone=None ) \u00b6\n    - Enables an existing instance. Note: This operation can be used to enable an instance that has been disabled due to low disk usage.\n      - Parameters\n      -\n        - instance ( str ) \u2013 The instance reference (e.g., projects/[PROJECT_ID]/zones/[ZONE]/instances/[INSTANCE_NAME]) that identifies the instance to update.\n        - project_id ( str ) \u2013 This argument is deprecated in favor of zone. To enable an instance in a region, the project ID is not required and the zone will specify the region name. To enable an instance in a zone, the project ID and zone must be specified.\n        - zone ( str ) \u2013 The zone in which to enable the instance. If not provided, the project ID must be specified.\n\n     "
    ],
    "smoothed_bleu_4": 0.5645055314870061
  },
  {
    "doc_id": 43,
    "doc": {
      "id": 43,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.get_database",
      "original_string": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_database(self, instance, database, project_id=None):\n        \"\"\"\n        Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().databases().get(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "get",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Retrieves a database resource from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: A Cloud SQL database resource, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n        :rtype: dict",
      "docstring_tokens": [
        "Retrieves",
        "a",
        "database",
        "resource",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L183-L202"
    },
    "target": "Retrieves a database resource from a Cloud SQL instance .",
    "arguments": [
      [
        "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". data\n\n## Step 4: Define the class method for the instance list\n```python\ndef get_instances ( self, project_id = None ) :\n     return self. get_conn ( ). instances ( ). list ( project__in = [ project_id ], num_retries = self. num_retries )\n```\n\n## Step 5: Define the class method for the project list\n```python\ndef get_projects ( self ) :\n     return self. get_conn ( ). projects ( ). list ( num_retries = self. num_retries )\n```\n\n## Step 6: Define the class method for getting the database user\n```python\ndef get_user ( self, instance, database, project_id = None ) :\n     return self. get_conn ( ). databases ( ). get ( project = project_id, instance = instance, database = database )\n```\n\n## Step 7: Define the class method for getting the instance\n```python\ndef get_instance ( self, instance ) :\n     return self. get_conn ( ). instances ( ). get ( instance = instance )\n```\n\n## Step 8: Define the class method for getting the project\n```python\ndef get_project"
      ]
    ],
    "filtered_resps": [
      ". data\n\n## Step 4: Define the class method for the instance list\n```python\ndef get_instances ( self, project_id = None ) :\n     return self. get_conn ( ). instances ( ). list ( project__in = [ project_id ], num_retries = self. num_retries )\n```\n\n## Step 5: Define the class method for the project list\n```python\ndef get_projects ( self ) :\n     return self. get_conn ( ). projects ( ). list ( num_retries = self. num_retries )\n```\n\n## Step 6: Define the class method for getting the database user\n```python\ndef get_user ( self, instance, database, project_id = None ) :\n     return self. get_conn ( ). databases ( ). get ( project = project_id, instance = instance, database = database )\n```\n\n## Step 7: Define the class method for getting the instance\n```python\ndef get_instance ( self, instance ) :\n     return self. get_conn ( ). instances ( ). get ( instance = instance )\n```\n\n## Step 8: Define the class method for getting the project\n```python\ndef get_project"
    ],
    "smoothed_bleu_4": 0.5018419820154427
  },
  {
    "doc_id": 44,
    "doc": {
      "id": 44,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.create_database",
      "original_string": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def create_database(self, instance, body, project_id=None):\n        \"\"\"\n        Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().insert(\n            project=project_id,\n            instance=instance,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "insert",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new database inside a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "database",
        "inside",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L205-L226"
    },
    "target": "Creates a new database inside a Cloud SQL instance .",
    "arguments": [
      [
        "def create_database ( self , instance , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n\ndef delete_database ( self, instance, body, project_id = None, instance_zone = None ) : response = self. get_conn ( ). databases ( ). delete ( project = project_id, instance = instance, instanceZone = instance_zone, body = body ). execute ( num_retries = self. num_retries ) operation_name = response [ \"name\" ] self. _wait_for_operation_to_complete ( project_id = project_id, operation_name = operation_name ) return response\n\ndef _wait_for_operation_to_complete ( self, project_id, operation_name ) : # Create a reference to the operation # If we need to retry, the operation won\u2019t be available yet, and will be deleted. # We can safely assume it will be available after a short delay. # See https://cloud.google.com/stackdriver-monitoring/support/limitations#retryable for details. operation = self. get_conn ( ). projects ( project = project_id ). operations ( name = operation_name ). get ( ) self. _check_valid_operation ( operation = operation ) if operation [ \"status\" ] [ \"state\" ]!= \"DONE\" : time. sleep ( 5 ) self. _check"
      ]
    ],
    "filtered_resps": [
      " return response\n\ndef delete_database ( self, instance, body, project_id = None, instance_zone = None ) : response = self. get_conn ( ). databases ( ). delete ( project = project_id, instance = instance, instanceZone = instance_zone, body = body ). execute ( num_retries = self. num_retries ) operation_name = response [ \"name\" ] self. _wait_for_operation_to_complete ( project_id = project_id, operation_name = operation_name ) return response\n\ndef _wait_for_operation_to_complete ( self, project_id, operation_name ) : # Create a reference to the operation # If we need to retry, the operation won\u2019t be available yet, and will be deleted. # We can safely assume it will be available after a short delay. # See https://cloud.google.com/stackdriver-monitoring/support/limitations#retryable for details. operation = self. get_conn ( ). projects ( project = project_id ). operations ( name = operation_name ). get ( ) self. _check_valid_operation ( operation = operation ) if operation [ \"status\" ] [ \"state\" ]!= \"DONE\" : time. sleep ( 5 ) self. _check"
    ],
    "smoothed_bleu_4": 0.5665490088279119
  },
  {
    "doc_id": 45,
    "doc": {
      "id": 45,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.patch_database",
      "original_string": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def patch_database(self, instance, database, body, project_id=None):\n        \"\"\"\n        Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().patch(\n            project=project_id,\n            instance=instance,\n            database=database,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "patch_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates a database resource inside a Cloud SQL instance.\n\n        This method supports patch semantics.\n        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be updated in the instance.\n        :type database: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "a",
        "database",
        "resource",
        "inside",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L229-L256"
    },
    "target": "Updates a database resource inside a Cloud SQL instance .",
    "arguments": [
      [
        "def patch_database ( self , instance , database , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " response [ \"database\" ] return response\npatch_database ( self, instance, database, body, project_id = None, wait = True ) : \"\"\" Patch database in project. Args: instance: instance to modify database: database to modify body: body of request project_id ( optional ): project ID. If None then project ID is deduced from instance Args: project_id: project ID. If None then project ID is deduced from instance Args: wait: bool, True to wait for completion\nreturn self. patch_database ( self, instance, database, body, project_id = project_id, wait = wait )\ndef start_backup ( self, instance, body, project_id = None ) : \"\"\" Start backup for instance. Args: instance: instance to backup body: body of request project_id ( optional ): project ID. If None then project ID is deduced from instance Args: project_id: project ID. If None then project ID is deduced from instance\nresponse = self. get_conn ( ). instances ( ). get ( project = project_id, id = instance, headers = { 'X-Auth-Token' : token } )\nresponse = self. get_conn ( ). instances ( ). list ( project ="
      ]
    ],
    "filtered_resps": [
      " response [ \"database\" ] return response\npatch_database ( self, instance, database, body, project_id = None, wait = True ) : \"\"\" Patch database in project. Args: instance: instance to modify database: database to modify body: body of request project_id ( optional ): project ID. If None then project ID is deduced from instance Args: project_id: project ID. If None then project ID is deduced from instance Args: wait: bool, True to wait for completion\nreturn self. patch_database ( self, instance, database, body, project_id = project_id, wait = wait )\ndef start_backup ( self, instance, body, project_id = None ) : \"\"\" Start backup for instance. Args: instance: instance to backup body: body of request project_id ( optional ): project ID. If None then project ID is deduced from instance Args: project_id: project ID. If None then project ID is deduced from instance\nresponse = self. get_conn ( ). instances ( ). get ( project = project_id, id = instance, headers = { 'X-Auth-Token' : token } )\nresponse = self. get_conn ( ). instances ( ). list ( project ="
    ],
    "smoothed_bleu_4": 0.5945270090983975
  },
  {
    "doc_id": 46,
    "doc": {
      "id": 46,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.delete_database",
      "original_string": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "language": "python",
      "code": "def delete_database(self, instance, database, project_id=None):\n        \"\"\"\n        Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().databases().delete(\n            project=project_id,\n            instance=instance,\n            database=database\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(project_id=project_id,\n                                             operation_name=operation_name)",
      "code_tokens": [
        "def",
        "delete_database",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "database",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "databases",
        "(",
        ")",
        ".",
        "delete",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "database",
        "=",
        "database",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Deletes a database from a Cloud SQL instance.\n\n        :param instance: Database instance ID. This does not include the project ID.\n        :type instance: str\n        :param database: Name of the database to be deleted in the instance.\n        :type database: str\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Deletes",
        "a",
        "database",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L259-L279"
    },
    "target": "Deletes a database from a Cloud SQL instance .",
    "arguments": [
      [
        "def delete_database ( self , instance , database , project_id = None ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return response\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 60, jitter = 0.1, max_retries = 10, exceptions = ( Exception, ), logger_name = \"google-cloud\" )\ndef wait_for_instance_started ( self, instance ) : start_time = time. time ( ) for _ in range ( self. num_retries + 1 ) : get_operation = self. get_conn ( ). instances ( ). get ( name = instance, fields = \"status\" ) if get_operation [ \"status\" ] == \"RUNNING\" : return instance, get_operation [ \"status\" ] time. sleep ( 2 )\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 120, jitter = 0.1, max_retries = 20, exceptions = ( Exception, ), logger_name = \"google-cloud\" )\ndef wait_for_instance_terminated ( self, instance, status ) : end_time = time. time ( ) for _ in range ( self. num_retries + 1 ) : get_operation = self. get_conn ( ). instances ( ). get"
      ]
    ],
    "filtered_resps": [
      " return response\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 60, jitter = 0.1, max_retries = 10, exceptions = ( Exception, ), logger_name = \"google-cloud\" )\ndef wait_for_instance_started ( self, instance ) : start_time = time. time ( ) for _ in range ( self. num_retries + 1 ) : get_operation = self. get_conn ( ). instances ( ). get ( name = instance, fields = \"status\" ) if get_operation [ \"status\" ] == \"RUNNING\" : return instance, get_operation [ \"status\" ] time. sleep ( 2 )\n@ retries. retry ( wait = 1, backoff_factor = 2, max_delay = 120, jitter = 0.1, max_retries = 20, exceptions = ( Exception, ), logger_name = \"google-cloud\" )\ndef wait_for_instance_terminated ( self, instance, status ) : end_time = time. time ( ) for _ in range ( self. num_retries + 1 ) : get_operation = self. get_conn ( ). instances ( ). get"
    ],
    "smoothed_bleu_4": 0.5176326524981613
  },
  {
    "doc_id": 47,
    "doc": {
      "id": 47,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlHook.export_instance",
      "original_string": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )",
      "language": "python",
      "code": "def export_instance(self, instance, body, project_id=None):\n        \"\"\"\n        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        try:\n            response = self.get_conn().instances().export(\n                project=project_id,\n                instance=instance,\n                body=body\n            ).execute(num_retries=self.num_retries)\n            operation_name = response[\"name\"]\n            self._wait_for_operation_to_complete(project_id=project_id,\n                                                 operation_name=operation_name)\n        except HttpError as ex:\n            raise AirflowException(\n                'Exporting instance {} failed: {}'.format(instance, ex.content)\n            )",
      "code_tokens": [
        "def",
        "export_instance",
        "(",
        "self",
        ",",
        "instance",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "try",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "instances",
        "(",
        ")",
        ".",
        "export",
        "(",
        "project",
        "=",
        "project_id",
        ",",
        "instance",
        "=",
        "instance",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "project_id",
        "=",
        "project_id",
        ",",
        "operation_name",
        "=",
        "operation_name",
        ")",
        "except",
        "HttpError",
        "as",
        "ex",
        ":",
        "raise",
        "AirflowException",
        "(",
        "'Exporting instance {} failed: {}'",
        ".",
        "format",
        "(",
        "instance",
        ",",
        "ex",
        ".",
        "content",
        ")",
        ")"
      ],
      "docstring": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n        or CSV file.\n\n        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n            project ID.\n        :type instance: str\n        :param body: The request body, as described in\n            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n        :type body: dict\n        :param project_id: Project ID of the project that contains the instance. If set\n            to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Exports",
        "data",
        "from",
        "a",
        "Cloud",
        "SQL",
        "instance",
        "to",
        "a",
        "Cloud",
        "Storage",
        "bucket",
        "as",
        "a",
        "SQL",
        "dump",
        "or",
        "CSV",
        "file",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L282-L310"
    },
    "target": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file .",
    "arguments": [
      [
        "def export_instance ( self , instance , body , project_id = None ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name ) except HttpError as ex : raise AirflowException ( 'Exporting instance {} failed: {}' . format ( instance , ex . content ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " from ex def get_conn ( self ) : return self. _conn\n### Step 5: Update the Airflow configuration to use the custom provider\n\nUpdate your Airflow configuration (`airflow.cfg` file) to use the `export_instance` function from your custom provider.\n\n```markdown\n[googlecloudstorageconnection]\nexport_instance = google.cloud.storage.provider:export_instance\n```\n\n### Step 6: Use the custom provider in Airflow\n\nNow, you should be able to use the custom provider in Airflow by specifying the `export_instance` function in your DAG.\n\n```yaml\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n   'start_date': datetime(2019, 1, 1),\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\nt1 = PythonOperator(\n    task_id='export_instance',\n    python_callable=operator_export_instance,\n)\n```\n\nNote that the `operator_export_instance` function is not real, but rather a placeholder for the actual function that uses the `export_instance` function from the custom provider.\n\n### Step 7: Test the custom"
      ]
    ],
    "filtered_resps": [
      " from ex def get_conn ( self ) : return self. _conn\n### Step 5: Update the Airflow configuration to use the custom provider\n\nUpdate your Airflow configuration (`airflow.cfg` file) to use the `export_instance` function from your custom provider.\n\n```markdown\n[googlecloudstorageconnection]\nexport_instance = google.cloud.storage.provider:export_instance\n```\n\n### Step 6: Use the custom provider in Airflow\n\nNow, you should be able to use the custom provider in Airflow by specifying the `export_instance` function in your DAG.\n\n```yaml\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n   'start_date': datetime(2019, 1, 1),\n}\n\ndag = DAG(\n    'example_dag',\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n)\n\nt1 = PythonOperator(\n    task_id='export_instance',\n    python_callable=operator_export_instance,\n)\n```\n\nNote that the `operator_export_instance` function is not real, but rather a placeholder for the actual function that uses the `export_instance` function from the custom provider.\n\n### Step 7: Test the custom"
    ],
    "smoothed_bleu_4": 0.6340437533925491
  },
  {
    "doc_id": 48,
    "doc": {
      "id": 48,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.start_proxy",
      "original_string": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return",
      "language": "python",
      "code": "def start_proxy(self):\n        \"\"\"\n        Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        if self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is already running: {}\".format(\n                self.sql_proxy_process))\n        else:\n            command_to_run = [self.sql_proxy_path]\n            command_to_run.extend(self.command_line_parameters)\n            try:\n                self.log.info(\"Creating directory %s\",\n                              self.cloud_sql_proxy_socket_directory)\n                os.makedirs(self.cloud_sql_proxy_socket_directory)\n            except OSError:\n                # Needed for python 2 compatibility (exists_ok missing)\n                pass\n            command_to_run.extend(self._get_credential_parameters())\n            self.log.info(\"Running the command: `%s`\", \" \".join(command_to_run))\n            self.sql_proxy_process = Popen(command_to_run,\n                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)\n            self.log.info(\"The pid of cloud_sql_proxy: %s\", self.sql_proxy_process.pid)\n            while True:\n                line = self.sql_proxy_process.stderr.readline().decode('utf-8')\n                return_code = self.sql_proxy_process.poll()\n                if line == '' and return_code is not None:\n                    self.sql_proxy_process = None\n                    raise AirflowException(\n                        \"The cloud_sql_proxy finished early with return code {}!\".format(\n                            return_code))\n                if line != '':\n                    self.log.info(line)\n                if \"googleapi: Error\" in line or \"invalid instance name:\" in line:\n                    self.stop_proxy()\n                    raise AirflowException(\n                        \"Error when starting the cloud_sql_proxy {}!\".format(\n                            line))\n                if \"Ready for new connections\" in line:\n                    return",
      "code_tokens": [
        "def",
        "start_proxy",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "_download_sql_proxy_if_needed",
        "(",
        ")",
        "if",
        "self",
        ".",
        "sql_proxy_process",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"The sql proxy is already running: {}\"",
        ".",
        "format",
        "(",
        "self",
        ".",
        "sql_proxy_process",
        ")",
        ")",
        "else",
        ":",
        "command_to_run",
        "=",
        "[",
        "self",
        ".",
        "sql_proxy_path",
        "]",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "command_line_parameters",
        ")",
        "try",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Creating directory %s\"",
        ",",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "os",
        ".",
        "makedirs",
        "(",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "except",
        "OSError",
        ":",
        "# Needed for python 2 compatibility (exists_ok missing)",
        "pass",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "_get_credential_parameters",
        "(",
        ")",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Running the command: `%s`\"",
        ",",
        "\" \"",
        ".",
        "join",
        "(",
        "command_to_run",
        ")",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "Popen",
        "(",
        "command_to_run",
        ",",
        "stdin",
        "=",
        "PIPE",
        ",",
        "stdout",
        "=",
        "PIPE",
        ",",
        "stderr",
        "=",
        "PIPE",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"The pid of cloud_sql_proxy: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "pid",
        ")",
        "while",
        "True",
        ":",
        "line",
        "=",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "stderr",
        ".",
        "readline",
        "(",
        ")",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "return_code",
        "=",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "poll",
        "(",
        ")",
        "if",
        "line",
        "==",
        "''",
        "and",
        "return_code",
        "is",
        "not",
        "None",
        ":",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "None",
        "raise",
        "AirflowException",
        "(",
        "\"The cloud_sql_proxy finished early with return code {}!\"",
        ".",
        "format",
        "(",
        "return_code",
        ")",
        ")",
        "if",
        "line",
        "!=",
        "''",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "line",
        ")",
        "if",
        "\"googleapi: Error\"",
        "in",
        "line",
        "or",
        "\"invalid instance name:\"",
        "in",
        "line",
        ":",
        "self",
        ".",
        "stop_proxy",
        "(",
        ")",
        "raise",
        "AirflowException",
        "(",
        "\"Error when starting the cloud_sql_proxy {}!\"",
        ".",
        "format",
        "(",
        "line",
        ")",
        ")",
        "if",
        "\"Ready for new connections\"",
        "in",
        "line",
        ":",
        "return"
      ],
      "docstring": "Starts Cloud SQL Proxy.\n\n        You have to remember to stop the proxy if you started it!",
      "docstring_tokens": [
        "Starts",
        "Cloud",
        "SQL",
        "Proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L524-L565"
    },
    "target": "Starts Cloud SQL Proxy .",
    "arguments": [
      [
        "def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : # Needed for python 2 compatibility (exists_ok missing) pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self . log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) if line != '' : self . log . info ( line ) if \"googleapi: Error\" in line or \"invalid instance name:\" in line : self . stop_proxy ( ) raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) if \"Ready for new connections\" in line : return",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` This code is complex because it uses a combination of tools such as `subprocess`, `os`, and `google.oauth2` to interact with the `cloud_sql_proxy` process. Additionally, it includes error handling and edge cases such as checking if the process is already running or if the connection to the proxy server fails.\n\nIn summary, the `start_proxy` method in the `CloudSqlProxyOperator` class ensures that the `cloud_sql_proxy` process is running and configured correctly, and it handles errors and edge cases accordingly.\n\nLet me know if you have any further questions or if you'd like me to clarify anything! \n\nPlease let me know if you want me to explain any part of this code, or if you want me to write an improved version. \n\nHere's the improved version of `start_proxy` method with more comments and improvements:\n\n```python\ndef start_proxy(self):\n    '''\n    Start and manage the cloud_sql_proxy process.\n\n    The process is started with credentials, instance connection name,\n    and tunnel mode. If the proxy process is already running, it raises an\n    exception. If the proxy process fails, the AirflowOperator raises an\n    exception.\n\n    :return: None\n    '''\n   "
      ]
    ],
    "filtered_resps": [
      " ``` This code is complex because it uses a combination of tools such as `subprocess`, `os`, and `google.oauth2` to interact with the `cloud_sql_proxy` process. Additionally, it includes error handling and edge cases such as checking if the process is already running or if the connection to the proxy server fails.\n\nIn summary, the `start_proxy` method in the `CloudSqlProxyOperator` class ensures that the `cloud_sql_proxy` process is running and configured correctly, and it handles errors and edge cases accordingly.\n\nLet me know if you have any further questions or if you'd like me to clarify anything! \n\nPlease let me know if you want me to explain any part of this code, or if you want me to write an improved version. \n\nHere's the improved version of `start_proxy` method with more comments and improvements:\n\n```python\ndef start_proxy(self):\n    '''\n    Start and manage the cloud_sql_proxy process.\n\n    The process is started with credentials, instance connection name,\n    and tunnel mode. If the proxy process is already running, it raises an\n    exception. If the proxy process fails, the AirflowOperator raises an\n    exception.\n\n    :return: None\n    '''\n   "
    ],
    "smoothed_bleu_4": 0.5628741380275326
  },
  {
    "doc_id": 49,
    "doc": {
      "id": 49,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.stop_proxy",
      "original_string": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)",
      "language": "python",
      "code": "def stop_proxy(self):\n        \"\"\"\n        Stops running proxy.\n\n        You should stop the proxy after you stop using it.\n        \"\"\"\n        if not self.sql_proxy_process:\n            raise AirflowException(\"The sql proxy is not started yet\")\n        else:\n            self.log.info(\"Stopping the cloud_sql_proxy pid: %s\",\n                          self.sql_proxy_process.pid)\n            self.sql_proxy_process.kill()\n            self.sql_proxy_process = None\n        # Cleanup!\n        self.log.info(\"Removing the socket directory: %s\",\n                      self.cloud_sql_proxy_socket_directory)\n        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)\n        if self.sql_proxy_was_downloaded:\n            self.log.info(\"Removing downloaded proxy: %s\", self.sql_proxy_path)\n            # Silently ignore if the file has already been removed (concurrency)\n            try:\n                os.remove(self.sql_proxy_path)\n            except OSError as e:\n                if not e.errno == errno.ENOENT:\n                    raise\n        else:\n            self.log.info(\"Skipped removing proxy - it was not downloaded: %s\",\n                          self.sql_proxy_path)\n        if os.path.isfile(self.credentials_path):\n            self.log.info(\"Removing generated credentials file %s\",\n                          self.credentials_path)\n            # Here file cannot be delete by concurrent task (each task has its own copy)\n            os.remove(self.credentials_path)",
      "code_tokens": [
        "def",
        "stop_proxy",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "sql_proxy_process",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"The sql proxy is not started yet\"",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Stopping the cloud_sql_proxy pid: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "pid",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        ".",
        "kill",
        "(",
        ")",
        "self",
        ".",
        "sql_proxy_process",
        "=",
        "None",
        "# Cleanup!",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing the socket directory: %s\"",
        ",",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ")",
        "shutil",
        ".",
        "rmtree",
        "(",
        "self",
        ".",
        "cloud_sql_proxy_socket_directory",
        ",",
        "ignore_errors",
        "=",
        "True",
        ")",
        "if",
        "self",
        ".",
        "sql_proxy_was_downloaded",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing downloaded proxy: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "# Silently ignore if the file has already been removed (concurrency)",
        "try",
        ":",
        "os",
        ".",
        "remove",
        "(",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "except",
        "OSError",
        "as",
        "e",
        ":",
        "if",
        "not",
        "e",
        ".",
        "errno",
        "==",
        "errno",
        ".",
        "ENOENT",
        ":",
        "raise",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Skipped removing proxy - it was not downloaded: %s\"",
        ",",
        "self",
        ".",
        "sql_proxy_path",
        ")",
        "if",
        "os",
        ".",
        "path",
        ".",
        "isfile",
        "(",
        "self",
        ".",
        "credentials_path",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Removing generated credentials file %s\"",
        ",",
        "self",
        ".",
        "credentials_path",
        ")",
        "# Here file cannot be delete by concurrent task (each task has its own copy)",
        "os",
        ".",
        "remove",
        "(",
        "self",
        ".",
        "credentials_path",
        ")"
      ],
      "docstring": "Stops running proxy.\n\n        You should stop the proxy after you stop using it.",
      "docstring_tokens": [
        "Stops",
        "running",
        "proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L567-L599"
    },
    "target": "Stops running proxy .",
    "arguments": [
      [
        "def stop_proxy ( self ) : if not self . sql_proxy_process : raise AirflowException ( \"The sql proxy is not started yet\" ) else : self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = None # Cleanup! self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) if self . sql_proxy_was_downloaded : self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) # Silently ignore if the file has already been removed (concurrency) try : os . remove ( self . sql_proxy_path ) except OSError as e : if not e . errno == errno . ENOENT : raise else : self . log . info ( \"Skipped removing proxy - it was not downloaded: %s\" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) # Here file cannot be delete by concurrent task (each task has its own copy) os . remove ( self . credentials_path )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " + @task def stop_cloud_sql_proxy_for_all_dags_in_pool ( * args, ** kwargs ) : from airflow. db. connections. bases import BaseDbOp # Create an Airflow DB connection to a backend, using the given name. # We can't use a default connection for this, since we need to access # Airflow's own connection pool, to read the pool dags, and we also need # to stop the connection to the connection pool. with connections. connections. get_connection ( 'airflow-db', conn_type = BaseDbOp. DEFAULT_CONN_TYPE ). get_uri () as conn_uri: # Get all the pools which contain the given pool from which this function is called base_pools = Pool. query (). filter ( Pool. pool == kwargs [ 'pool' ]) << ( Pool. id & ( Pool. dags << ( DagModel. id = DagModel. dagbag. id )). id & ( DagModel. pool == kwargs [ 'pool' ])) << end DagBag. query (). filter ( Pool. id = kwargs [ 'pool' ]) base_pools >> ( pool_id = Pool. id ) # Now, for each pool which contains a DAG,"
      ]
    ],
    "filtered_resps": [
      " + @task def stop_cloud_sql_proxy_for_all_dags_in_pool ( * args, ** kwargs ) : from airflow. db. connections. bases import BaseDbOp # Create an Airflow DB connection to a backend, using the given name. # We can't use a default connection for this, since we need to access # Airflow's own connection pool, to read the pool dags, and we also need # to stop the connection to the connection pool. with connections. connections. get_connection ( 'airflow-db', conn_type = BaseDbOp. DEFAULT_CONN_TYPE ). get_uri () as conn_uri: # Get all the pools which contain the given pool from which this function is called base_pools = Pool. query (). filter ( Pool. pool == kwargs [ 'pool' ]) << ( Pool. id & ( Pool. dags << ( DagModel. id = DagModel. dagbag. id )). id & ( DagModel. pool == kwargs [ 'pool' ])) << end DagBag. query (). filter ( Pool. id = kwargs [ 'pool' ]) base_pools >> ( pool_id = Pool. id ) # Now, for each pool which contains a DAG,"
    ],
    "smoothed_bleu_4": 0.4466527277312615
  },
  {
    "doc_id": 50,
    "doc": {
      "id": 50,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlProxyRunner.get_proxy_version",
      "original_string": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
      "language": "python",
      "code": "def get_proxy_version(self):\n        \"\"\"\n        Returns version of the Cloud SQL Proxy.\n        \"\"\"\n        self._download_sql_proxy_if_needed()\n        command_to_run = [self.sql_proxy_path]\n        command_to_run.extend(['--version'])\n        command_to_run.extend(self._get_credential_parameters())\n        result = subprocess.check_output(command_to_run).decode('utf-8')\n        pattern = re.compile(\"^.*[V|v]ersion ([^;]*);.*$\")\n        m = pattern.match(result)\n        if m:\n            return m.group(1)\n        else:\n            return None",
      "code_tokens": [
        "def",
        "get_proxy_version",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "_download_sql_proxy_if_needed",
        "(",
        ")",
        "command_to_run",
        "=",
        "[",
        "self",
        ".",
        "sql_proxy_path",
        "]",
        "command_to_run",
        ".",
        "extend",
        "(",
        "[",
        "'--version'",
        "]",
        ")",
        "command_to_run",
        ".",
        "extend",
        "(",
        "self",
        ".",
        "_get_credential_parameters",
        "(",
        ")",
        ")",
        "result",
        "=",
        "subprocess",
        ".",
        "check_output",
        "(",
        "command_to_run",
        ")",
        ".",
        "decode",
        "(",
        "'utf-8'",
        ")",
        "pattern",
        "=",
        "re",
        ".",
        "compile",
        "(",
        "\"^.*[V|v]ersion ([^;]*);.*$\"",
        ")",
        "m",
        "=",
        "pattern",
        ".",
        "match",
        "(",
        "result",
        ")",
        "if",
        "m",
        ":",
        "return",
        "m",
        ".",
        "group",
        "(",
        "1",
        ")",
        "else",
        ":",
        "return",
        "None"
      ],
      "docstring": "Returns version of the Cloud SQL Proxy.",
      "docstring_tokens": [
        "Returns",
        "version",
        "of",
        "the",
        "Cloud",
        "SQL",
        "Proxy",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L601-L615"
    },
    "target": "Returns version of the Cloud SQL Proxy .",
    "arguments": [
      [
        "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n    - _get_credential_parameters ( ) \u00b6\n    - Get the Azure SQL Database proxy client library credential parameters. These parameters can be used to authenticate the Azure SQL Database proxy client library to the proxy server. If credential parameters are not needed by the client library, an empty list is returned.\n\nReturns: A list of strings representing the credential parameters that must be used to authenticate the proxy client library. The strings must match the format of the arguments that are expected by the sqlcmd utility. An empty list is returned if no credential parameters are needed by the client library. Examples ``` >>>  _get_credential_parameters ( ) ['username=exampleuser', 'password=examplepassword', 'encrypt=\\ NO'] ``` ``` >>>  _get_credential_parameters ( ) [] ```\n\n- _get_download_url ( package, version ) \u00b6\n    - Get the download URL of the Azure SQL Database proxy package. Given the name and version of an Azure SQL Database proxy package, return the download URL of the package.\n\nParameters: - package ( ) \u2013 The string name of the Azure SQL Database proxy package.\n      - version ( ) \u2013 The string version of the Azure SQL Database proxy package.\nReturns: The download URL of the Azure SQL Database"
      ]
    ],
    "filtered_resps": [
      "\n    - _get_credential_parameters ( ) \u00b6\n    - Get the Azure SQL Database proxy client library credential parameters. These parameters can be used to authenticate the Azure SQL Database proxy client library to the proxy server. If credential parameters are not needed by the client library, an empty list is returned.\n\nReturns: A list of strings representing the credential parameters that must be used to authenticate the proxy client library. The strings must match the format of the arguments that are expected by the sqlcmd utility. An empty list is returned if no credential parameters are needed by the client library. Examples ``` >>>  _get_credential_parameters ( ) ['username=exampleuser', 'password=examplepassword', 'encrypt=\\ NO'] ``` ``` >>>  _get_credential_parameters ( ) [] ```\n\n- _get_download_url ( package, version ) \u00b6\n    - Get the download URL of the Azure SQL Database proxy package. Given the name and version of an Azure SQL Database proxy package, return the download URL of the package.\n\nParameters: - package ( ) \u2013 The string name of the Azure SQL Database proxy package.\n      - version ( ) \u2013 The string version of the Azure SQL Database proxy package.\nReturns: The download URL of the Azure SQL Database"
    ],
    "smoothed_bleu_4": 0.9819644856533378
  },
  {
    "doc_id": 51,
    "doc": {
      "id": 51,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.create_connection",
      "original_string": "def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()",
      "language": "python",
      "code": "def create_connection(self, session=None):\n        \"\"\"\n        Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        connection = Connection(conn_id=self.db_conn_id)\n        uri = self._generate_connection_uri()\n        self.log.info(\"Creating connection %s\", self.db_conn_id)\n        connection.parse_from_uri(uri)\n        session.add(connection)\n        session.commit()",
      "code_tokens": [
        "def",
        "create_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "connection",
        "=",
        "Connection",
        "(",
        "conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ")",
        "uri",
        "=",
        "self",
        ".",
        "_generate_connection_uri",
        "(",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Creating connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connection",
        ".",
        "parse_from_uri",
        "(",
        "uri",
        ")",
        "session",
        ".",
        "add",
        "(",
        "connection",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Create connection in the Connection table, according to whether it uses\n        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Create",
        "connection",
        "in",
        "the",
        "Connection",
        "table",
        "according",
        "to",
        "whether",
        "it",
        "uses",
        "proxy",
        "TCP",
        "UNIX",
        "sockets",
        "SSL",
        ".",
        "Connection",
        "ID",
        "will",
        "be",
        "randomly",
        "generated",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L895-L908"
    },
    "target": "Create connection in the Connection table according to whether it uses proxy TCP UNIX sockets SSL . Connection ID will be randomly generated .",
    "arguments": [
      [
        "def create_connection ( self , session = None ) : connection = Connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( \"Creating connection %s\" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add ( connection ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return connection ``` The bug is in the line ``` session.add(connection) ``` The bug is that the `connection` object is not being properly added to the session due to a missing commit.\nSolution: To fix the bug, you should add a commit statement after `session.add(connection)` to ensure that the changes are persisted. Here's the corrected code: ```python def create_connection ( self, session = None ) : connection = Connection ( conn_id = self. db_conn_id ) uri = self. _generate_connection_uri ( ) self. log. info ( \"Creating connection %s\", self. db_conn_id ) connection. parse_from_uri ( uri ) session. add ( connection ) session. commit ( ) # Add a commit statement here to fix the bug session. commit ( ) # Remove the duplicate session.commit() return connection ```\n\nHowever, in real-world scenarios, you should avoid duplicate `session.commit()` calls. The corrected code would be:\n```python\ndef create_connection ( self, session = None ) :\nconnection = Connection ( conn_id = self. db_conn_id )\nuri = self. _generate_connection_uri ( )\nself. log. info ( \"Creating connection %s\", self. db_conn_id )\n"
      ]
    ],
    "filtered_resps": [
      " return connection ``` The bug is in the line ``` session.add(connection) ``` The bug is that the `connection` object is not being properly added to the session due to a missing commit.\nSolution: To fix the bug, you should add a commit statement after `session.add(connection)` to ensure that the changes are persisted. Here's the corrected code: ```python def create_connection ( self, session = None ) : connection = Connection ( conn_id = self. db_conn_id ) uri = self. _generate_connection_uri ( ) self. log. info ( \"Creating connection %s\", self. db_conn_id ) connection. parse_from_uri ( uri ) session. add ( connection ) session. commit ( ) # Add a commit statement here to fix the bug session. commit ( ) # Remove the duplicate session.commit() return connection ```\n\nHowever, in real-world scenarios, you should avoid duplicate `session.commit()` calls. The corrected code would be:\n```python\ndef create_connection ( self, session = None ) :\nconnection = Connection ( conn_id = self. db_conn_id )\nuri = self. _generate_connection_uri ( )\nself. log. info ( \"Creating connection %s\", self. db_conn_id )\n"
    ],
    "smoothed_bleu_4": 0.7487452644166613
  },
  {
    "doc_id": 52,
    "doc": {
      "id": 52,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.retrieve_connection",
      "original_string": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None",
      "language": "python",
      "code": "def retrieve_connection(self, session=None):\n        \"\"\"\n        Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Retrieving connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            return connections[0]\n        return None",
      "code_tokens": [
        "def",
        "retrieve_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Retrieving connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connections",
        "=",
        "session",
        ".",
        "query",
        "(",
        "Connection",
        ")",
        ".",
        "filter",
        "(",
        "Connection",
        ".",
        "conn_id",
        "==",
        "self",
        ".",
        "db_conn_id",
        ")",
        "if",
        "connections",
        ".",
        "count",
        "(",
        ")",
        ":",
        "return",
        "connections",
        "[",
        "0",
        "]",
        "return",
        "None"
      ],
      "docstring": "Retrieves the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Retrieves",
        "the",
        "dynamically",
        "created",
        "connection",
        "from",
        "the",
        "Connection",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923"
    },
    "target": "Retrieves the dynamically created connection from the Connection table .",
    "arguments": [
      [
        "def retrieve_connection ( self , session = None ) : self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : return connections [ 0 ] return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` Note: To apply this patch, you would create a new file `__init__.py` with the above code in it, and replace the old `__init__.py` file with the new one.\n  - Add a new `init_db_conn_id` method to the `Connection` class to initialize the database connection id: ``` class   Connection ( ) : def   init_db_conn_id ( self, db_conn_id ) : self. db_conn_id   =   db_conn_id ``` Note: To apply this patch, you would update the `Connection` class with the new `init_db_conn_id` method.\n  - Replace the code in the `get_connection` method with the new code: ``` def   get_connection ( self, session = None ) : self. log. info ( \"Retrieving connection %s\", self. db_conn_id ) connections = session. query ( Connection ). filter ( Connection. conn_id == self. db_conn_id ) if connections. count ( ) : return connections [ 0 ] return None ``` Note: To apply this patch, you would update the `get_connection` method by replacing the old code with the new code.\n\nNote: The above patches assume that the `"
      ]
    ],
    "filtered_resps": [
      " ``` Note: To apply this patch, you would create a new file `__init__.py` with the above code in it, and replace the old `__init__.py` file with the new one.\n  - Add a new `init_db_conn_id` method to the `Connection` class to initialize the database connection id: ``` class   Connection ( ) : def   init_db_conn_id ( self, db_conn_id ) : self. db_conn_id   =   db_conn_id ``` Note: To apply this patch, you would update the `Connection` class with the new `init_db_conn_id` method.\n  - Replace the code in the `get_connection` method with the new code: ``` def   get_connection ( self, session = None ) : self. log. info ( \"Retrieving connection %s\", self. db_conn_id ) connections = session. query ( Connection ). filter ( Connection. conn_id == self. db_conn_id ) if connections. count ( ) : return connections [ 0 ] return None ``` Note: To apply this patch, you would update the `get_connection` method by replacing the old code with the new code.\n\nNote: The above patches assume that the `"
    ],
    "smoothed_bleu_4": 0.5335790659821658
  },
  {
    "doc_id": 53,
    "doc": {
      "id": 53,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.delete_connection",
      "original_string": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")",
      "language": "python",
      "code": "def delete_connection(self, session=None):\n        \"\"\"\n        Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).\n        \"\"\"\n        self.log.info(\"Deleting connection %s\", self.db_conn_id)\n        connections = session.query(Connection).filter(\n            Connection.conn_id == self.db_conn_id)\n        if connections.count():\n            connection = connections[0]\n            session.delete(connection)\n            session.commit()\n        else:\n            self.log.info(\"Connection was already deleted!\")",
      "code_tokens": [
        "def",
        "delete_connection",
        "(",
        "self",
        ",",
        "session",
        "=",
        "None",
        ")",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Deleting connection %s\"",
        ",",
        "self",
        ".",
        "db_conn_id",
        ")",
        "connections",
        "=",
        "session",
        ".",
        "query",
        "(",
        "Connection",
        ")",
        ".",
        "filter",
        "(",
        "Connection",
        ".",
        "conn_id",
        "==",
        "self",
        ".",
        "db_conn_id",
        ")",
        "if",
        "connections",
        ".",
        "count",
        "(",
        ")",
        ":",
        "connection",
        "=",
        "connections",
        "[",
        "0",
        "]",
        "session",
        ".",
        "delete",
        "(",
        "connection",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Connection was already deleted!\"",
        ")"
      ],
      "docstring": "Delete the dynamically created connection from the Connection table.\n\n        :param session: Session of the SQL Alchemy ORM (automatically generated with\n                        decorator).",
      "docstring_tokens": [
        "Delete",
        "the",
        "dynamically",
        "created",
        "connection",
        "from",
        "the",
        "Connection",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L926-L941"
    },
    "target": "Delete the dynamically created connection from the Connection table .",
    "arguments": [
      [
        "def delete_connection ( self , session = None ) : self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( \"Connection was already deleted!\" )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return True\n```\n\n#### Linters Used\n\n* flake8\n\n#### Linters Configuration\n\n* flake8 version 4.0.1\n\n#### Linters Output for the Given Code\n\n```\n1:1:1: WPS101: PEP 8 line too long (85 > 79 characters)\n1:1:1: F841: defined but unused (session, logger)\n\ndef delete_connection ( self, session = None ):\n1:14:1: WPS101: PEP 8 line too long (83 > 79 characters)\n    self.logger.info ( \"Deleting connection %s\", self. db_conn_id )\n1:28:1: E501: line too long (80 > 79 characters)\n1:5:1: W292: a space is not allowed after comment (should be a block comment)\n\n    self.log = logger ( \"sqlalchemy\" )\n1:28:1: E501: line too long (80 > 79 characters)\n1:5:1: W292: a space is not allowed after comment (should be a block comment)\n\n    logger.info ( \"Deleting connection %s\", self. db_conn_id )\n```\n\n"
      ]
    ],
    "filtered_resps": [
      " return True\n```\n\n#### Linters Used\n\n* flake8\n\n#### Linters Configuration\n\n* flake8 version 4.0.1\n\n#### Linters Output for the Given Code\n\n```\n1:1:1: WPS101: PEP 8 line too long (85 > 79 characters)\n1:1:1: F841: defined but unused (session, logger)\n\ndef delete_connection ( self, session = None ):\n1:14:1: WPS101: PEP 8 line too long (83 > 79 characters)\n    self.logger.info ( \"Deleting connection %s\", self. db_conn_id )\n1:28:1: E501: line too long (80 > 79 characters)\n1:5:1: W292: a space is not allowed after comment (should be a block comment)\n\n    self.log = logger ( \"sqlalchemy\" )\n1:28:1: E501: line too long (80 > 79 characters)\n1:5:1: W292: a space is not allowed after comment (should be a block comment)\n\n    logger.info ( \"Deleting connection %s\", self. db_conn_id )\n```\n\n"
    ],
    "smoothed_bleu_4": 0.6329557678691783
  },
  {
    "doc_id": 54,
    "doc": {
      "id": 54,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.get_sqlproxy_runner",
      "original_string": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
      "language": "python",
      "code": "def get_sqlproxy_runner(self):\n        \"\"\"\n        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner\n        \"\"\"\n        if not self.use_proxy:\n            raise AirflowException(\"Proxy runner can only be retrieved in case of use_proxy = True\")\n        return CloudSqlProxyRunner(\n            path_prefix=self.sql_proxy_unique_path,\n            instance_specification=self._get_sqlproxy_instance_specification(),\n            project_id=self.project_id,\n            sql_proxy_version=self.sql_proxy_version,\n            sql_proxy_binary_path=self.sql_proxy_binary_path\n        )",
      "code_tokens": [
        "def",
        "get_sqlproxy_runner",
        "(",
        "self",
        ")",
        ":",
        "if",
        "not",
        "self",
        ".",
        "use_proxy",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"Proxy runner can only be retrieved in case of use_proxy = True\"",
        ")",
        "return",
        "CloudSqlProxyRunner",
        "(",
        "path_prefix",
        "=",
        "self",
        ".",
        "sql_proxy_unique_path",
        ",",
        "instance_specification",
        "=",
        "self",
        ".",
        "_get_sqlproxy_instance_specification",
        "(",
        ")",
        ",",
        "project_id",
        "=",
        "self",
        ".",
        "project_id",
        ",",
        "sql_proxy_version",
        "=",
        "self",
        ".",
        "sql_proxy_version",
        ",",
        "sql_proxy_binary_path",
        "=",
        "self",
        ".",
        "sql_proxy_binary_path",
        ")"
      ],
      "docstring": "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n        lifecycle per task.\n\n        :return: The Cloud SQL Proxy runner.\n        :rtype: CloudSqlProxyRunner",
      "docstring_tokens": [
        "Retrieve",
        "Cloud",
        "SQL",
        "Proxy",
        "runner",
        ".",
        "It",
        "is",
        "used",
        "to",
        "manage",
        "the",
        "proxy",
        "lifecycle",
        "per",
        "task",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L943-L959"
    },
    "target": "Retrieve Cloud SQL Proxy runner . It is used to manage the proxy lifecycle per task .",
    "arguments": [
      [
        "def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( \"Proxy runner can only be retrieved in case of use_proxy = True\" ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_conn ( self, pool ) : if self. _dialect == \"mysql\" : return create_engine ( f \"mysql+pymysql://@ {self. username} : {self. password} @ {self. host} / { self. database}? charset=utf8mb4\" ) elif self. _dialect == \"postgres\" : return create_engine ( f \"postgresql+pymysql://@ {self. username} : {self. password} @ {self. host} / {self. database}? client_encoding=utf8mb4\" ) return None class CloudSqlConfig ( object ) : def __init__ ( self, project_id, instance_class, instance_specification, sql_proxy_binary_path, sql_proxy_version ) : self. project_id = project_id self. instance_class = instance_class self. instance_specification = instance_specification self. sql_proxy_binary_path = sql_proxy_binary_path self. sql_proxy_version = sql_proxy_version # Check if an older version of SQL proxy is being used if sys. platform == \"darwin\" : # OS X if not self. sql_proxy_binary_path or not self. sql_proxy_version : # Use bundled version raise Exception ( \"Please specify correct SQL"
      ]
    ],
    "filtered_resps": [
      " def get_conn ( self, pool ) : if self. _dialect == \"mysql\" : return create_engine ( f \"mysql+pymysql://@ {self. username} : {self. password} @ {self. host} / { self. database}? charset=utf8mb4\" ) elif self. _dialect == \"postgres\" : return create_engine ( f \"postgresql+pymysql://@ {self. username} : {self. password} @ {self. host} / {self. database}? client_encoding=utf8mb4\" ) return None class CloudSqlConfig ( object ) : def __init__ ( self, project_id, instance_class, instance_specification, sql_proxy_binary_path, sql_proxy_version ) : self. project_id = project_id self. instance_class = instance_class self. instance_specification = instance_specification self. sql_proxy_binary_path = sql_proxy_binary_path self. sql_proxy_version = sql_proxy_version # Check if an older version of SQL proxy is being used if sys. platform == \"darwin\" : # OS X if not self. sql_proxy_binary_path or not self. sql_proxy_version : # Use bundled version raise Exception ( \"Please specify correct SQL"
    ],
    "smoothed_bleu_4": 0.6951823960505625
  },
  {
    "doc_id": 55,
    "doc": {
      "id": 55,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.get_database_hook",
      "original_string": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook",
      "language": "python",
      "code": "def get_database_hook(self):\n        \"\"\"\n        Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.\n        \"\"\"\n        if self.database_type == 'postgres':\n            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,\n                                        schema=self.database)\n        else:\n            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,\n                                     schema=self.database)\n        return self.db_hook",
      "code_tokens": [
        "def",
        "get_database_hook",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "database_type",
        "==",
        "'postgres'",
        ":",
        "self",
        ".",
        "db_hook",
        "=",
        "PostgresHook",
        "(",
        "postgres_conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ",",
        "schema",
        "=",
        "self",
        ".",
        "database",
        ")",
        "else",
        ":",
        "self",
        ".",
        "db_hook",
        "=",
        "MySqlHook",
        "(",
        "mysql_conn_id",
        "=",
        "self",
        ".",
        "db_conn_id",
        ",",
        "schema",
        "=",
        "self",
        ".",
        "database",
        ")",
        "return",
        "self",
        ".",
        "db_hook"
      ],
      "docstring": "Retrieve database hook. This is the actual Postgres or MySQL database hook\n        that uses proxy or connects directly to the Google Cloud SQL database.",
      "docstring_tokens": [
        "Retrieve",
        "database",
        "hook",
        ".",
        "This",
        "is",
        "the",
        "actual",
        "Postgres",
        "or",
        "MySQL",
        "database",
        "hook",
        "that",
        "uses",
        "proxy",
        "or",
        "connects",
        "directly",
        "to",
        "the",
        "Google",
        "Cloud",
        "SQL",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L961-L972"
    },
    "target": "Retrieve database hook . This is the actual Postgres or MySQL database hook that uses proxy or connects directly to the Google Cloud SQL database .",
    "arguments": [
      [
        "def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n@staticmethod def build_connection ( db_hook : PostgresHook ): conn = db_hook. get_conn ( ) conn. cursor ( ). execute ( 'SHOW TABLES' ) tables = conn. cursor ( ). fetchall ( ) return conn\n@staticmethod def build_connection ( db_hook : MySqlHook ): conn = db_hook. get_conn ( ) conn = conn. cursor ( ). execute ( 'SHOW TABLES' ) tables = conn. fetchall ( ) return conn\ndef run_sql ( self, sql : str ) -> str : conn = get_database_hook ( self ). build_connection ( ) cursor = conn. cursor ( ) try : # Run query on all tables cursor. execute ( sql )\n```\n\nThis code defines a class `MigrationRunner` with three methods: `get_database_hook`, `build_connection`, and `run_sql`. The `get_database_hook` method returns a database hook object based on the database type defined in the `database` attribute. The `build_connection` method builds a connection to the database and fetches all table names. The `run_sql` method runs an SQL query on all tables in the database using the database hook and returns the result as a string.\n\nThe `build_connection` method"
      ]
    ],
    "filtered_resps": [
      "\n@staticmethod def build_connection ( db_hook : PostgresHook ): conn = db_hook. get_conn ( ) conn. cursor ( ). execute ( 'SHOW TABLES' ) tables = conn. cursor ( ). fetchall ( ) return conn\n@staticmethod def build_connection ( db_hook : MySqlHook ): conn = db_hook. get_conn ( ) conn = conn. cursor ( ). execute ( 'SHOW TABLES' ) tables = conn. fetchall ( ) return conn\ndef run_sql ( self, sql : str ) -> str : conn = get_database_hook ( self ). build_connection ( ) cursor = conn. cursor ( ) try : # Run query on all tables cursor. execute ( sql )\n```\n\nThis code defines a class `MigrationRunner` with three methods: `get_database_hook`, `build_connection`, and `run_sql`. The `get_database_hook` method returns a database hook object based on the database type defined in the `database` attribute. The `build_connection` method builds a connection to the database and fetches all table names. The `run_sql` method runs an SQL query on all tables in the database using the database hook and returns the result as a string.\n\nThe `build_connection` method"
    ],
    "smoothed_bleu_4": 1.033675355256627
  },
  {
    "doc_id": 56,
    "doc": {
      "id": 56,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.cleanup_database_hook",
      "original_string": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)",
      "language": "python",
      "code": "def cleanup_database_hook(self):\n        \"\"\"\n        Clean up database hook after it was used.\n        \"\"\"\n        if self.database_type == 'postgres':\n            if hasattr(self.db_hook,\n                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:\n                for output in self.db_hook.conn.notices:\n                    self.log.info(output)",
      "code_tokens": [
        "def",
        "cleanup_database_hook",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "database_type",
        "==",
        "'postgres'",
        ":",
        "if",
        "hasattr",
        "(",
        "self",
        ".",
        "db_hook",
        ",",
        "'conn'",
        ")",
        "and",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        "and",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        ".",
        "notices",
        ":",
        "for",
        "output",
        "in",
        "self",
        ".",
        "db_hook",
        ".",
        "conn",
        ".",
        "notices",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "output",
        ")"
      ],
      "docstring": "Clean up database hook after it was used.",
      "docstring_tokens": [
        "Clean",
        "up",
        "database",
        "hook",
        "after",
        "it",
        "was",
        "used",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L974-L982"
    },
    "target": "Clean up database hook after it was used .",
    "arguments": [
      [
        "def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. db_hook. conn. reset ( )\n```\n\n### Changed: Updated the database clean-up script to check for the existence of a connection and its associated object, `conn`, when running as a post-process hook. This check prevents the database clean-up process from crashing when running in a mode that doesn\u2019t create the `conn` object.\n\n## Step 3: Ensure Proper Database Connection Handling\n\nIn addition to the code changes, ensure that you properly handle database connections in your application. This includes closing the connections when not in use to prevent resource leakage and potential security vulnerabilities.\n\nExample of proper connection handling using a context manager:\n```python\nwith self. db_hook. conn. connection () as conn :\ntry :\n# perform database operations\nexcept DatabaseError as e :\n# handle database errors\nfinally :\nconn. close ()\n```\n\n## Step 4: Verify and Test Database Hooks\n\nOnce you've modified the database hooks, verify that the changes work as expected by running unit tests and integration tests that simulate various scenarios, including database clean-up and data migration.\n\nExample Test Code:\n```python\ndef test_database_hooks ( self ) :\n# Create a temporary database connection\nwith db. conn :\n# Create test data\ndata1 ="
      ]
    ],
    "filtered_resps": [
      " self. db_hook. conn. reset ( )\n```\n\n### Changed: Updated the database clean-up script to check for the existence of a connection and its associated object, `conn`, when running as a post-process hook. This check prevents the database clean-up process from crashing when running in a mode that doesn\u2019t create the `conn` object.\n\n## Step 3: Ensure Proper Database Connection Handling\n\nIn addition to the code changes, ensure that you properly handle database connections in your application. This includes closing the connections when not in use to prevent resource leakage and potential security vulnerabilities.\n\nExample of proper connection handling using a context manager:\n```python\nwith self. db_hook. conn. connection () as conn :\ntry :\n# perform database operations\nexcept DatabaseError as e :\n# handle database errors\nfinally :\nconn. close ()\n```\n\n## Step 4: Verify and Test Database Hooks\n\nOnce you've modified the database hooks, verify that the changes work as expected by running unit tests and integration tests that simulate various scenarios, including database clean-up and data migration.\n\nExample Test Code:\n```python\ndef test_database_hooks ( self ) :\n# Create a temporary database connection\nwith db. conn :\n# Create test data\ndata1 ="
    ],
    "smoothed_bleu_4": 0.5745845835126728
  },
  {
    "doc_id": 57,
    "doc": {
      "id": 57,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_sql_hook.py",
      "func_name": "CloudSqlDatabaseHook.reserve_free_tcp_port",
      "original_string": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",
      "language": "python",
      "code": "def reserve_free_tcp_port(self):\n        \"\"\"\n        Reserve free TCP port to be used by Cloud SQL Proxy\n        \"\"\"\n        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.reserved_tcp_socket.bind(('127.0.0.1', 0))\n        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]",
      "code_tokens": [
        "def",
        "reserve_free_tcp_port",
        "(",
        "self",
        ")",
        ":",
        "self",
        ".",
        "reserved_tcp_socket",
        "=",
        "socket",
        ".",
        "socket",
        "(",
        "socket",
        ".",
        "AF_INET",
        ",",
        "socket",
        ".",
        "SOCK_STREAM",
        ")",
        "self",
        ".",
        "reserved_tcp_socket",
        ".",
        "bind",
        "(",
        "(",
        "'127.0.0.1'",
        ",",
        "0",
        ")",
        ")",
        "self",
        ".",
        "sql_proxy_tcp_port",
        "=",
        "self",
        ".",
        "reserved_tcp_socket",
        ".",
        "getsockname",
        "(",
        ")",
        "[",
        "1",
        "]"
      ],
      "docstring": "Reserve free TCP port to be used by Cloud SQL Proxy",
      "docstring_tokens": [
        "Reserve",
        "free",
        "TCP",
        "port",
        "to",
        "be",
        "used",
        "by",
        "Cloud",
        "SQL",
        "Proxy"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990"
    },
    "target": "Reserve free TCP port to be used by Cloud SQL Proxy",
    "arguments": [
      [
        "def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SO_REUSEADDR, 1 ) self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SOCKETOPT, socket. SO_REUSEADDER, 1 ) self. reserved_tcp_socket. listen ( 1 ) \u00a0\n```\nThis code defines an object with a TCP socket bound to an available ephemeral port (one that isn't currently being used by another process), and makes that socket listen for incoming connections on that port. That ephemeral port is stored and used later by the database connection object.\n\nThe `connect` method connects to that socket on the same host as the original connection (localhost).\n```\ndef   connect ( self ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0self. socket. connect ( self. reserved_tcp_socket. getsockname ( ) ) \u00a0\n```\nAnd the `get_proxy_tcp_port` method returns the port assigned to the ephemeral socket. You can use these methods in your code to create a connection object, reserve a port, connect, and get the proxy TCP port:\n```\nimport   socket\nfrom   your_module   import   DBProxyConnection\n\nconn   =   DBProxyConnection ( )\nreserved_tcp_port"
      ]
    ],
    "filtered_resps": [
      " self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SO_REUSEADDR, 1 ) self. reserved_tcp_socket. setsockopt ( socket. SOL_SOCKET, socket. SOCKETOPT, socket. SO_REUSEADDER, 1 ) self. reserved_tcp_socket. listen ( 1 ) \u00a0\n```\nThis code defines an object with a TCP socket bound to an available ephemeral port (one that isn't currently being used by another process), and makes that socket listen for incoming connections on that port. That ephemeral port is stored and used later by the database connection object.\n\nThe `connect` method connects to that socket on the same host as the original connection (localhost).\n```\ndef   connect ( self ) : \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0self. socket. connect ( self. reserved_tcp_socket. getsockname ( ) ) \u00a0\n```\nAnd the `get_proxy_tcp_port` method returns the port assigned to the ephemeral socket. You can use these methods in your code to create a connection object, reserve a port, connect, and get the proxy TCP port:\n```\nimport   socket\nfrom   your_module   import   DBProxyConnection\n\nconn   =   DBProxyConnection ( )\nreserved_tcp_port"
    ],
    "smoothed_bleu_4": 0.887340021036921
  },
  {
    "doc_id": 58,
    "doc": {
      "id": 58,
      "repo": "apache/airflow",
      "path": "airflow/contrib/operators/mlengine_operator.py",
      "func_name": "_normalize_mlengine_job_id",
      "original_string": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id",
      "language": "python",
      "code": "def _normalize_mlengine_job_id(job_id):\n    \"\"\"\n    Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.\n    \"\"\"\n\n    # Add a prefix when a job_id starts with a digit or a template\n    match = re.search(r'\\d|\\{{2}', job_id)\n    if match and match.start() == 0:\n        job = 'z_{}'.format(job_id)\n    else:\n        job = job_id\n\n    # Clean up 'bad' characters except templates\n    tracker = 0\n    cleansed_job_id = ''\n    for m in re.finditer(r'\\{{2}.+?\\}{2}', job):\n        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',\n                                  job[tracker:m.start()])\n        cleansed_job_id += job[m.start():m.end()]\n        tracker = m.end()\n\n    # Clean up last substring or the full string if no templates\n    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])\n\n    return cleansed_job_id",
      "code_tokens": [
        "def",
        "_normalize_mlengine_job_id",
        "(",
        "job_id",
        ")",
        ":",
        "# Add a prefix when a job_id starts with a digit or a template",
        "match",
        "=",
        "re",
        ".",
        "search",
        "(",
        "r'\\d|\\{{2}'",
        ",",
        "job_id",
        ")",
        "if",
        "match",
        "and",
        "match",
        ".",
        "start",
        "(",
        ")",
        "==",
        "0",
        ":",
        "job",
        "=",
        "'z_{}'",
        ".",
        "format",
        "(",
        "job_id",
        ")",
        "else",
        ":",
        "job",
        "=",
        "job_id",
        "# Clean up 'bad' characters except templates",
        "tracker",
        "=",
        "0",
        "cleansed_job_id",
        "=",
        "''",
        "for",
        "m",
        "in",
        "re",
        ".",
        "finditer",
        "(",
        "r'\\{{2}.+?\\}{2}'",
        ",",
        "job",
        ")",
        ":",
        "cleansed_job_id",
        "+=",
        "re",
        ".",
        "sub",
        "(",
        "r'[^0-9a-zA-Z]+'",
        ",",
        "'_'",
        ",",
        "job",
        "[",
        "tracker",
        ":",
        "m",
        ".",
        "start",
        "(",
        ")",
        "]",
        ")",
        "cleansed_job_id",
        "+=",
        "job",
        "[",
        "m",
        ".",
        "start",
        "(",
        ")",
        ":",
        "m",
        ".",
        "end",
        "(",
        ")",
        "]",
        "tracker",
        "=",
        "m",
        ".",
        "end",
        "(",
        ")",
        "# Clean up last substring or the full string if no templates",
        "cleansed_job_id",
        "+=",
        "re",
        ".",
        "sub",
        "(",
        "r'[^0-9a-zA-Z]+'",
        ",",
        "'_'",
        ",",
        "job",
        "[",
        "tracker",
        ":",
        "]",
        ")",
        "return",
        "cleansed_job_id"
      ],
      "docstring": "Replaces invalid MLEngine job_id characters with '_'.\n\n    This also adds a leading 'z' in case job_id starts with an invalid\n    character.\n\n    Args:\n        job_id: A job_id str that may have invalid characters.\n\n    Returns:\n        A valid job_id representation.",
      "docstring_tokens": [
        "Replaces",
        "invalid",
        "MLEngine",
        "job_id",
        "characters",
        "with",
        "_",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mlengine_operator.py#L29-L62"
    },
    "target": "Replaces invalid MLEngine job_id characters with _ .",
    "arguments": [
      [
        "def _normalize_mlengine_job_id ( job_id ) : # Add a prefix when a job_id starts with a digit or a template match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id # Clean up 'bad' characters except templates tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) # Clean up last substring or the full string if no templates cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n### Step 5: Clean up the cleansed job_id to make it fit 63 characters.\n```_clean_up_cleansed_job_id(job_id): #Cleanup last character if job_id exceeds 63 chars cleansed_id = job_id[:-1] if len(job_id) > 63 else job_id #Remove leading underscores except for the first one cleansed_id = re.sub('_+', '_', cleansed_id.lstrip('_')) return cleansed_id ```\n### Step 6: Check if the cleaned job_id starts with a digit and add a prefix if so.\n\n```_prefix_for_cleansed_job_id(job_id): if job_id and not job_id.lstrip('-_').isdigit(): return job_id #Add prefix if it starts with a digit prefix = 'z_' if job_id.startswith('-'): prefix = '-' + prefix job_id = prefix +job_id[:-1] if len(job_id) > 63 else job_id return job_id ```\n### Step 7: Combine the above five steps and check if the given job_id starts with one of the following values\n```_check_job_id(job_id): job_id = _prefix_for_cleansed_job_id(job_id) if any(job_id.startswith(starting"
      ]
    ],
    "filtered_resps": [
      " ```\n### Step 5: Clean up the cleansed job_id to make it fit 63 characters.\n```_clean_up_cleansed_job_id(job_id): #Cleanup last character if job_id exceeds 63 chars cleansed_id = job_id[:-1] if len(job_id) > 63 else job_id #Remove leading underscores except for the first one cleansed_id = re.sub('_+', '_', cleansed_id.lstrip('_')) return cleansed_id ```\n### Step 6: Check if the cleaned job_id starts with a digit and add a prefix if so.\n\n```_prefix_for_cleansed_job_id(job_id): if job_id and not job_id.lstrip('-_').isdigit(): return job_id #Add prefix if it starts with a digit prefix = 'z_' if job_id.startswith('-'): prefix = '-' + prefix job_id = prefix +job_id[:-1] if len(job_id) > 63 else job_id return job_id ```\n### Step 7: Combine the above five steps and check if the given job_id starts with one of the following values\n```_check_job_id(job_id): job_id = _prefix_for_cleansed_job_id(job_id) if any(job_id.startswith(starting"
    ],
    "smoothed_bleu_4": 0.7755485220103939
  },
  {
    "doc_id": 59,
    "doc": {
      "id": 59,
      "repo": "apache/airflow",
      "path": "airflow/contrib/sensors/ftp_sensor.py",
      "func_name": "FTPSensor._get_error_code",
      "original_string": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e",
      "language": "python",
      "code": "def _get_error_code(self, e):\n        \"\"\"Extract error code from ftp exception\"\"\"\n        try:\n            matches = self.error_code_pattern.match(str(e))\n            code = int(matches.group(0))\n            return code\n        except ValueError:\n            return e",
      "code_tokens": [
        "def",
        "_get_error_code",
        "(",
        "self",
        ",",
        "e",
        ")",
        ":",
        "try",
        ":",
        "matches",
        "=",
        "self",
        ".",
        "error_code_pattern",
        ".",
        "match",
        "(",
        "str",
        "(",
        "e",
        ")",
        ")",
        "code",
        "=",
        "int",
        "(",
        "matches",
        ".",
        "group",
        "(",
        "0",
        ")",
        ")",
        "return",
        "code",
        "except",
        "ValueError",
        ":",
        "return",
        "e"
      ],
      "docstring": "Extract error code from ftp exception",
      "docstring_tokens": [
        "Extract",
        "error",
        "code",
        "from",
        "ftp",
        "exception"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/ftp_sensor.py#L69-L76"
    },
    "target": "Extract error code from ftp exception",
    "arguments": [
      [
        "def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except ValueError : return e",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". errno\ndef get_error_code ( self ) : \"\"\" Returns the error code ( int ) for the last error message. \"\"\" return _get_error_code ( self, self. error )\n#.........\n\n## Step 5: Replace the _get_error_code function with the new implementation\n\nReplace the _get_error_code function with the new implementation:\n```\ndef _get_error_code ( self, e ) :\ntry :\nmatches = re. search ( r 'error code = ( \\d+) ', str ( e ) )\nif matches :\ncode = int ( matches. group ( 1 ) )\nreturn code\nexcept ValueError :\nreturn e. errno\n```\nThis function will use regular expression to search for the error code in the string representation of the exception.\n\n## Step 6: Add the error pattern search in try-except block\n\nAdd the try-except block:\n```\ntry :\n#... original code...\nexcept Exception as e :\nif not isinstance ( e, str ) :\ntry :\nerror_code = _get_error_code ( self, e )\nif error_code :\nreturn error_code\nexcept ( ValueError, AttributeError ) :\npass\nerror = \" {0}: {1} on line {2} \". format"
      ]
    ],
    "filtered_resps": [
      ". errno\ndef get_error_code ( self ) : \"\"\" Returns the error code ( int ) for the last error message. \"\"\" return _get_error_code ( self, self. error )\n#.........\n\n## Step 5: Replace the _get_error_code function with the new implementation\n\nReplace the _get_error_code function with the new implementation:\n```\ndef _get_error_code ( self, e ) :\ntry :\nmatches = re. search ( r 'error code = ( \\d+) ', str ( e ) )\nif matches :\ncode = int ( matches. group ( 1 ) )\nreturn code\nexcept ValueError :\nreturn e. errno\n```\nThis function will use regular expression to search for the error code in the string representation of the exception.\n\n## Step 6: Add the error pattern search in try-except block\n\nAdd the try-except block:\n```\ntry :\n#... original code...\nexcept Exception as e :\nif not isinstance ( e, str ) :\ntry :\nerror_code = _get_error_code ( self, e )\nif error_code :\nreturn error_code\nexcept ( ValueError, AttributeError ) :\npass\nerror = \" {0}: {1} on line {2} \". format"
    ],
    "smoothed_bleu_4": 0.5686073345256623
  },
  {
    "doc_id": 60,
    "doc": {
      "id": 60,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "clear_dag_runs",
      "original_string": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)",
      "language": "python",
      "code": "def clear_dag_runs():\n    \"\"\"\n    Remove any existing DAG runs for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    drs = session.query(DagRun).filter(\n        DagRun.dag_id.in_(DAG_IDS),\n    ).all()\n    for dr in drs:\n        logging.info('Deleting DagRun :: {}'.format(dr))\n        session.delete(dr)",
      "code_tokens": [
        "def",
        "clear_dag_runs",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "drs",
        "=",
        "session",
        ".",
        "query",
        "(",
        "DagRun",
        ")",
        ".",
        "filter",
        "(",
        "DagRun",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ",",
        ")",
        ".",
        "all",
        "(",
        ")",
        "for",
        "dr",
        "in",
        "drs",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Deleting DagRun :: {}'",
        ".",
        "format",
        "(",
        "dr",
        ")",
        ")",
        "session",
        ".",
        "delete",
        "(",
        "dr",
        ")"
      ],
      "docstring": "Remove any existing DAG runs for the perf test DAGs.",
      "docstring_tokens": [
        "Remove",
        "any",
        "existing",
        "DAG",
        "runs",
        "for",
        "the",
        "perf",
        "test",
        "DAGs",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L138-L148"
    },
    "target": "Remove any existing DAG runs for the perf test DAGs .",
    "arguments": [
      [
        "def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session . delete ( dr )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " session. commit ( ) return ```\n- delete_dag ( dag_id ) \u00b6\n``` def delete_dag ( self,   dag_id ) : self. dag_handler. delete_dag ( dag_id ) ```\n- delete_all_dags ( ) \u00b6\n``` def delete_all_dags ( self ) : self. dag_handler. delete_all_dags ( ) ```\n- update_dag ( dag_id, version ) \u00b6\n``` def update_dag ( self,   dag_id,   version ) : self. dag_handler. save_dag ( dag_id,   version )   # Save a Dag to the database self. dag_handler. save_dag ( dag_id,   version,   replace = True )   # Replace a dag in the database ```\n- run ( date = 'tomorrow', dag_ids=None, raise_on_failure=True, poke_interval=None ) \u00b6\n``` dag_ids   =   ['my_dag_1' ] date   =   datetime. datetime. today ( ). tomorrow ( ). isoformat () result   =   self. scheduler. run ( date,   dag_ids ) ```\n- start ( start_date=None, end_date=None, dag_ids=None ) \u00b6\n``` self."
      ]
    ],
    "filtered_resps": [
      " session. commit ( ) return ```\n- delete_dag ( dag_id ) \u00b6\n``` def delete_dag ( self,   dag_id ) : self. dag_handler. delete_dag ( dag_id ) ```\n- delete_all_dags ( ) \u00b6\n``` def delete_all_dags ( self ) : self. dag_handler. delete_all_dags ( ) ```\n- update_dag ( dag_id, version ) \u00b6\n``` def update_dag ( self,   dag_id,   version ) : self. dag_handler. save_dag ( dag_id,   version )   # Save a Dag to the database self. dag_handler. save_dag ( dag_id,   version,   replace = True )   # Replace a dag in the database ```\n- run ( date = 'tomorrow', dag_ids=None, raise_on_failure=True, poke_interval=None ) \u00b6\n``` dag_ids   =   ['my_dag_1' ] date   =   datetime. datetime. today ( ). tomorrow ( ). isoformat () result   =   self. scheduler. run ( date,   dag_ids ) ```\n- start ( start_date=None, end_date=None, dag_ids=None ) \u00b6\n``` self."
    ],
    "smoothed_bleu_4": 0.5100883293004987
  },
  {
    "doc_id": 61,
    "doc": {
      "id": 61,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "clear_dag_task_instances",
      "original_string": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()",
      "language": "python",
      "code": "def clear_dag_task_instances():\n    \"\"\"\n    Remove any existing task instances for the perf test DAGs.\n    \"\"\"\n    session = settings.Session()\n    TI = TaskInstance\n    tis = (\n        session\n        .query(TI)\n        .filter(TI.dag_id.in_(DAG_IDS))\n        .all()\n    )\n    for ti in tis:\n        logging.info('Deleting TaskInstance :: {}'.format(ti))\n        session.delete(ti)\n    session.commit()",
      "code_tokens": [
        "def",
        "clear_dag_task_instances",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "TI",
        "=",
        "TaskInstance",
        "tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "for",
        "ti",
        "in",
        "tis",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Deleting TaskInstance :: {}'",
        ".",
        "format",
        "(",
        "ti",
        ")",
        ")",
        "session",
        ".",
        "delete",
        "(",
        "ti",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Remove any existing task instances for the perf test DAGs.",
      "docstring_tokens": [
        "Remove",
        "any",
        "existing",
        "task",
        "instances",
        "for",
        "the",
        "perf",
        "test",
        "DAGs",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L151-L166"
    },
    "target": "Remove any existing task instances for the perf test DAGs .",
    "arguments": [
      [
        "def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " logging. info ( 'Deleted all TaskInstances from DAGs :: {}'. format ( DAG_IDS ) ) end ```\n  - 5. The next step is to clear the dag_runs table of the Airflow database by using a query. ``` # Clear dag_runs table clear_dag_runs ( ) def clear_dag_runs ( ) :   # Assuming using SQLAlchemy SQL = 'TRUNCATE dag_runs' engine = settings. Engine ( ) with engine. connection. begin ( ) :   engine. connection. execute ( SQL ) end ```\n  - 6. Finally, you can execute the previously defined functions in sequence using the threading package to clear the dag_run table, delete all task_instances, and clear the scheduler. Here\u2019s how you can do it. ``` # Clear scheduler, task_instances and dag_runs using threads def clear_airflow ( ) :   # Import logging logger = logging. getLogger ( __name__ ) logger. info ( 'Started clearing Airflow DB' ) thread_1   =   threading. Thread ( target = clear_scheduler ) thread_2   =   threading. Thread ( target = clear_task_instances ) thread_3   =   threading. Thread ( target = clear_dag_runs ) # Start"
      ]
    ],
    "filtered_resps": [
      " logging. info ( 'Deleted all TaskInstances from DAGs :: {}'. format ( DAG_IDS ) ) end ```\n  - 5. The next step is to clear the dag_runs table of the Airflow database by using a query. ``` # Clear dag_runs table clear_dag_runs ( ) def clear_dag_runs ( ) :   # Assuming using SQLAlchemy SQL = 'TRUNCATE dag_runs' engine = settings. Engine ( ) with engine. connection. begin ( ) :   engine. connection. execute ( SQL ) end ```\n  - 6. Finally, you can execute the previously defined functions in sequence using the threading package to clear the dag_run table, delete all task_instances, and clear the scheduler. Here\u2019s how you can do it. ``` # Clear scheduler, task_instances and dag_runs using threads def clear_airflow ( ) :   # Import logging logger = logging. getLogger ( __name__ ) logger. info ( 'Started clearing Airflow DB' ) thread_1   =   threading. Thread ( target = clear_scheduler ) thread_2   =   threading. Thread ( target = clear_task_instances ) thread_3   =   threading. Thread ( target = clear_dag_runs ) # Start"
    ],
    "smoothed_bleu_4": 0.581285347685132
  },
  {
    "doc_id": 62,
    "doc": {
      "id": 62,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "set_dags_paused_state",
      "original_string": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
      "language": "python",
      "code": "def set_dags_paused_state(is_paused):\n    \"\"\"\n    Toggle the pause state of the DAGs in the test.\n    \"\"\"\n    session = settings.Session()\n    dms = session.query(DagModel).filter(\n        DagModel.dag_id.in_(DAG_IDS))\n    for dm in dms:\n        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))\n        dm.is_paused = is_paused\n    session.commit()",
      "code_tokens": [
        "def",
        "set_dags_paused_state",
        "(",
        "is_paused",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "dms",
        "=",
        "session",
        ".",
        "query",
        "(",
        "DagModel",
        ")",
        ".",
        "filter",
        "(",
        "DagModel",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        "for",
        "dm",
        "in",
        "dms",
        ":",
        "logging",
        ".",
        "info",
        "(",
        "'Setting DAG :: {} is_paused={}'",
        ".",
        "format",
        "(",
        "dm",
        ",",
        "is_paused",
        ")",
        ")",
        "dm",
        ".",
        "is_paused",
        "=",
        "is_paused",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Toggle the pause state of the DAGs in the test.",
      "docstring_tokens": [
        "Toggle",
        "the",
        "pause",
        "state",
        "of",
        "the",
        "DAGs",
        "in",
        "the",
        "test",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L169-L179"
    },
    "target": "Toggle the pause state of the DAGs in the test .",
    "arguments": [
      [
        "def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except SQLAlchemyError as e : logging. error ( e ), ```\n\nThis code snippet assumes you already have a connection to your Celery broker, and you are running it within the context of a task. The script iterates over all DAG instances and checks if the DAG is currently running. If the DAG is not running, it pauses the DAG; otherwise, it resumes it. The DAG's current state can be retrieved using the `is_paused` attribute.\n\nHere is an example of how you can call this function:\n\n```python\nfrom myapp.tasks import set_dags_paused_state\n\n@app.task\ndef set_dags_paused():\n    set_dags_paused_state(paused=False)\n```\n\nPlease replace `'myapp.tasks'` and `'app'` with the actual module name of your Celery task and the actual app instance in your Celery configuration, respectively.\n\nMake sure to replace `'DAG_IDS'` with the list of DAG IDs you are trying to manage. Also, ensure that you are properly handling potential SQLAlchemy errors to ensure your task does not crash when an error occurs during the database transaction.\n\nPlease note that using `set_dags_paused_state` function will only pause or resume your DAG,"
      ]
    ],
    "filtered_resps": [
      " except SQLAlchemyError as e : logging. error ( e ), ```\n\nThis code snippet assumes you already have a connection to your Celery broker, and you are running it within the context of a task. The script iterates over all DAG instances and checks if the DAG is currently running. If the DAG is not running, it pauses the DAG; otherwise, it resumes it. The DAG's current state can be retrieved using the `is_paused` attribute.\n\nHere is an example of how you can call this function:\n\n```python\nfrom myapp.tasks import set_dags_paused_state\n\n@app.task\ndef set_dags_paused():\n    set_dags_paused_state(paused=False)\n```\n\nPlease replace `'myapp.tasks'` and `'app'` with the actual module name of your Celery task and the actual app instance in your Celery configuration, respectively.\n\nMake sure to replace `'DAG_IDS'` with the list of DAG IDs you are trying to manage. Also, ensure that you are properly handling potential SQLAlchemy errors to ensure your task does not crash when an error occurs during the database transaction.\n\nPlease note that using `set_dags_paused_state` function will only pause or resume your DAG,"
    ],
    "smoothed_bleu_4": 0.6759258086128188
  },
  {
    "doc_id": 63,
    "doc": {
      "id": 63,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "SchedulerMetricsJob.print_stats",
      "original_string": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()",
      "language": "python",
      "code": "def print_stats(self):\n        \"\"\"\n        Print operational metrics for the scheduler test.\n        \"\"\"\n        session = settings.Session()\n        TI = TaskInstance\n        tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .all()\n        )\n        successful_tis = [x for x in tis if x.state == State.SUCCESS]\n        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,\n                    (ti.queued_dttm - self.start_date).total_seconds(),\n                    (ti.start_date - self.start_date).total_seconds(),\n                    (ti.end_date - self.start_date).total_seconds(),\n                    ti.duration) for ti in successful_tis]\n        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',\n                                                    'execution_date',\n                                                    'queue_delay',\n                                                    'start_delay', 'land_time',\n                                                    'duration'])\n\n        print('Performance Results')\n        print('###################')\n        for dag_id in DAG_IDS:\n            print('DAG {}'.format(dag_id))\n            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])\n        print('###################')\n        if len(tis) > len(successful_tis):\n            print(\"WARNING!! The following task instances haven't completed\")\n            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)\n                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],\n                  columns=['dag_id', 'task_id', 'execution_date', 'state']))\n\n        session.commit()",
      "code_tokens": [
        "def",
        "print_stats",
        "(",
        "self",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "TI",
        "=",
        "TaskInstance",
        "tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "successful_tis",
        "=",
        "[",
        "x",
        "for",
        "x",
        "in",
        "tis",
        "if",
        "x",
        ".",
        "state",
        "==",
        "State",
        ".",
        "SUCCESS",
        "]",
        "ti_perf",
        "=",
        "[",
        "(",
        "ti",
        ".",
        "dag_id",
        ",",
        "ti",
        ".",
        "task_id",
        ",",
        "ti",
        ".",
        "execution_date",
        ",",
        "(",
        "ti",
        ".",
        "queued_dttm",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "(",
        "ti",
        ".",
        "start_date",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "(",
        "ti",
        ".",
        "end_date",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ",",
        "ti",
        ".",
        "duration",
        ")",
        "for",
        "ti",
        "in",
        "successful_tis",
        "]",
        "ti_perf_df",
        "=",
        "pd",
        ".",
        "DataFrame",
        "(",
        "ti_perf",
        ",",
        "columns",
        "=",
        "[",
        "'dag_id'",
        ",",
        "'task_id'",
        ",",
        "'execution_date'",
        ",",
        "'queue_delay'",
        ",",
        "'start_delay'",
        ",",
        "'land_time'",
        ",",
        "'duration'",
        "]",
        ")",
        "print",
        "(",
        "'Performance Results'",
        ")",
        "print",
        "(",
        "'###################'",
        ")",
        "for",
        "dag_id",
        "in",
        "DAG_IDS",
        ":",
        "print",
        "(",
        "'DAG {}'",
        ".",
        "format",
        "(",
        "dag_id",
        ")",
        ")",
        "print",
        "(",
        "ti_perf_df",
        "[",
        "ti_perf_df",
        "[",
        "'dag_id'",
        "]",
        "==",
        "dag_id",
        "]",
        ")",
        "print",
        "(",
        "'###################'",
        ")",
        "if",
        "len",
        "(",
        "tis",
        ")",
        ">",
        "len",
        "(",
        "successful_tis",
        ")",
        ":",
        "print",
        "(",
        "\"WARNING!! The following task instances haven't completed\"",
        ")",
        "print",
        "(",
        "pd",
        ".",
        "DataFrame",
        "(",
        "[",
        "(",
        "ti",
        ".",
        "dag_id",
        ",",
        "ti",
        ".",
        "task_id",
        ",",
        "ti",
        ".",
        "execution_date",
        ",",
        "ti",
        ".",
        "state",
        ")",
        "for",
        "ti",
        "in",
        "filter",
        "(",
        "lambda",
        "x",
        ":",
        "x",
        ".",
        "state",
        "!=",
        "State",
        ".",
        "SUCCESS",
        ",",
        "tis",
        ")",
        "]",
        ",",
        "columns",
        "=",
        "[",
        "'dag_id'",
        ",",
        "'task_id'",
        ",",
        "'execution_date'",
        ",",
        "'state'",
        "]",
        ")",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")"
      ],
      "docstring": "Print operational metrics for the scheduler test.",
      "docstring_tokens": [
        "Print",
        "operational",
        "metrics",
        "for",
        "the",
        "scheduler",
        "test",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L65-L101"
    },
    "target": "Print operational metrics for the scheduler test .",
    "arguments": [
      [
        "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nThis code creates a session with Airflow, retrieves TaskInstance objects matching certain condition and prints statistics on these tasks. It also commits the session at the end.\n\nTo execute this code successfully, you will need the following packages installed: pandas and sqlalchemy.\n\n### Example Usage:\n\nWhen you run the above script, it will print performance statistics on `dag_1` and `dag_2`, including the execution time, queue delay, start delay of each task.\n\n### Note:\n\nThe actual task instance data may change depending on the DAGs and tasks being executed. You may need to inspect the data to determine the cause of any performance issues. Also, the DAGs being executed should include task instances that have both'start_date' and 'end_date' set. Otherwise, you will get error messages. This is because the code is trying to calculate durations based on the execution dates of the task instances. If any task instance is missing execution dates, it can not be used for the performance analysis. The above code also prints a warning if any task instance is not complete for the specified DAGs. \n\nPlease run the code from an environment where Airflow metadata can be accessed with sqlalchemy. Airflow needs to be running for the code to"
      ]
    ],
    "filtered_resps": [
      " ```\n\nThis code creates a session with Airflow, retrieves TaskInstance objects matching certain condition and prints statistics on these tasks. It also commits the session at the end.\n\nTo execute this code successfully, you will need the following packages installed: pandas and sqlalchemy.\n\n### Example Usage:\n\nWhen you run the above script, it will print performance statistics on `dag_1` and `dag_2`, including the execution time, queue delay, start delay of each task.\n\n### Note:\n\nThe actual task instance data may change depending on the DAGs and tasks being executed. You may need to inspect the data to determine the cause of any performance issues. Also, the DAGs being executed should include task instances that have both'start_date' and 'end_date' set. Otherwise, you will get error messages. This is because the code is trying to calculate durations based on the execution dates of the task instances. If any task instance is missing execution dates, it can not be used for the performance analysis. The above code also prints a warning if any task instance is not complete for the specified DAGs. \n\nPlease run the code from an environment where Airflow metadata can be accessed with sqlalchemy. Airflow needs to be running for the code to"
    ],
    "smoothed_bleu_4": 0.6747450938454923
  },
  {
    "doc_id": 64,
    "doc": {
      "id": 64,
      "repo": "apache/airflow",
      "path": "scripts/perf/scheduler_ops_metrics.py",
      "func_name": "SchedulerMetricsJob.heartbeat",
      "original_string": "def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()",
      "language": "python",
      "code": "def heartbeat(self):\n        \"\"\"\n        Override the scheduler heartbeat to determine when the test is complete\n        \"\"\"\n        super(SchedulerMetricsJob, self).heartbeat()\n        session = settings.Session()\n        # Get all the relevant task instances\n        TI = TaskInstance\n        successful_tis = (\n            session\n            .query(TI)\n            .filter(TI.dag_id.in_(DAG_IDS))\n            .filter(TI.state.in_([State.SUCCESS]))\n            .all()\n        )\n        session.commit()\n\n        dagbag = DagBag(SUBDIR)\n        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]\n        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.\n        num_task_instances = sum([(timezone.utcnow() - task.start_date).days\n                                 for dag in dags for task in dag.tasks])\n\n        if (len(successful_tis) == num_task_instances or\n                (timezone.utcnow() - self.start_date).total_seconds() >\n                MAX_RUNTIME_SECS):\n            if len(successful_tis) == num_task_instances:\n                self.log.info(\"All tasks processed! Printing stats.\")\n            else:\n                self.log.info(\"Test timeout reached. Printing available stats.\")\n            self.print_stats()\n            set_dags_paused_state(True)\n            sys.exit()",
      "code_tokens": [
        "def",
        "heartbeat",
        "(",
        "self",
        ")",
        ":",
        "super",
        "(",
        "SchedulerMetricsJob",
        ",",
        "self",
        ")",
        ".",
        "heartbeat",
        "(",
        ")",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "# Get all the relevant task instances",
        "TI",
        "=",
        "TaskInstance",
        "successful_tis",
        "=",
        "(",
        "session",
        ".",
        "query",
        "(",
        "TI",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "dag_id",
        ".",
        "in_",
        "(",
        "DAG_IDS",
        ")",
        ")",
        ".",
        "filter",
        "(",
        "TI",
        ".",
        "state",
        ".",
        "in_",
        "(",
        "[",
        "State",
        ".",
        "SUCCESS",
        "]",
        ")",
        ")",
        ".",
        "all",
        "(",
        ")",
        ")",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "dagbag",
        "=",
        "DagBag",
        "(",
        "SUBDIR",
        ")",
        "dags",
        "=",
        "[",
        "dagbag",
        ".",
        "dags",
        "[",
        "dag_id",
        "]",
        "for",
        "dag_id",
        "in",
        "DAG_IDS",
        "]",
        "# the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.",
        "num_task_instances",
        "=",
        "sum",
        "(",
        "[",
        "(",
        "timezone",
        ".",
        "utcnow",
        "(",
        ")",
        "-",
        "task",
        ".",
        "start_date",
        ")",
        ".",
        "days",
        "for",
        "dag",
        "in",
        "dags",
        "for",
        "task",
        "in",
        "dag",
        ".",
        "tasks",
        "]",
        ")",
        "if",
        "(",
        "len",
        "(",
        "successful_tis",
        ")",
        "==",
        "num_task_instances",
        "or",
        "(",
        "timezone",
        ".",
        "utcnow",
        "(",
        ")",
        "-",
        "self",
        ".",
        "start_date",
        ")",
        ".",
        "total_seconds",
        "(",
        ")",
        ">",
        "MAX_RUNTIME_SECS",
        ")",
        ":",
        "if",
        "len",
        "(",
        "successful_tis",
        ")",
        "==",
        "num_task_instances",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"All tasks processed! Printing stats.\"",
        ")",
        "else",
        ":",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Test timeout reached. Printing available stats.\"",
        ")",
        "self",
        ".",
        "print_stats",
        "(",
        ")",
        "set_dags_paused_state",
        "(",
        "True",
        ")",
        "sys",
        ".",
        "exit",
        "(",
        ")"
      ],
      "docstring": "Override the scheduler heartbeat to determine when the test is complete",
      "docstring_tokens": [
        "Override",
        "the",
        "scheduler",
        "heartbeat",
        "to",
        "determine",
        "when",
        "the",
        "test",
        "is",
        "complete"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L103-L135"
    },
    "target": "Override the scheduler heartbeat to determine when the test is complete",
    "arguments": [
      [
        "def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) # Get all the relevant task instances TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval. num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` The code is designed to monitor a set of DAGs which includes two DAGs with a daily schedule. It runs a series of tests to simulate execution of the tasks within the DAGs and prints statistics such as task and dag runtime. This code assumes that there are two DAGs named `perf_dag_1` and `perf_dag_2`. If all tasks are run successfully before the maximum allowed runtime, the code will stop the test, otherwise it will continue until the test runs for a maximum specified period.\nIn conclusion, the code provided is a Python script that extends the `SchedulerMetricsJob` class to create a test job that simulates task execution of a specified directory of DAGs by Apache Airflow. The script includes various functions to simulate task execution, check the state of running tasks, and print statistics. However, it requires adjustments to be made to the `print_stats()` method to correctly print the desired statistics. The final solution provides a more detailed implementation of the test job, taking into consideration additional tasks and runtime constraints.\nThis is a solution to the initial problem. I have not seen any issues with the code so far. It is a complete and correct solution to the problem. The code is well-organized"
      ]
    ],
    "filtered_resps": [
      " ``` The code is designed to monitor a set of DAGs which includes two DAGs with a daily schedule. It runs a series of tests to simulate execution of the tasks within the DAGs and prints statistics such as task and dag runtime. This code assumes that there are two DAGs named `perf_dag_1` and `perf_dag_2`. If all tasks are run successfully before the maximum allowed runtime, the code will stop the test, otherwise it will continue until the test runs for a maximum specified period.\nIn conclusion, the code provided is a Python script that extends the `SchedulerMetricsJob` class to create a test job that simulates task execution of a specified directory of DAGs by Apache Airflow. The script includes various functions to simulate task execution, check the state of running tasks, and print statistics. However, it requires adjustments to be made to the `print_stats()` method to correctly print the desired statistics. The final solution provides a more detailed implementation of the test job, taking into consideration additional tasks and runtime constraints.\nThis is a solution to the initial problem. I have not seen any issues with the code so far. It is a complete and correct solution to the problem. The code is well-organized"
    ],
    "smoothed_bleu_4": 0.7558254947495182
  },
  {
    "doc_id": 65,
    "doc": {
      "id": 65,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/aws_lambda_hook.py",
      "func_name": "AwsLambdaHook.invoke_lambda",
      "original_string": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response",
      "language": "python",
      "code": "def invoke_lambda(self, payload):\n        \"\"\"\n        Invoke Lambda Function\n        \"\"\"\n\n        awslambda_conn = self.get_conn()\n\n        response = awslambda_conn.invoke(\n            FunctionName=self.function_name,\n            InvocationType=self.invocation_type,\n            LogType=self.log_type,\n            Payload=payload,\n            Qualifier=self.qualifier\n        )\n\n        return response",
      "code_tokens": [
        "def",
        "invoke_lambda",
        "(",
        "self",
        ",",
        "payload",
        ")",
        ":",
        "awslambda_conn",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        "response",
        "=",
        "awslambda_conn",
        ".",
        "invoke",
        "(",
        "FunctionName",
        "=",
        "self",
        ".",
        "function_name",
        ",",
        "InvocationType",
        "=",
        "self",
        ".",
        "invocation_type",
        ",",
        "LogType",
        "=",
        "self",
        ".",
        "log_type",
        ",",
        "Payload",
        "=",
        "payload",
        ",",
        "Qualifier",
        "=",
        "self",
        ".",
        "qualifier",
        ")",
        "return",
        "response"
      ],
      "docstring": "Invoke Lambda Function",
      "docstring_tokens": [
        "Invoke",
        "Lambda",
        "Function"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_lambda_hook.py#L53-L68"
    },
    "target": "Invoke Lambda Function",
    "arguments": [
      [
        "def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( FunctionName = self . function_name , InvocationType = self . invocation_type , LogType = self . log_type , Payload = payload , Qualifier = self . qualifier ) return response",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". get ( 'StatusCode' )\n#... more code...\n\nclass LambdaFunctionEventBridgeEvent ( AWSLambdaEventBridgeEvent ) :\n...\n\ninvoke_lambda = invoke_lambda_handler. invoke_lambda\n\nclass TestLambdaFunctionEventBridgeEvent ( unittest. TestCase ) :\ndef test_invoke_lambda ( self ) : # Given the test setup: given the test data: given a function that returns a mock response response = Response ( statusCode = 200 ) # When the function is invoked with the test data: result = invoke_lambda ( '{\"test\": \"data\"}' ) # Then the function should return a successful status: self. assertEqual ( result, 200 )\n[ end of code ]\n\n## Discussion\n\nThis example code defines a unit test class `TestLambdaFunctionEventBridgeEvent` that tests the `invoke_lambda` method within the `LambdaFunctionEventBridgeEvent` class. The test method `test_invoke_lambda` verifies that when the `invoke_lambda` method is invoked with a specific test payload, it returns a successful status code (200). This ensures that the method behaves correctly under this specific scenario. This kind of testing helps ensure that code behaves as expected in certain situations, which is an essential aspect of software development.\n\n## Step 1: Identify the"
      ]
    ],
    "filtered_resps": [
      ". get ( 'StatusCode' )\n#... more code...\n\nclass LambdaFunctionEventBridgeEvent ( AWSLambdaEventBridgeEvent ) :\n...\n\ninvoke_lambda = invoke_lambda_handler. invoke_lambda\n\nclass TestLambdaFunctionEventBridgeEvent ( unittest. TestCase ) :\ndef test_invoke_lambda ( self ) : # Given the test setup: given the test data: given a function that returns a mock response response = Response ( statusCode = 200 ) # When the function is invoked with the test data: result = invoke_lambda ( '{\"test\": \"data\"}' ) # Then the function should return a successful status: self. assertEqual ( result, 200 )\n[ end of code ]\n\n## Discussion\n\nThis example code defines a unit test class `TestLambdaFunctionEventBridgeEvent` that tests the `invoke_lambda` method within the `LambdaFunctionEventBridgeEvent` class. The test method `test_invoke_lambda` verifies that when the `invoke_lambda` method is invoked with a specific test payload, it returns a successful status code (200). This ensures that the method behaves correctly under this specific scenario. This kind of testing helps ensure that code behaves as expected in certain situations, which is an essential aspect of software development.\n\n## Step 1: Identify the"
    ],
    "smoothed_bleu_4": 0.5366285598032902
  },
  {
    "doc_id": 66,
    "doc": {
      "id": 66,
      "repo": "apache/airflow",
      "path": "airflow/contrib/utils/mlengine_operator_utils.py",
      "func_name": "create_evaluate_ops",
      "original_string": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation",
      "language": "python",
      "code": "def create_evaluate_ops(task_prefix,\n                        data_format,\n                        input_paths,\n                        prediction_path,\n                        metric_fn_and_keys,\n                        validate_fn,\n                        batch_prediction_job_id=None,\n                        project_id=None,\n                        region=None,\n                        dataflow_options=None,\n                        model_uri=None,\n                        model_name=None,\n                        version_name=None,\n                        dag=None):\n    \"\"\"\n    Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)\n    \"\"\"\n\n    # Verify that task_prefix doesn't have any special characters except hyphen\n    # '-', which is the only allowed non-alphanumeric character by Dataflow.\n    if not re.match(r\"^[a-zA-Z][-A-Za-z0-9]*$\", task_prefix):\n        raise AirflowException(\n            \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"\n            \"and hyphens are allowed but got: \" + task_prefix)\n\n    metric_fn, metric_keys = metric_fn_and_keys\n    if not callable(metric_fn):\n        raise AirflowException(\"`metric_fn` param must be callable.\")\n    if not callable(validate_fn):\n        raise AirflowException(\"`validate_fn` param must be callable.\")\n\n    if dag is not None and dag.default_args is not None:\n        default_args = dag.default_args\n        project_id = project_id or default_args.get('project_id')\n        region = region or default_args.get('region')\n        model_name = model_name or default_args.get('model_name')\n        version_name = version_name or default_args.get('version_name')\n        dataflow_options = dataflow_options or \\\n            default_args.get('dataflow_default_options')\n\n    evaluate_prediction = MLEngineBatchPredictionOperator(\n        task_id=(task_prefix + \"-prediction\"),\n        project_id=project_id,\n        job_id=batch_prediction_job_id,\n        region=region,\n        data_format=data_format,\n        input_paths=input_paths,\n        output_path=prediction_path,\n        uri=model_uri,\n        model_name=model_name,\n        version_name=version_name,\n        dag=dag)\n\n    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))\n    evaluate_summary = DataFlowPythonOperator(\n        task_id=(task_prefix + \"-summary\"),\n        py_options=[\"-m\"],\n        py_file=\"airflow.contrib.utils.mlengine_prediction_summary\",\n        dataflow_default_options=dataflow_options,\n        options={\n            \"prediction_path\": prediction_path,\n            \"metric_fn_encoded\": metric_fn_encoded,\n            \"metric_keys\": ','.join(metric_keys)\n        },\n        dag=dag)\n    evaluate_summary.set_upstream(evaluate_prediction)\n\n    def apply_validate_fn(*args, **kwargs):\n        prediction_path = kwargs[\"templates_dict\"][\"prediction_path\"]\n        scheme, bucket, obj, _, _ = urlsplit(prediction_path)\n        if scheme != \"gs\" or not bucket or not obj:\n            raise ValueError(\"Wrong format prediction_path: %s\",\n                             prediction_path)\n        summary = os.path.join(obj.strip(\"/\"),\n                               \"prediction.summary.json\")\n        gcs_hook = GoogleCloudStorageHook()\n        summary = json.loads(gcs_hook.download(bucket, summary))\n        return validate_fn(summary)\n\n    evaluate_validation = PythonOperator(\n        task_id=(task_prefix + \"-validation\"),\n        python_callable=apply_validate_fn,\n        provide_context=True,\n        templates_dict={\"prediction_path\": prediction_path},\n        dag=dag)\n    evaluate_validation.set_upstream(evaluate_summary)\n\n    return evaluate_prediction, evaluate_summary, evaluate_validation",
      "code_tokens": [
        "def",
        "create_evaluate_ops",
        "(",
        "task_prefix",
        ",",
        "data_format",
        ",",
        "input_paths",
        ",",
        "prediction_path",
        ",",
        "metric_fn_and_keys",
        ",",
        "validate_fn",
        ",",
        "batch_prediction_job_id",
        "=",
        "None",
        ",",
        "project_id",
        "=",
        "None",
        ",",
        "region",
        "=",
        "None",
        ",",
        "dataflow_options",
        "=",
        "None",
        ",",
        "model_uri",
        "=",
        "None",
        ",",
        "model_name",
        "=",
        "None",
        ",",
        "version_name",
        "=",
        "None",
        ",",
        "dag",
        "=",
        "None",
        ")",
        ":",
        "# Verify that task_prefix doesn't have any special characters except hyphen",
        "# '-', which is the only allowed non-alphanumeric character by Dataflow.",
        "if",
        "not",
        "re",
        ".",
        "match",
        "(",
        "r\"^[a-zA-Z][-A-Za-z0-9]*$\"",
        ",",
        "task_prefix",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"Malformed task_id for DataFlowPythonOperator (only alphanumeric \"",
        "\"and hyphens are allowed but got: \"",
        "+",
        "task_prefix",
        ")",
        "metric_fn",
        ",",
        "metric_keys",
        "=",
        "metric_fn_and_keys",
        "if",
        "not",
        "callable",
        "(",
        "metric_fn",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"`metric_fn` param must be callable.\"",
        ")",
        "if",
        "not",
        "callable",
        "(",
        "validate_fn",
        ")",
        ":",
        "raise",
        "AirflowException",
        "(",
        "\"`validate_fn` param must be callable.\"",
        ")",
        "if",
        "dag",
        "is",
        "not",
        "None",
        "and",
        "dag",
        ".",
        "default_args",
        "is",
        "not",
        "None",
        ":",
        "default_args",
        "=",
        "dag",
        ".",
        "default_args",
        "project_id",
        "=",
        "project_id",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'project_id'",
        ")",
        "region",
        "=",
        "region",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'region'",
        ")",
        "model_name",
        "=",
        "model_name",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'model_name'",
        ")",
        "version_name",
        "=",
        "version_name",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'version_name'",
        ")",
        "dataflow_options",
        "=",
        "dataflow_options",
        "or",
        "default_args",
        ".",
        "get",
        "(",
        "'dataflow_default_options'",
        ")",
        "evaluate_prediction",
        "=",
        "MLEngineBatchPredictionOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-prediction\"",
        ")",
        ",",
        "project_id",
        "=",
        "project_id",
        ",",
        "job_id",
        "=",
        "batch_prediction_job_id",
        ",",
        "region",
        "=",
        "region",
        ",",
        "data_format",
        "=",
        "data_format",
        ",",
        "input_paths",
        "=",
        "input_paths",
        ",",
        "output_path",
        "=",
        "prediction_path",
        ",",
        "uri",
        "=",
        "model_uri",
        ",",
        "model_name",
        "=",
        "model_name",
        ",",
        "version_name",
        "=",
        "version_name",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "metric_fn_encoded",
        "=",
        "base64",
        ".",
        "b64encode",
        "(",
        "dill",
        ".",
        "dumps",
        "(",
        "metric_fn",
        ",",
        "recurse",
        "=",
        "True",
        ")",
        ")",
        "evaluate_summary",
        "=",
        "DataFlowPythonOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-summary\"",
        ")",
        ",",
        "py_options",
        "=",
        "[",
        "\"-m\"",
        "]",
        ",",
        "py_file",
        "=",
        "\"airflow.contrib.utils.mlengine_prediction_summary\"",
        ",",
        "dataflow_default_options",
        "=",
        "dataflow_options",
        ",",
        "options",
        "=",
        "{",
        "\"prediction_path\"",
        ":",
        "prediction_path",
        ",",
        "\"metric_fn_encoded\"",
        ":",
        "metric_fn_encoded",
        ",",
        "\"metric_keys\"",
        ":",
        "','",
        ".",
        "join",
        "(",
        "metric_keys",
        ")",
        "}",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "evaluate_summary",
        ".",
        "set_upstream",
        "(",
        "evaluate_prediction",
        ")",
        "def",
        "apply_validate_fn",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "prediction_path",
        "=",
        "kwargs",
        "[",
        "\"templates_dict\"",
        "]",
        "[",
        "\"prediction_path\"",
        "]",
        "scheme",
        ",",
        "bucket",
        ",",
        "obj",
        ",",
        "_",
        ",",
        "_",
        "=",
        "urlsplit",
        "(",
        "prediction_path",
        ")",
        "if",
        "scheme",
        "!=",
        "\"gs\"",
        "or",
        "not",
        "bucket",
        "or",
        "not",
        "obj",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"Wrong format prediction_path: %s\"",
        ",",
        "prediction_path",
        ")",
        "summary",
        "=",
        "os",
        ".",
        "path",
        ".",
        "join",
        "(",
        "obj",
        ".",
        "strip",
        "(",
        "\"/\"",
        ")",
        ",",
        "\"prediction.summary.json\"",
        ")",
        "gcs_hook",
        "=",
        "GoogleCloudStorageHook",
        "(",
        ")",
        "summary",
        "=",
        "json",
        ".",
        "loads",
        "(",
        "gcs_hook",
        ".",
        "download",
        "(",
        "bucket",
        ",",
        "summary",
        ")",
        ")",
        "return",
        "validate_fn",
        "(",
        "summary",
        ")",
        "evaluate_validation",
        "=",
        "PythonOperator",
        "(",
        "task_id",
        "=",
        "(",
        "task_prefix",
        "+",
        "\"-validation\"",
        ")",
        ",",
        "python_callable",
        "=",
        "apply_validate_fn",
        ",",
        "provide_context",
        "=",
        "True",
        ",",
        "templates_dict",
        "=",
        "{",
        "\"prediction_path\"",
        ":",
        "prediction_path",
        "}",
        ",",
        "dag",
        "=",
        "dag",
        ")",
        "evaluate_validation",
        ".",
        "set_upstream",
        "(",
        "evaluate_summary",
        ")",
        "return",
        "evaluate_prediction",
        ",",
        "evaluate_summary",
        ",",
        "evaluate_validation"
      ],
      "docstring": "Creates Operators needed for model evaluation and returns.\n\n    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n    calling MLEngineBatchPredictionOperator, then summarize and validate\n    the result via Cloud Dataflow using DataFlowPythonOperator.\n\n    For details and pricing about Batch prediction, please refer to the website\n    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n\n    It returns three chained operators for prediction, summary, and validation,\n    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n    respectively.\n    (<prefix> should contain only alphanumeric characters or hyphen.)\n\n    The upstream and downstream can be set accordingly like:\n      pred, _, val = create_evaluate_ops(...)\n      pred.set_upstream(upstream_op)\n      ...\n      downstream_op.set_upstream(val)\n\n    Callers will provide two python callables, metric_fn and validate_fn, in\n    order to customize the evaluation behavior as they wish.\n    - metric_fn receives a dictionary per instance derived from json in the\n      batch prediction result. The keys might vary depending on the model.\n      It should return a tuple of metrics.\n    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n      generated over all instances.\n      The key/value of the dictionary matches to what's given by\n      metric_fn_and_keys arg.\n      The dictionary contains an additional metric, 'count' to represent the\n      total number of instances received for evaluation.\n      The function would raise an exception to mark the task as failed, in a\n      case the validation result is not okay to proceed (i.e. to set the trained\n      version as default).\n\n    Typical examples are like this:\n\n    def get_metric_fn_and_keys():\n        import math  # imports should be outside of the metric_fn below.\n        def error_and_squared_error(inst):\n            label = float(inst['input_label'])\n            classes = float(inst['classes'])  # 0 or 1\n            err = abs(classes-label)\n            squared_err = math.pow(classes-label, 2)\n            return (err, squared_err)  # returns a tuple.\n        return error_and_squared_error, ['err', 'mse']  # key order must match.\n\n    def validate_err_and_count(summary):\n        if summary['err'] > 0.2:\n            raise ValueError('Too high err>0.2; summary=%s' % summary)\n        if summary['mse'] > 0.05:\n            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n        if summary['count'] < 1000:\n            raise ValueError('Too few instances<1000; summary=%s' % summary)\n        return summary\n\n    For the details on the other BatchPrediction-related arguments (project_id,\n    job_id, region, data_format, input_paths, prediction_path, model_uri),\n    please refer to MLEngineBatchPredictionOperator too.\n\n    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n        hyphen are allowed (no underscores), since this will be used as dataflow\n        job name, which doesn't allow other characters.\n    :type task_prefix: str\n\n    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n    :type data_format: str\n\n    :param input_paths: a list of input paths to be sent to BatchPrediction.\n    :type input_paths: list[str]\n\n    :param prediction_path: GCS path to put the prediction results in.\n    :type prediction_path: str\n\n    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n        - metric_fn is a function that accepts a dictionary (for an instance),\n          and returns a tuple of metric(s) that it calculates.\n        - metric_keys is a list of strings to denote the key of each metric.\n    :type metric_fn_and_keys: tuple of a function and a list[str]\n\n    :param validate_fn: a function to validate whether the averaged metric(s) is\n        good enough to push the model.\n    :type validate_fn: function\n\n    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n        the job_id argument.\n    :type batch_prediction_job_id: str\n\n    :param project_id: the Google Cloud Platform project id in which to execute\n        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['project_id']` will be used.\n    :type project_id: str\n\n    :param region: the Google Cloud Platform region in which to execute Cloud ML\n        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n        `default_args['region']` will be used.\n    :type region: str\n\n    :param dataflow_options: options to run Dataflow jobs. If None, then the\n        `dag`'s `default_args['dataflow_default_options']` will be used.\n    :type dataflow_options: dictionary\n\n    :param model_uri: GCS path of the model exported by Tensorflow using\n        tensorflow.estimator.export_savedmodel(). It cannot be used with\n        model_name or version_name below. See MLEngineBatchPredictionOperator for\n        more detail.\n    :type model_uri: str\n\n    :param model_name: Used to indicate a model to use for prediction. Can be\n        used in combination with version_name, but cannot be used together with\n        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n        then the `dag`'s `default_args['model_name']` will be used.\n    :type model_name: str\n\n    :param version_name: Used to indicate a model version to use for prediction,\n        in combination with model_name. Cannot be used together with model_uri.\n        See MLEngineBatchPredictionOperator for more detail. If None, then the\n        `dag`'s `default_args['version_name']` will be used.\n    :type version_name: str\n\n    :param dag: The `DAG` to use for all Operators.\n    :type dag: airflow.models.DAG\n\n    :returns: a tuple of three operators, (prediction, summary, validation)\n    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n                  PythonOperator)",
      "docstring_tokens": [
        "Creates",
        "Operators",
        "needed",
        "for",
        "model",
        "evaluation",
        "and",
        "returns",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/mlengine_operator_utils.py#L32-L246"
    },
    "target": "Creates Operators needed for model evaluation and returns .",
    "arguments": [
      [
        "def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : # Verify that task_prefix doesn't have any special characters except hyphen # '-', which is the only allowed non-alphanumeric character by Dataflow. if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise AirflowException ( \"`metric_fn` param must be callable.\" ) if not callable ( validate_fn ) : raise AirflowException ( \"`validate_fn` param must be callable.\" ) if dag is not None and dag . default_args is not None : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , * * kwargs ) : prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != \"gs\" or not bucket or not obj : raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) gcs_hook = GoogleCloudStorageHook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " [docs] class GCSObjectList ( list ) : \"\"\"A list object that can be written to GCS Bucket in a batched way.\"\"\" [docs] @classmethod def from_uri ( cls, uri ) : \"\"\"Create a GCSObjectList from a string URI like gs://bucket/folder/*.csv. Returns a list of objects which can be read/written with GoogleCloudStorageHook.\"\"\" bucket, prefix_path = common. gcs_path_to_bucket_and_path ( uri ) obj_list = GoogleCloudStorageHook. object_list ( bucket, prefix_path ) return cls ( obj_list ) @classmethod def from_json ( cls, uri, encoding = \"utf-8\" ) : \"\"\"Create a GCSObjectList from a JSON file which may be a list of object specifications (e.g. a list of strings) or a list of list of object specifications. Returns a list of objects which can be read/written with GoogleCloudStorageHook.\"\"\" bucket, prefix_path = common. gcs_path_to_bucket_and_path ( uri ) with GoogleCloudStorageHook. download ( bucket, prefix_path + \".json\" ) as ( f, _ ) : data = json. load ( f, strict = False, encoding = encoding )"
      ]
    ],
    "filtered_resps": [
      " [docs] class GCSObjectList ( list ) : \"\"\"A list object that can be written to GCS Bucket in a batched way.\"\"\" [docs] @classmethod def from_uri ( cls, uri ) : \"\"\"Create a GCSObjectList from a string URI like gs://bucket/folder/*.csv. Returns a list of objects which can be read/written with GoogleCloudStorageHook.\"\"\" bucket, prefix_path = common. gcs_path_to_bucket_and_path ( uri ) obj_list = GoogleCloudStorageHook. object_list ( bucket, prefix_path ) return cls ( obj_list ) @classmethod def from_json ( cls, uri, encoding = \"utf-8\" ) : \"\"\"Create a GCSObjectList from a JSON file which may be a list of object specifications (e.g. a list of strings) or a list of list of object specifications. Returns a list of objects which can be read/written with GoogleCloudStorageHook.\"\"\" bucket, prefix_path = common. gcs_path_to_bucket_and_path ( uri ) with GoogleCloudStorageHook. download ( bucket, prefix_path + \".json\" ) as ( f, _ ) : data = json. load ( f, strict = False, encoding = encoding )"
    ],
    "smoothed_bleu_4": 0.4980437080356114
  },
  {
    "doc_id": 67,
    "doc": {
      "id": 67,
      "repo": "apache/airflow",
      "path": "airflow/utils/file.py",
      "func_name": "mkdirs",
      "original_string": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
      "language": "python",
      "code": "def mkdirs(path, mode):\n    \"\"\"\n    Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int\n    \"\"\"\n    try:\n        o_umask = os.umask(0)\n        os.makedirs(path, mode)\n    except OSError:\n        if not os.path.isdir(path):\n            raise\n    finally:\n        os.umask(o_umask)",
      "code_tokens": [
        "def",
        "mkdirs",
        "(",
        "path",
        ",",
        "mode",
        ")",
        ":",
        "try",
        ":",
        "o_umask",
        "=",
        "os",
        ".",
        "umask",
        "(",
        "0",
        ")",
        "os",
        ".",
        "makedirs",
        "(",
        "path",
        ",",
        "mode",
        ")",
        "except",
        "OSError",
        ":",
        "if",
        "not",
        "os",
        ".",
        "path",
        ".",
        "isdir",
        "(",
        "path",
        ")",
        ":",
        "raise",
        "finally",
        ":",
        "os",
        ".",
        "umask",
        "(",
        "o_umask",
        ")"
      ],
      "docstring": "Creates the directory specified by path, creating intermediate directories\n    as necessary. If directory already exists, this is a no-op.\n\n    :param path: The directory to create\n    :type path: str\n    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n    :type mode: int",
      "docstring_tokens": [
        "Creates",
        "the",
        "directory",
        "specified",
        "by",
        "path",
        "creating",
        "intermediate",
        "directories",
        "as",
        "necessary",
        ".",
        "If",
        "directory",
        "already",
        "exists",
        "this",
        "is",
        "a",
        "no",
        "-",
        "op",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/file.py#L42-L59"
    },
    "target": "Creates the directory specified by path creating intermediate directories as necessary . If directory already exists this is a no - op .",
    "arguments": [
      [
        "def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except OSError : if not os . path . isdir ( path ) : raise finally : os . umask ( o_umask )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return def _make_temporary_directory ( ) : tmpdir = mkdtemp ( 'pycqa' ) mkadir ( tmpdir, os. mode_t ( 0o701 ) ) ( os. chown, tempfile. set_last_modified, os. utime ) = ( None, None, None ) try : if hasattr ( tempfile, 'gettempdir' ) : os. chown ( tmpdir, 0, 0 ) os. utime ( tmpdir, ( 0, 0 ) ) finally : ( os. chown, tempfile. set_last_modified, os. utime ) = ( tempfile. chown, tempfile. set_last_modified, tempfile. utime ) return tmpdir def _run_program ( program, * args, stdin = None, stdout = None, envvars = None, ignore_errors = True, max_memory_usage = None, log_file = None, cwd = None ) : import _subprocess import _tempfile tmpdir = _tempfile. _get_temp_dir ( ) fargs = list ( args ) if ( isinstance ( envvars, dict ) or ( envvars is None and envvars is not False ) ) and ( _run_program."
      ]
    ],
    "filtered_resps": [
      " return def _make_temporary_directory ( ) : tmpdir = mkdtemp ( 'pycqa' ) mkadir ( tmpdir, os. mode_t ( 0o701 ) ) ( os. chown, tempfile. set_last_modified, os. utime ) = ( None, None, None ) try : if hasattr ( tempfile, 'gettempdir' ) : os. chown ( tmpdir, 0, 0 ) os. utime ( tmpdir, ( 0, 0 ) ) finally : ( os. chown, tempfile. set_last_modified, os. utime ) = ( tempfile. chown, tempfile. set_last_modified, tempfile. utime ) return tmpdir def _run_program ( program, * args, stdin = None, stdout = None, envvars = None, ignore_errors = True, max_memory_usage = None, log_file = None, cwd = None ) : import _subprocess import _tempfile tmpdir = _tempfile. _get_temp_dir ( ) fargs = list ( args ) if ( isinstance ( envvars, dict ) or ( envvars is None and envvars is not False ) ) and ( _run_program."
    ],
    "smoothed_bleu_4": 0.6522829110648919
  },
  {
    "doc_id": 68,
    "doc": {
      "id": 68,
      "repo": "apache/airflow",
      "path": "airflow/operators/check_operator.py",
      "func_name": "_convert_to_float_if_possible",
      "original_string": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str\n    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret",
      "language": "python",
      "code": "def _convert_to_float_if_possible(s):\n    \"\"\"\n    A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str\n    \"\"\"\n    try:\n        ret = float(s)\n    except (ValueError, TypeError):\n        ret = s\n    return ret",
      "code_tokens": [
        "def",
        "_convert_to_float_if_possible",
        "(",
        "s",
        ")",
        ":",
        "try",
        ":",
        "ret",
        "=",
        "float",
        "(",
        "s",
        ")",
        "except",
        "(",
        "ValueError",
        ",",
        "TypeError",
        ")",
        ":",
        "ret",
        "=",
        "s",
        "return",
        "ret"
      ],
      "docstring": "A small helper function to convert a string to a numeric value\n    if appropriate\n\n    :param s: the string to be converted\n    :type s: str",
      "docstring_tokens": [
        "A",
        "small",
        "helper",
        "function",
        "to",
        "convert",
        "a",
        "string",
        "to",
        "a",
        "numeric",
        "value",
        "if",
        "appropriate"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/check_operator.py#L98-L110"
    },
    "target": "A small helper function to convert a string to a numeric value if appropriate",
    "arguments": [
      [
        "def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( ValueError , TypeError ) : ret = s return ret",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _convert_to_list ( l ) : ret = [ ]\n    -...  # other code...\n\ndef convert_to_string ( l ) : return _convert_to_list ( l )\n```\n\nThis code snippet introduces a naming convention conflict by using both camelCase and underscores for variable names. The function `convert_to_string` is renamed to follow the original naming convention. The use of a separate function `_convert_to_list` is consistent with the previous version, but it now takes an argument to accommodate the parameter types it processes.\n```python\ndef convert_to_string ( l ) : return _process_values ( l )\n```\n\nThis updated code provides the required function name change and maintains consistency in naming conventions. The `_process_values` function remains unchanged, allowing for easier maintenance and modifications.\n```python\ndef _process_values ( l ) : ret = _normalize_values ( l )\n    #...  # other code...\n\ndef _normalize_values ( s ) : try : ret = float ( s ) except ( ValueError, TypeError ) : ret = s return ret def _convert_to_list ( l ) : ret = [ ]\n    -...  # other code...\n\ndef _process_values ( l ) : ret = _convert_to_float_or_s"
      ]
    ],
    "filtered_resps": [
      " def _convert_to_list ( l ) : ret = [ ]\n    -...  # other code...\n\ndef convert_to_string ( l ) : return _convert_to_list ( l )\n```\n\nThis code snippet introduces a naming convention conflict by using both camelCase and underscores for variable names. The function `convert_to_string` is renamed to follow the original naming convention. The use of a separate function `_convert_to_list` is consistent with the previous version, but it now takes an argument to accommodate the parameter types it processes.\n```python\ndef convert_to_string ( l ) : return _process_values ( l )\n```\n\nThis updated code provides the required function name change and maintains consistency in naming conventions. The `_process_values` function remains unchanged, allowing for easier maintenance and modifications.\n```python\ndef _process_values ( l ) : ret = _normalize_values ( l )\n    #...  # other code...\n\ndef _normalize_values ( s ) : try : ret = float ( s ) except ( ValueError, TypeError ) : ret = s return ret def _convert_to_list ( l ) : ret = [ ]\n    -...  # other code...\n\ndef _process_values ( l ) : ret = _convert_to_float_or_s"
    ],
    "smoothed_bleu_4": 0.5722369352901879
  },
  {
    "doc_id": 69,
    "doc": {
      "id": 69,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "make_aware",
      "original_string": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\n            \"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)",
      "language": "python",
      "code": "def make_aware(value, timezone=None):\n    \"\"\"\n    Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone\n\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Check that we won't overwrite the timezone of an aware datetime.\n    if is_localized(value):\n        raise ValueError(\n            \"make_aware expects a naive datetime, got %s\" % value)\n    if hasattr(value, 'fold'):\n        # In case of python 3.6 we want to do the same that pendulum does for python3.5\n        # i.e in case we move clock back we want to schedule the run at the time of the second\n        # instance of the same clock time rather than the first one.\n        # Fold parameter has no impact in other cases so we can safely set it to 1 here\n        value = value.replace(fold=1)\n    if hasattr(timezone, 'localize'):\n        # This method is available for pytz time zones.\n        return timezone.localize(value)\n    elif hasattr(timezone, 'convert'):\n        # For pendulum\n        return timezone.convert(value)\n    else:\n        # This may be wrong around DST changes!\n        return value.replace(tzinfo=timezone)",
      "code_tokens": [
        "def",
        "make_aware",
        "(",
        "value",
        ",",
        "timezone",
        "=",
        "None",
        ")",
        ":",
        "if",
        "timezone",
        "is",
        "None",
        ":",
        "timezone",
        "=",
        "TIMEZONE",
        "# Check that we won't overwrite the timezone of an aware datetime.",
        "if",
        "is_localized",
        "(",
        "value",
        ")",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"make_aware expects a naive datetime, got %s\"",
        "%",
        "value",
        ")",
        "if",
        "hasattr",
        "(",
        "value",
        ",",
        "'fold'",
        ")",
        ":",
        "# In case of python 3.6 we want to do the same that pendulum does for python3.5",
        "# i.e in case we move clock back we want to schedule the run at the time of the second",
        "# instance of the same clock time rather than the first one.",
        "# Fold parameter has no impact in other cases so we can safely set it to 1 here",
        "value",
        "=",
        "value",
        ".",
        "replace",
        "(",
        "fold",
        "=",
        "1",
        ")",
        "if",
        "hasattr",
        "(",
        "timezone",
        ",",
        "'localize'",
        ")",
        ":",
        "# This method is available for pytz time zones.",
        "return",
        "timezone",
        ".",
        "localize",
        "(",
        "value",
        ")",
        "elif",
        "hasattr",
        "(",
        "timezone",
        ",",
        "'convert'",
        ")",
        ":",
        "# For pendulum",
        "return",
        "timezone",
        ".",
        "convert",
        "(",
        "value",
        ")",
        "else",
        ":",
        "# This may be wrong around DST changes!",
        "return",
        "value",
        ".",
        "replace",
        "(",
        "tzinfo",
        "=",
        "timezone",
        ")"
      ],
      "docstring": "Make a naive datetime.datetime in a given time zone aware.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: localized datetime in settings.TIMEZONE or timezone",
      "docstring_tokens": [
        "Make",
        "a",
        "naive",
        "datetime",
        ".",
        "datetime",
        "in",
        "a",
        "given",
        "time",
        "zone",
        "aware",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L98-L128"
    },
    "target": "Make a naive datetime . datetime in a given time zone aware .",
    "arguments": [
      [
        "def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Check that we won't overwrite the timezone of an aware datetime. if is_localized ( value ) : raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) if hasattr ( value , 'fold' ) : # In case of python 3.6 we want to do the same that pendulum does for python3.5 # i.e in case we move clock back we want to schedule the run at the time of the second # instance of the same clock time rather than the first one. # Fold parameter has no impact in other cases so we can safely set it to 1 here value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : # This method is available for pytz time zones. return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : # For pendulum return timezone . convert ( value ) else : # This may be wrong around DST changes! return value . replace ( tzinfo = timezone )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def convert_timezone ( value, from_timezone, to_timezone ) : local_tz = to_timezone if isinstance ( value, tz ) : value = make_aware ( value, from_timezone ) return value. astimezone ( to_timezone ) else : return value. replace ( tzinfo = to_timezone ) def get_aware ( value, timezone = None ) : if timezone is None : timezone = TIMEZONE return make_aware ( value, timezone )```\n\nThe provided code defines four functions for working with dates and times, particularly for converting between different timezones, making aware datetime objects, and converting timezone-aware datetime objects to different timezones.\n\nHere's a breakdown of each function:\n\n1.  `is_aware` check: This function checks whether a given datetime object is timezone-aware. It uses the `tzinfo` attribute to check if the object has any timezone information associated with it.\n2.  `make_aware` function: This function takes a naive datetime object and converts it into a timezone-aware object based on the specified timezone. It uses various methods (depending on the type of timezone) to localize the datetime object, ensuring that it is properly associated with a timezone.\n3.  `convert_timezone` function: This"
      ]
    ],
    "filtered_resps": [
      " def convert_timezone ( value, from_timezone, to_timezone ) : local_tz = to_timezone if isinstance ( value, tz ) : value = make_aware ( value, from_timezone ) return value. astimezone ( to_timezone ) else : return value. replace ( tzinfo = to_timezone ) def get_aware ( value, timezone = None ) : if timezone is None : timezone = TIMEZONE return make_aware ( value, timezone )```\n\nThe provided code defines four functions for working with dates and times, particularly for converting between different timezones, making aware datetime objects, and converting timezone-aware datetime objects to different timezones.\n\nHere's a breakdown of each function:\n\n1.  `is_aware` check: This function checks whether a given datetime object is timezone-aware. It uses the `tzinfo` attribute to check if the object has any timezone information associated with it.\n2.  `make_aware` function: This function takes a naive datetime object and converts it into a timezone-aware object based on the specified timezone. It uses various methods (depending on the type of timezone) to localize the datetime object, ensuring that it is properly associated with a timezone.\n3.  `convert_timezone` function: This"
    ],
    "smoothed_bleu_4": 1.2536351882947958
  },
  {
    "doc_id": 70,
    "doc": {
      "id": 70,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "make_naive",
      "original_string": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive",
      "language": "python",
      "code": "def make_naive(value, timezone=None):\n    \"\"\"\n    Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime\n    \"\"\"\n    if timezone is None:\n        timezone = TIMEZONE\n\n    # Emulate the behavior of astimezone() on Python < 3.6.\n    if is_naive(value):\n        raise ValueError(\"make_naive() cannot be applied to a naive datetime\")\n\n    o = value.astimezone(timezone)\n\n    # cross library compatibility\n    naive = dt.datetime(o.year,\n                        o.month,\n                        o.day,\n                        o.hour,\n                        o.minute,\n                        o.second,\n                        o.microsecond)\n\n    return naive",
      "code_tokens": [
        "def",
        "make_naive",
        "(",
        "value",
        ",",
        "timezone",
        "=",
        "None",
        ")",
        ":",
        "if",
        "timezone",
        "is",
        "None",
        ":",
        "timezone",
        "=",
        "TIMEZONE",
        "# Emulate the behavior of astimezone() on Python < 3.6.",
        "if",
        "is_naive",
        "(",
        "value",
        ")",
        ":",
        "raise",
        "ValueError",
        "(",
        "\"make_naive() cannot be applied to a naive datetime\"",
        ")",
        "o",
        "=",
        "value",
        ".",
        "astimezone",
        "(",
        "timezone",
        ")",
        "# cross library compatibility",
        "naive",
        "=",
        "dt",
        ".",
        "datetime",
        "(",
        "o",
        ".",
        "year",
        ",",
        "o",
        ".",
        "month",
        ",",
        "o",
        ".",
        "day",
        ",",
        "o",
        ".",
        "hour",
        ",",
        "o",
        ".",
        "minute",
        ",",
        "o",
        ".",
        "second",
        ",",
        "o",
        ".",
        "microsecond",
        ")",
        "return",
        "naive"
      ],
      "docstring": "Make an aware datetime.datetime naive in a given time zone.\n\n    :param value: datetime\n    :param timezone: timezone\n    :return: naive datetime",
      "docstring_tokens": [
        "Make",
        "an",
        "aware",
        "datetime",
        ".",
        "datetime",
        "naive",
        "in",
        "a",
        "given",
        "time",
        "zone",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L131-L157"
    },
    "target": "Make an aware datetime . datetime naive in a given time zone .",
    "arguments": [
      [
        "def make_naive ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Emulate the behavior of astimezone() on Python < 3.6. if is_naive ( value ) : raise ValueError ( \"make_naive() cannot be applied to a naive datetime\" ) o = value . astimezone ( timezone ) # cross library compatibility naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return naive",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " else : return value\n\ndef make_aware ( naivedt, timezone = None ) : if timezone is None : timezone = TIMEZONE from pytz import timezone as pytz_timezone # Emulate the behavior of astimezone() on Python < 3.6. naivedt = make_naive ( naivedt, timezone = timezone ) # Use the new tzlocal library to handle tzlocal timezone timezone = pytz_timezone ( naivedt. tzinfo. name ) # We need to create an aware datetime for use with the timezone. aware = naivedt. replace ( tzinfo = None ) aware = timezone. localize ( aware ) return aware\n```\nI've updated the `make_naive` to include the behavior of the `astimezone()` function on Python < 3.6 and fixed the docstrings and comments to match the original code. In the `make_naive` function, I've added a check for `timezone` to not be `None` and raised a `ValueError` if the `timezone` is `None` and the `value` is a naive date. This is to emulate the behavior of `astimezone()` function on Python < 3.6"
      ]
    ],
    "filtered_resps": [
      " else : return value\n\ndef make_aware ( naivedt, timezone = None ) : if timezone is None : timezone = TIMEZONE from pytz import timezone as pytz_timezone # Emulate the behavior of astimezone() on Python < 3.6. naivedt = make_naive ( naivedt, timezone = timezone ) # Use the new tzlocal library to handle tzlocal timezone timezone = pytz_timezone ( naivedt. tzinfo. name ) # We need to create an aware datetime for use with the timezone. aware = naivedt. replace ( tzinfo = None ) aware = timezone. localize ( aware ) return aware\n```\nI've updated the `make_naive` to include the behavior of the `astimezone()` function on Python < 3.6 and fixed the docstrings and comments to match the original code. In the `make_naive` function, I've added a check for `timezone` to not be `None` and raised a `ValueError` if the `timezone` is `None` and the `value` is a naive date. This is to emulate the behavior of `astimezone()` function on Python < 3.6"
    ],
    "smoothed_bleu_4": 1.198154358857071
  },
  {
    "doc_id": 71,
    "doc": {
      "id": 71,
      "repo": "apache/airflow",
      "path": "airflow/utils/timezone.py",
      "func_name": "datetime",
      "original_string": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)",
      "language": "python",
      "code": "def datetime(*args, **kwargs):\n    \"\"\"\n    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime\n    \"\"\"\n    if 'tzinfo' not in kwargs:\n        kwargs['tzinfo'] = TIMEZONE\n\n    return dt.datetime(*args, **kwargs)",
      "code_tokens": [
        "def",
        "datetime",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "if",
        "'tzinfo'",
        "not",
        "in",
        "kwargs",
        ":",
        "kwargs",
        "[",
        "'tzinfo'",
        "]",
        "=",
        "TIMEZONE",
        "return",
        "dt",
        ".",
        "datetime",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")"
      ],
      "docstring": "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n\n    :return: datetime.datetime",
      "docstring_tokens": [
        "Wrapper",
        "around",
        "datetime",
        ".",
        "datetime",
        "that",
        "adds",
        "settings",
        ".",
        "TIMEZONE",
        "if",
        "tzinfo",
        "not",
        "specified"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L160-L169"
    },
    "target": "Wrapper around datetime . datetime that adds settings . TIMEZONE if tzinfo not specified",
    "arguments": [
      [
        "def datetime ( * args , * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt . datetime ( * args , * * kwargs )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        ". astimezone ( kwargs [ 'tzinfo' ] )\ndef date ( * args, * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt. date ( * args, * * kwargs )\nThe date functions are used to handle date fields and the datetime function handles datetime fields.\n### Step 3: Create the data\nCreate a Pandas DataFrame to store the data. The data includes a datetime field and several other fields.\n\n```python\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', 100)\n\nstart_datetime = datetime(2022, 1, 1)\nend_datetime = datetime(2022, 12, 31)\n\ndata = {\n    'datetime': [datetime(2022, 1, 1), datetime(2022, 1, 2), datetime(2022, 1, 3)],\n    'field1': [1, 2, 3],\n    'field2': [4, 5, 6],\n    'field3': [7, 8, 9],\n}\n\ndf = pd.DataFrame(data)\n```\n\n### Step 4"
      ]
    ],
    "filtered_resps": [
      ". astimezone ( kwargs [ 'tzinfo' ] )\ndef date ( * args, * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt. date ( * args, * * kwargs )\nThe date functions are used to handle date fields and the datetime function handles datetime fields.\n### Step 3: Create the data\nCreate a Pandas DataFrame to store the data. The data includes a datetime field and several other fields.\n\n```python\nimport pandas as pd\nimport numpy as np\n\npd.set_option('display.max_columns', 100)\n\nstart_datetime = datetime(2022, 1, 1)\nend_datetime = datetime(2022, 12, 31)\n\ndata = {\n    'datetime': [datetime(2022, 1, 1), datetime(2022, 1, 2), datetime(2022, 1, 3)],\n    'field1': [1, 2, 3],\n    'field2': [4, 5, 6],\n    'field3': [7, 8, 9],\n}\n\ndf = pd.DataFrame(data)\n```\n\n### Step 4"
    ],
    "smoothed_bleu_4": 0.7149004416146778
  },
  {
    "doc_id": 72,
    "doc": {
      "id": 72,
      "repo": "apache/airflow",
      "path": "airflow/hooks/druid_hook.py",
      "func_name": "DruidDbApiHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.\n        \"\"\"\n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Establish a connection to druid broker.\n        \"\"\"\n        conn = self.get_connection(self.druid_broker_conn_id)\n        druid_broker_conn = connect(\n            host=conn.host,\n            port=conn.port,\n            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),\n            scheme=conn.extra_dejson.get('schema', 'http')\n        )\n        self.log.info('Get the connection to druid broker on %s', conn.host)\n        return druid_broker_conn",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "druid_broker_conn_id",
        ")",
        "druid_broker_conn",
        "=",
        "connect",
        "(",
        "host",
        "=",
        "conn",
        ".",
        "host",
        ",",
        "port",
        "=",
        "conn",
        ".",
        "port",
        ",",
        "path",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'endpoint'",
        ",",
        "'/druid/v2/sql'",
        ")",
        ",",
        "scheme",
        "=",
        "conn",
        ".",
        "extra_dejson",
        ".",
        "get",
        "(",
        "'schema'",
        ",",
        "'http'",
        ")",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "'Get the connection to druid broker on %s'",
        ",",
        "conn",
        ".",
        "host",
        ")",
        "return",
        "druid_broker_conn"
      ],
      "docstring": "Establish a connection to druid broker.",
      "docstring_tokens": [
        "Establish",
        "a",
        "connection",
        "to",
        "druid",
        "broker",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/druid_hook.py#L127-L139"
    },
    "target": "Establish a connection to druid broker .",
    "arguments": [
      [
        "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\n\nYou should replace the `conn` with the actual connection object from the Airflow database.\n\n### Connect to Druid Broker\n\n```python\nfrom airflow.providers.apache.druid.hooks.druid import DruidBroker\nfrom airflow.models import Connection\n\ndef get_drb_conn ( self ) :\n    druid_broker_conn = DruidBroker (\n        connection = self. conn\n    )\n    return druid_broker_conn\n```\n\nYou should replace the `self_CONN` with the actual Airflow connection object.\n\n### Using the Connection Object\n\nTo use the custom hook method, you need to get the `Airflow connection object` using the `Airflow API`. You can get the connection object using the following command in Airflow:\n\n```bash\nairflow connections get conn_id druid_broker_conn_id\n```\n\nYou can replace `druid_broker_conn_id` with the actual Airflow connection ID.\n\nThen, you can use the `get_conn` method like this:\n\n```python\nfrom custom_hooks import DruidBrokerHook\n\ndef example_job ( self ):\n    druid_broker_hook = DruidBrokerHook (\n        druid_broker_conn_id = 'druid_broker_conn_id'\n    )\n    conn = druid"
      ]
    ],
    "filtered_resps": [
      " ```\n\nYou should replace the `conn` with the actual connection object from the Airflow database.\n\n### Connect to Druid Broker\n\n```python\nfrom airflow.providers.apache.druid.hooks.druid import DruidBroker\nfrom airflow.models import Connection\n\ndef get_drb_conn ( self ) :\n    druid_broker_conn = DruidBroker (\n        connection = self. conn\n    )\n    return druid_broker_conn\n```\n\nYou should replace the `self_CONN` with the actual Airflow connection object.\n\n### Using the Connection Object\n\nTo use the custom hook method, you need to get the `Airflow connection object` using the `Airflow API`. You can get the connection object using the following command in Airflow:\n\n```bash\nairflow connections get conn_id druid_broker_conn_id\n```\n\nYou can replace `druid_broker_conn_id` with the actual Airflow connection ID.\n\nThen, you can use the `get_conn` method like this:\n\n```python\nfrom custom_hooks import DruidBrokerHook\n\ndef example_job ( self ):\n    druid_broker_hook = DruidBrokerHook (\n        druid_broker_conn_id = 'druid_broker_conn_id'\n    )\n    conn = druid"
    ],
    "smoothed_bleu_4": 0.9581825259507295
  },
  {
    "doc_id": 73,
    "doc": {
      "id": 73,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.get_conn",
      "original_string": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session",
      "language": "python",
      "code": "def get_conn(self, headers=None):\n        \"\"\"\n        Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        \"\"\"\n        session = requests.Session()\n        if self.http_conn_id:\n            conn = self.get_connection(self.http_conn_id)\n\n            if \"://\" in conn.host:\n                self.base_url = conn.host\n            else:\n                # schema defaults to HTTP\n                schema = conn.schema if conn.schema else \"http\"\n                self.base_url = schema + \"://\" + conn.host\n\n            if conn.port:\n                self.base_url = self.base_url + \":\" + str(conn.port)\n            if conn.login:\n                session.auth = (conn.login, conn.password)\n            if conn.extra:\n                try:\n                    session.headers.update(conn.extra_dejson)\n                except TypeError:\n                    self.log.warn('Connection to %s has invalid extra field.', conn.host)\n        if headers:\n            session.headers.update(headers)\n\n        return session",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ",",
        "headers",
        "=",
        "None",
        ")",
        ":",
        "session",
        "=",
        "requests",
        ".",
        "Session",
        "(",
        ")",
        "if",
        "self",
        ".",
        "http_conn_id",
        ":",
        "conn",
        "=",
        "self",
        ".",
        "get_connection",
        "(",
        "self",
        ".",
        "http_conn_id",
        ")",
        "if",
        "\"://\"",
        "in",
        "conn",
        ".",
        "host",
        ":",
        "self",
        ".",
        "base_url",
        "=",
        "conn",
        ".",
        "host",
        "else",
        ":",
        "# schema defaults to HTTP",
        "schema",
        "=",
        "conn",
        ".",
        "schema",
        "if",
        "conn",
        ".",
        "schema",
        "else",
        "\"http\"",
        "self",
        ".",
        "base_url",
        "=",
        "schema",
        "+",
        "\"://\"",
        "+",
        "conn",
        ".",
        "host",
        "if",
        "conn",
        ".",
        "port",
        ":",
        "self",
        ".",
        "base_url",
        "=",
        "self",
        ".",
        "base_url",
        "+",
        "\":\"",
        "+",
        "str",
        "(",
        "conn",
        ".",
        "port",
        ")",
        "if",
        "conn",
        ".",
        "login",
        ":",
        "session",
        ".",
        "auth",
        "=",
        "(",
        "conn",
        ".",
        "login",
        ",",
        "conn",
        ".",
        "password",
        ")",
        "if",
        "conn",
        ".",
        "extra",
        ":",
        "try",
        ":",
        "session",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "conn",
        ".",
        "extra_dejson",
        ")",
        "except",
        "TypeError",
        ":",
        "self",
        ".",
        "log",
        ".",
        "warn",
        "(",
        "'Connection to %s has invalid extra field.'",
        ",",
        "conn",
        ".",
        "host",
        ")",
        "if",
        "headers",
        ":",
        "session",
        ".",
        "headers",
        ".",
        "update",
        "(",
        "headers",
        ")",
        "return",
        "session"
      ],
      "docstring": "Returns http session for use with requests\n\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict",
      "docstring_tokens": [
        "Returns",
        "http",
        "session",
        "for",
        "use",
        "with",
        "requests"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L53-L83"
    },
    "target": "Returns http session for use with requests",
    "arguments": [
      [
        "def get_conn ( self , headers = None ) : session = requests . Session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if \"://\" in conn . host : self . base_url = conn . host else : # schema defaults to HTTP schema = conn . schema if conn . schema else \"http\" self . base_url = schema + \"://\" + conn . host if conn . port : self . base_url = self . base_url + \":\" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " else : return None  # If the connection does not exist, raise an exception return self. get_connection ( self. http_conn_id ) [ \"uri\" ]  # returns the uri without any query parameters\nconn = self. get_connection ( self. http_conn_id ) if conn : return conn. extra_dejson. get ( \"username\" ) else : return None  # If the connection does not exist, raise an exception\ndef close ( self, conn_id = None, key = None ) :  if not conn :\nconn = self. get_connection ( conn_id )  return\nconn. close ()\nself. pool. connections. remove_conn ( conn )\ndef execute ( self, query, parameters, conn_id ) :  cursor = conn. cursor ()\ntry :\ntry :\nself. log. debug ( \"'%s' executing query '%s'\",  cursor. execute ( query, parameters )\nconn_id, query )  if isinstance ( query, basestring ):\ncursor. close ()\nresult = {'records': 0}\nresult = cursor. fetchall ()\nself. log. debug ( ( \"'%s' fetched %s rows\",  result = {'records': cursor.rowcount, 'rows': result}\n"
      ]
    ],
    "filtered_resps": [
      " else : return None  # If the connection does not exist, raise an exception return self. get_connection ( self. http_conn_id ) [ \"uri\" ]  # returns the uri without any query parameters\nconn = self. get_connection ( self. http_conn_id ) if conn : return conn. extra_dejson. get ( \"username\" ) else : return None  # If the connection does not exist, raise an exception\ndef close ( self, conn_id = None, key = None ) :  if not conn :\nconn = self. get_connection ( conn_id )  return\nconn. close ()\nself. pool. connections. remove_conn ( conn )\ndef execute ( self, query, parameters, conn_id ) :  cursor = conn. cursor ()\ntry :\ntry :\nself. log. debug ( \"'%s' executing query '%s'\",  cursor. execute ( query, parameters )\nconn_id, query )  if isinstance ( query, basestring ):\ncursor. close ()\nresult = {'records': 0}\nresult = cursor. fetchall ()\nself. log. debug ( ( \"'%s' fetched %s rows\",  result = {'records': cursor.rowcount, 'rows': result}\n"
    ],
    "smoothed_bleu_4": 0.45520119072873444
  },
  {
    "doc_id": 74,
    "doc": {
      "id": 74,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.run",
      "original_string": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)",
      "language": "python",
      "code": "def run(self, endpoint, data=None, headers=None, extra_options=None):\n        \"\"\"\n        Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        session = self.get_conn(headers)\n\n        if self.base_url and not self.base_url.endswith('/') and \\\n           endpoint and not endpoint.startswith('/'):\n            url = self.base_url + '/' + endpoint\n        else:\n            url = (self.base_url or '') + (endpoint or '')\n\n        req = None\n        if self.method == 'GET':\n            # GET uses params\n            req = requests.Request(self.method,\n                                   url,\n                                   params=data,\n                                   headers=headers)\n        elif self.method == 'HEAD':\n            # HEAD doesn't use params\n            req = requests.Request(self.method,\n                                   url,\n                                   headers=headers)\n        else:\n            # Others use data\n            req = requests.Request(self.method,\n                                   url,\n                                   data=data,\n                                   headers=headers)\n\n        prepped_request = session.prepare_request(req)\n        self.log.info(\"Sending '%s' to url: %s\", self.method, url)\n        return self.run_and_check(session, prepped_request, extra_options)",
      "code_tokens": [
        "def",
        "run",
        "(",
        "self",
        ",",
        "endpoint",
        ",",
        "data",
        "=",
        "None",
        ",",
        "headers",
        "=",
        "None",
        ",",
        "extra_options",
        "=",
        "None",
        ")",
        ":",
        "extra_options",
        "=",
        "extra_options",
        "or",
        "{",
        "}",
        "session",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        "headers",
        ")",
        "if",
        "self",
        ".",
        "base_url",
        "and",
        "not",
        "self",
        ".",
        "base_url",
        ".",
        "endswith",
        "(",
        "'/'",
        ")",
        "and",
        "endpoint",
        "and",
        "not",
        "endpoint",
        ".",
        "startswith",
        "(",
        "'/'",
        ")",
        ":",
        "url",
        "=",
        "self",
        ".",
        "base_url",
        "+",
        "'/'",
        "+",
        "endpoint",
        "else",
        ":",
        "url",
        "=",
        "(",
        "self",
        ".",
        "base_url",
        "or",
        "''",
        ")",
        "+",
        "(",
        "endpoint",
        "or",
        "''",
        ")",
        "req",
        "=",
        "None",
        "if",
        "self",
        ".",
        "method",
        "==",
        "'GET'",
        ":",
        "# GET uses params",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "params",
        "=",
        "data",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "elif",
        "self",
        ".",
        "method",
        "==",
        "'HEAD'",
        ":",
        "# HEAD doesn't use params",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "else",
        ":",
        "# Others use data",
        "req",
        "=",
        "requests",
        ".",
        "Request",
        "(",
        "self",
        ".",
        "method",
        ",",
        "url",
        ",",
        "data",
        "=",
        "data",
        ",",
        "headers",
        "=",
        "headers",
        ")",
        "prepped_request",
        "=",
        "session",
        ".",
        "prepare_request",
        "(",
        "req",
        ")",
        "self",
        ".",
        "log",
        ".",
        "info",
        "(",
        "\"Sending '%s' to url: %s\"",
        ",",
        "self",
        ".",
        "method",
        ",",
        "url",
        ")",
        "return",
        "self",
        ".",
        "run_and_check",
        "(",
        "session",
        ",",
        "prepped_request",
        ",",
        "extra_options",
        ")"
      ],
      "docstring": "Performs the request\n\n        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n        :type endpoint: str\n        :param data: payload to be uploaded or request parameters\n        :type data: dict\n        :param headers: additional headers to be passed through as a dictionary\n        :type headers: dict\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non\n            2XX or 3XX status codes\n        :type extra_options: dict",
      "docstring_tokens": [
        "Performs",
        "the",
        "request"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L85-L131"
    },
    "target": "Performs the request",
    "arguments": [
      [
        "def run ( self , endpoint , data = None , headers = None , extra_options = None ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = None if self . method == 'GET' : # GET uses params req = requests . Request ( self . method , url , params = data , headers = headers ) elif self . method == 'HEAD' : # HEAD doesn't use params req = requests . Request ( self . method , url , headers = headers ) else : # Others use data req = requests . Request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( \"Sending '%s' to url: %s\" , self . method , url ) return self . run_and_check ( session , prepped_request , extra_options )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_conn ( self, headers = None ) : \"Creates a connection/session using the global http session if None is returned.\" global _default_session if headers : # If there is a headers parameter, make a new session connection session = requests. Session ( ) self. log. info ( \"Sending '%s' without any session, just a request object with %s headers\", self. endpoint, headers ) req = requests. Request ( self. method, self. endpoint, headers = headers ) return req else : # If there's no headers parameter, return a connection to the global requests.Session return _default_session def run_and_check ( self, session, prepped_request, extra_options = None ) : extra_options = extra_options or { } # Create a request object from the 'Request' object prepped_request = session. send ( prepped_request, ** extra_options ) try : if prepped_request. encoding == requests. encoding_types. CHUNKED ( ) : response = prepped_request. raw response = prepped_request. response self. log. info ( \"Finished request\" ) return response except Exception as e : self. log. critical ( \"Sending a %s request to '%s' raised an"
      ]
    ],
    "filtered_resps": [
      " def get_conn ( self, headers = None ) : \"Creates a connection/session using the global http session if None is returned.\" global _default_session if headers : # If there is a headers parameter, make a new session connection session = requests. Session ( ) self. log. info ( \"Sending '%s' without any session, just a request object with %s headers\", self. endpoint, headers ) req = requests. Request ( self. method, self. endpoint, headers = headers ) return req else : # If there's no headers parameter, return a connection to the global requests.Session return _default_session def run_and_check ( self, session, prepped_request, extra_options = None ) : extra_options = extra_options or { } # Create a request object from the 'Request' object prepped_request = session. send ( prepped_request, ** extra_options ) try : if prepped_request. encoding == requests. encoding_types. CHUNKED ( ) : response = prepped_request. raw response = prepped_request. response self. log. info ( \"Finished request\" ) return response except Exception as e : self. log. critical ( \"Sending a %s request to '%s' raised an"
    ],
    "smoothed_bleu_4": 0.44833663759284287
  },
  {
    "doc_id": 75,
    "doc": {
      "id": 75,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.check_response",
      "original_string": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)",
      "language": "python",
      "code": "def check_response(self, response):\n        \"\"\"\n        Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError:\n            self.log.error(\"HTTP error: %s\", response.reason)\n            if self.method not in ['GET', 'HEAD']:\n                self.log.error(response.text)\n            raise AirflowException(str(response.status_code) + \":\" + response.reason)",
      "code_tokens": [
        "def",
        "check_response",
        "(",
        "self",
        ",",
        "response",
        ")",
        ":",
        "try",
        ":",
        "response",
        ".",
        "raise_for_status",
        "(",
        ")",
        "except",
        "requests",
        ".",
        "exceptions",
        ".",
        "HTTPError",
        ":",
        "self",
        ".",
        "log",
        ".",
        "error",
        "(",
        "\"HTTP error: %s\"",
        ",",
        "response",
        ".",
        "reason",
        ")",
        "if",
        "self",
        ".",
        "method",
        "not",
        "in",
        "[",
        "'GET'",
        ",",
        "'HEAD'",
        "]",
        ":",
        "self",
        ".",
        "log",
        ".",
        "error",
        "(",
        "response",
        ".",
        "text",
        ")",
        "raise",
        "AirflowException",
        "(",
        "str",
        "(",
        "response",
        ".",
        "status_code",
        ")",
        "+",
        "\":\"",
        "+",
        "response",
        ".",
        "reason",
        ")"
      ],
      "docstring": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n        status codes\n\n        :param response: A requests response object\n        :type response: requests.response",
      "docstring_tokens": [
        "Checks",
        "the",
        "status",
        "code",
        "and",
        "raise",
        "an",
        "AirflowException",
        "exception",
        "on",
        "non",
        "2XX",
        "or",
        "3XX",
        "status",
        "codes"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L133-L147"
    },
    "target": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes",
    "arguments": [
      [
        "def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . HTTPError : self . log . error ( \"HTTP error: %s\" , response . reason ) if self . method not in [ 'GET' , 'HEAD' ] : self . log . error ( response . text ) raise AirflowException ( str ( response . status_code ) + \":\" + response . reason )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def request ( self, method, url, ** kwargs ) : headers = kwargs. get ( 'headers', { } ) headers. setdefault ( 'Accept', \"application/json\" ) auth = kwargs. get ( 'auth', ( None, None ) ) proxies = kwargs. get ( 'proxies', None ) kwargs [ 'timeout' ] = 1.5 try : response = requests. request ( method, url, headers = headers, auth = auth, proxies = proxies, ** kwargs ) except requests. exceptions. ProxyError as e : self. log. error ( \"Proxy error: %s\", str ( e ) ) raise except requests. exceptions. Timeout as e : self. log. error ( \"Timeout: %s\", str ( e ) ) raise except requests. exceptions. RequestException as e : self. log. error ( \"Request failure ( {} ): {} \". format ( str ( e. request ) ), str ( e ) ) raise return response class CustomAPISession ( requests. Session ) : def __init__ ( self, method, api_root_url ) : super ( CustomAPISession, self ). __init__ ( ) self."
      ]
    ],
    "filtered_resps": [
      " def request ( self, method, url, ** kwargs ) : headers = kwargs. get ( 'headers', { } ) headers. setdefault ( 'Accept', \"application/json\" ) auth = kwargs. get ( 'auth', ( None, None ) ) proxies = kwargs. get ( 'proxies', None ) kwargs [ 'timeout' ] = 1.5 try : response = requests. request ( method, url, headers = headers, auth = auth, proxies = proxies, ** kwargs ) except requests. exceptions. ProxyError as e : self. log. error ( \"Proxy error: %s\", str ( e ) ) raise except requests. exceptions. Timeout as e : self. log. error ( \"Timeout: %s\", str ( e ) ) raise except requests. exceptions. RequestException as e : self. log. error ( \"Request failure ( {} ): {} \". format ( str ( e. request ) ), str ( e ) ) raise return response class CustomAPISession ( requests. Session ) : def __init__ ( self, method, api_root_url ) : super ( CustomAPISession, self ). __init__ ( ) self."
    ],
    "smoothed_bleu_4": 0.39960259542968934
  },
  {
    "doc_id": 76,
    "doc": {
      "id": 76,
      "repo": "apache/airflow",
      "path": "airflow/hooks/http_hook.py",
      "func_name": "HttpHook.run_and_check",
      "original_string": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex",
      "language": "python",
      "code": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex",
      "code_tokens": [
        "def",
        "run_and_check",
        "(",
        "self",
        ",",
        "session",
        ",",
        "prepped_request",
        ",",
        "extra_options",
        ")",
        ":",
        "extra_options",
        "=",
        "extra_options",
        "or",
        "{",
        "}",
        "try",
        ":",
        "response",
        "=",
        "session",
        ".",
        "send",
        "(",
        "prepped_request",
        ",",
        "stream",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"stream\"",
        ",",
        "False",
        ")",
        ",",
        "verify",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"verify\"",
        ",",
        "True",
        ")",
        ",",
        "proxies",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"proxies\"",
        ",",
        "{",
        "}",
        ")",
        ",",
        "cert",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"cert\"",
        ")",
        ",",
        "timeout",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"timeout\"",
        ")",
        ",",
        "allow_redirects",
        "=",
        "extra_options",
        ".",
        "get",
        "(",
        "\"allow_redirects\"",
        ",",
        "True",
        ")",
        ")",
        "if",
        "extra_options",
        ".",
        "get",
        "(",
        "'check_response'",
        ",",
        "True",
        ")",
        ":",
        "self",
        ".",
        "check_response",
        "(",
        "response",
        ")",
        "return",
        "response",
        "except",
        "requests",
        ".",
        "exceptions",
        ".",
        "ConnectionError",
        "as",
        "ex",
        ":",
        "self",
        ".",
        "log",
        ".",
        "warn",
        "(",
        "str",
        "(",
        "ex",
        ")",
        "+",
        "' Tenacity will retry to execute the operation'",
        ")",
        "raise",
        "ex"
      ],
      "docstring": "Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict",
      "docstring_tokens": [
        "Grabs",
        "extra",
        "options",
        "like",
        "timeout",
        "and",
        "actually",
        "runs",
        "the",
        "request",
        "checking",
        "for",
        "the",
        "result"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181"
    },
    "target": "Grabs extra options like timeout and actually runs the request checking for the result",
    "arguments": [
      [
        "def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( \"stream\" , False ) , verify = extra_options . get ( \"verify\" , True ) , proxies = extra_options . get ( \"proxies\" , { } ) , cert = extra_options . get ( \"cert\" ) , timeout = extra_options . get ( \"timeout\" ) , allow_redirects = extra_options . get ( \"allow_redirects\" , True ) ) if extra_options . get ( 'check_response' , True ) : self . check_response ( response ) return response except requests . exceptions . ConnectionError as ex : self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) raise ex",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " from requests. exceptions import SSLError, Timeout, ConnectionError def _set_user_agent ( self, url ) : return { 'User-Agent' : 'Apache-HttpClient', 'Accept' : 'text/html,application/json' } class Session ( object ) : def __init__ ( self, session ) : self. session = session def send ( self, request, *, stream = None, timeout = None, verify = None ) : return self. session. send ( request, stream, timeout, verify ) def requests_get ( __url__ ) : headers = requests. Session ( ). _set_user_agent ( __url__ ) response = requests. Session ( ). send ( requests. PreppedRequest ( 'GET', __url__ ), headers = headers ) return response text = requests_get ( 'https://localhost:8443/sap/bc/https/srvc' ) assert text is not None # py3k: assert isinstance(text, six.text_type) assert text.isspace() ```\n    1. This test suite only runs the test_request method.\n4. The test_request method only calls run_and_check method, which sends a request and checks for response.\n5. The check_response"
      ]
    ],
    "filtered_resps": [
      " from requests. exceptions import SSLError, Timeout, ConnectionError def _set_user_agent ( self, url ) : return { 'User-Agent' : 'Apache-HttpClient', 'Accept' : 'text/html,application/json' } class Session ( object ) : def __init__ ( self, session ) : self. session = session def send ( self, request, *, stream = None, timeout = None, verify = None ) : return self. session. send ( request, stream, timeout, verify ) def requests_get ( __url__ ) : headers = requests. Session ( ). _set_user_agent ( __url__ ) response = requests. Session ( ). send ( requests. PreppedRequest ( 'GET', __url__ ), headers = headers ) return response text = requests_get ( 'https://localhost:8443/sap/bc/https/srvc' ) assert text is not None # py3k: assert isinstance(text, six.text_type) assert text.isspace() ```\n    1. This test suite only runs the test_request method.\n4. The test_request method only calls run_and_check method, which sends a request and checks for response.\n5. The check_response"
    ],
    "smoothed_bleu_4": 0.7157615296128191
  },
  {
    "doc_id": 77,
    "doc": {
      "id": 77,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "create_session",
      "original_string": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
      "language": "python",
      "code": "def create_session():\n    \"\"\"\n    Contextmanager that will create and teardown a session.\n    \"\"\"\n    session = settings.Session()\n    try:\n        yield session\n        session.commit()\n    except Exception:\n        session.rollback()\n        raise\n    finally:\n        session.close()",
      "code_tokens": [
        "def",
        "create_session",
        "(",
        ")",
        ":",
        "session",
        "=",
        "settings",
        ".",
        "Session",
        "(",
        ")",
        "try",
        ":",
        "yield",
        "session",
        "session",
        ".",
        "commit",
        "(",
        ")",
        "except",
        "Exception",
        ":",
        "session",
        ".",
        "rollback",
        "(",
        ")",
        "raise",
        "finally",
        ":",
        "session",
        ".",
        "close",
        "(",
        ")"
      ],
      "docstring": "Contextmanager that will create and teardown a session.",
      "docstring_tokens": [
        "Contextmanager",
        "that",
        "will",
        "create",
        "and",
        "teardown",
        "a",
        "session",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L32-L44"
    },
    "target": "Contextmanager that will create and teardown a session .",
    "arguments": [
      [
        "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` You can then use it as the context manager for the `app.db` object: ``` def my_function ( ) : with create_session ( ) as session : # do some database operation session. add ( obj ) app. db. session = session ```\n  - I have a class with several static methods that operate on the class instance data. Is it a good practice to make these static methods as instance methods? Why? It depends. ``` class MyClass ( ) : def __init__ ( self ) : self. instance_data = [ ] def instance_method ( self ) : #... def static_method ( ) : #... ``` Making a static method a regular instance method would mean you no longer need to pass the instance. However, this is not always useful. But if you have a class with state that is used in the static method, then this might be useful. Here\u2019s an example: ``` class MyClass ( ) : def __init__ ( self, data ) : self. data = data def instance_method ( self ) : #... def static_method ( ) : # this method would now be able to access self. data ``` In this case, making it an instance method allows you to avoid having to pass the data to"
      ]
    ],
    "filtered_resps": [
      " ``` You can then use it as the context manager for the `app.db` object: ``` def my_function ( ) : with create_session ( ) as session : # do some database operation session. add ( obj ) app. db. session = session ```\n  - I have a class with several static methods that operate on the class instance data. Is it a good practice to make these static methods as instance methods? Why? It depends. ``` class MyClass ( ) : def __init__ ( self ) : self. instance_data = [ ] def instance_method ( self ) : #... def static_method ( ) : #... ``` Making a static method a regular instance method would mean you no longer need to pass the instance. However, this is not always useful. But if you have a class with state that is used in the static method, then this might be useful. Here\u2019s an example: ``` class MyClass ( ) : def __init__ ( self, data ) : self. data = data def instance_method ( self ) : #... def static_method ( ) : # this method would now be able to access self. data ``` In this case, making it an instance method allows you to avoid having to pass the data to"
    ],
    "smoothed_bleu_4": 0.6322798739327005
  },
  {
    "doc_id": 78,
    "doc": {
      "id": 78,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "provide_session",
      "original_string": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper",
      "language": "python",
      "code": "def provide_session(func):\n    \"\"\"\n    Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arg_session = 'session'\n\n        func_params = func.__code__.co_varnames\n        session_in_args = arg_session in func_params and \\\n            func_params.index(arg_session) < len(args)\n        session_in_kwargs = arg_session in kwargs\n\n        if session_in_kwargs or session_in_args:\n            return func(*args, **kwargs)\n        else:\n            with create_session() as session:\n                kwargs[arg_session] = session\n                return func(*args, **kwargs)\n\n    return wrapper",
      "code_tokens": [
        "def",
        "provide_session",
        "(",
        "func",
        ")",
        ":",
        "@",
        "wraps",
        "(",
        "func",
        ")",
        "def",
        "wrapper",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        ":",
        "arg_session",
        "=",
        "'session'",
        "func_params",
        "=",
        "func",
        ".",
        "__code__",
        ".",
        "co_varnames",
        "session_in_args",
        "=",
        "arg_session",
        "in",
        "func_params",
        "and",
        "func_params",
        ".",
        "index",
        "(",
        "arg_session",
        ")",
        "<",
        "len",
        "(",
        "args",
        ")",
        "session_in_kwargs",
        "=",
        "arg_session",
        "in",
        "kwargs",
        "if",
        "session_in_kwargs",
        "or",
        "session_in_args",
        ":",
        "return",
        "func",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "else",
        ":",
        "with",
        "create_session",
        "(",
        ")",
        "as",
        "session",
        ":",
        "kwargs",
        "[",
        "arg_session",
        "]",
        "=",
        "session",
        "return",
        "func",
        "(",
        "*",
        "args",
        ",",
        "*",
        "*",
        "kwargs",
        ")",
        "return",
        "wrapper"
      ],
      "docstring": "Function decorator that provides a session if it isn't provided.\n    If you want to reuse a session or run the function as part of a\n    database transaction, you pass it to the function, if not this wrapper\n    will create one and close it for you.",
      "docstring_tokens": [
        "Function",
        "decorator",
        "that",
        "provides",
        "a",
        "session",
        "if",
        "it",
        "isn",
        "t",
        "provided",
        ".",
        "If",
        "you",
        "want",
        "to",
        "reuse",
        "a",
        "session",
        "or",
        "run",
        "the",
        "function",
        "as",
        "part",
        "of",
        "a",
        "database",
        "transaction",
        "you",
        "pass",
        "it",
        "to",
        "the",
        "function",
        "if",
        "not",
        "this",
        "wrapper",
        "will",
        "create",
        "one",
        "and",
        "close",
        "it",
        "for",
        "you",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L47-L70"
    },
    "target": "Function decorator that provides a session if it isn t provided . If you want to reuse a session or run the function as part of a database transaction you pass it to the function if not this wrapper will create one and close it for you .",
    "arguments": [
      [
        "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , * * kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , * * kwargs ) return wrapper",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\n\nThis documentation is generated from the code sample and provides a detailed explanation of the `provide_session` decorator's functionality.\n\n**Notes**\n\n* The `create_session` function is not defined in the code sample and is assumed to be a function that creates a session object.\n* The `session_in_args` and `session_in_kwargs` variables are used to check if the `session` argument is present in the function's arguments or keyword arguments, respectively.\n* The `wrapper` function calls the wrapped function with the created session object as an argument if `session` is not already present in the arguments or keyword arguments.\n* The `wrapper` function returns the result of the wrapped function.\n* If `session` is not present in the arguments or keyword arguments and the `create_session` function fails to create a session object, the code will raise an exception.  # Add a docstring to the create_session() function\n```python\ndef create_session ( ) :\n    \"\"\"\n    Creates a new session object.\n    \"\"\"\n    # Code to create a session object goes here\n    session = {}  # Simple example: return an empty dictionary\n    return session\n```\nThis documentation is generated from the code sample and provides a"
      ]
    ],
    "filtered_resps": [
      "\n```\n\nThis documentation is generated from the code sample and provides a detailed explanation of the `provide_session` decorator's functionality.\n\n**Notes**\n\n* The `create_session` function is not defined in the code sample and is assumed to be a function that creates a session object.\n* The `session_in_args` and `session_in_kwargs` variables are used to check if the `session` argument is present in the function's arguments or keyword arguments, respectively.\n* The `wrapper` function calls the wrapped function with the created session object as an argument if `session` is not already present in the arguments or keyword arguments.\n* The `wrapper` function returns the result of the wrapped function.\n* If `session` is not present in the arguments or keyword arguments and the `create_session` function fails to create a session object, the code will raise an exception.  # Add a docstring to the create_session() function\n```python\ndef create_session ( ) :\n    \"\"\"\n    Creates a new session object.\n    \"\"\"\n    # Code to create a session object goes here\n    session = {}  # Simple example: return an empty dictionary\n    return session\n```\nThis documentation is generated from the code sample and provides a"
    ],
    "smoothed_bleu_4": 1.4118357895949034
  },
  {
    "doc_id": 79,
    "doc": {
      "id": 79,
      "repo": "apache/airflow",
      "path": "airflow/utils/db.py",
      "func_name": "resetdb",
      "original_string": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
      "language": "python",
      "code": "def resetdb():\n    \"\"\"\n    Clear out the database\n    \"\"\"\n    from airflow import models\n\n    # alembic adds significant import time, so we import it lazily\n    from alembic.migration import MigrationContext\n\n    log.info(\"Dropping tables that exist\")\n\n    models.base.Base.metadata.drop_all(settings.engine)\n    mc = MigrationContext.configure(settings.engine)\n    if mc._version.exists(settings.engine):\n        mc._version.drop(settings.engine)\n\n    from flask_appbuilder.models.sqla import Base\n    Base.metadata.drop_all(settings.engine)\n\n    initdb()",
      "code_tokens": [
        "def",
        "resetdb",
        "(",
        ")",
        ":",
        "from",
        "airflow",
        "import",
        "models",
        "# alembic adds significant import time, so we import it lazily",
        "from",
        "alembic",
        ".",
        "migration",
        "import",
        "MigrationContext",
        "log",
        ".",
        "info",
        "(",
        "\"Dropping tables that exist\"",
        ")",
        "models",
        ".",
        "base",
        ".",
        "Base",
        ".",
        "metadata",
        ".",
        "drop_all",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "mc",
        "=",
        "MigrationContext",
        ".",
        "configure",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "if",
        "mc",
        ".",
        "_version",
        ".",
        "exists",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        ":",
        "mc",
        ".",
        "_version",
        ".",
        "drop",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "from",
        "flask_appbuilder",
        ".",
        "models",
        ".",
        "sqla",
        "import",
        "Base",
        "Base",
        ".",
        "metadata",
        ".",
        "drop_all",
        "(",
        "settings",
        ".",
        "engine",
        ")",
        "initdb",
        "(",
        ")"
      ],
      "docstring": "Clear out the database",
      "docstring_tokens": [
        "Clear",
        "out",
        "the",
        "database"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L312-L331"
    },
    "target": "Clear out the database",
    "arguments": [
      [
        "def resetdb ( ) : from airflow import models # alembic adds significant import time, so we import it lazily from alembic . migration import MigrationContext log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def initdb ( ) : from airflow import models from flask_appbuilder. models. sqla import Base from alembic. migration import MigrationContext from sqlalchemy. exc import IntegrityError log. info ( \"Creating tables that exist\" ) Base. metadata. create_all ( settings. engine ) mc = MigrationContext. configure ( settings. engine ) mc. _version. create ( settings. engine ) models. base. Base. metadata. create_all ( settings. engine ) # create all databases table for all database log. info ( \"Initializing db\" ) from airflow. models import db session = db. Session ( db. get_url ( ) [ 'echo' ] ) init_tabledefs ( session ) init_permisos ( session ) def init_tabledefs ( session ) : # Initialize Table definitions log. info ( \"Initializing tabledefs\" ) def init_permisos ( session ) : # Initialize permisos # TODO: Fixme : This function is not implemented, and it looks like it does some kind of permissions setup. It should probably be replaced with something that actually sets up permissions instead of just ignoring this. # For now, just log a warning that it is being skipped log. warn ( \"Permissions setup is not implemented, skipping"
      ]
    ],
    "filtered_resps": [
      " def initdb ( ) : from airflow import models from flask_appbuilder. models. sqla import Base from alembic. migration import MigrationContext from sqlalchemy. exc import IntegrityError log. info ( \"Creating tables that exist\" ) Base. metadata. create_all ( settings. engine ) mc = MigrationContext. configure ( settings. engine ) mc. _version. create ( settings. engine ) models. base. Base. metadata. create_all ( settings. engine ) # create all databases table for all database log. info ( \"Initializing db\" ) from airflow. models import db session = db. Session ( db. get_url ( ) [ 'echo' ] ) init_tabledefs ( session ) init_permisos ( session ) def init_tabledefs ( session ) : # Initialize Table definitions log. info ( \"Initializing tabledefs\" ) def init_permisos ( session ) : # Initialize permisos # TODO: Fixme : This function is not implemented, and it looks like it does some kind of permissions setup. It should probably be replaced with something that actually sets up permissions instead of just ignoring this. # For now, just log a warning that it is being skipped log. warn ( \"Permissions setup is not implemented, skipping"
    ],
    "smoothed_bleu_4": 0.4127991183512739
  },
  {
    "doc_id": 80,
    "doc": {
      "id": 80,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook._get_pretty_exception_message",
      "original_string": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)",
      "language": "python",
      "code": "def _get_pretty_exception_message(e):\n        \"\"\"\n        Parses some DatabaseError to provide a better error message\n        \"\"\"\n        if (hasattr(e, 'message') and\n            'errorName' in e.message and\n                'message' in e.message):\n            return ('{name}: {message}'.format(\n                    name=e.message['errorName'],\n                    message=e.message['message']))\n        else:\n            return str(e)",
      "code_tokens": [
        "def",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ":",
        "if",
        "(",
        "hasattr",
        "(",
        "e",
        ",",
        "'message'",
        ")",
        "and",
        "'errorName'",
        "in",
        "e",
        ".",
        "message",
        "and",
        "'message'",
        "in",
        "e",
        ".",
        "message",
        ")",
        ":",
        "return",
        "(",
        "'{name}: {message}'",
        ".",
        "format",
        "(",
        "name",
        "=",
        "e",
        ".",
        "message",
        "[",
        "'errorName'",
        "]",
        ",",
        "message",
        "=",
        "e",
        ".",
        "message",
        "[",
        "'message'",
        "]",
        ")",
        ")",
        "else",
        ":",
        "return",
        "str",
        "(",
        "e",
        ")"
      ],
      "docstring": "Parses some DatabaseError to provide a better error message",
      "docstring_tokens": [
        "Parses",
        "some",
        "DatabaseError",
        "to",
        "provide",
        "a",
        "better",
        "error",
        "message"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L67-L78"
    },
    "target": "Parses some DatabaseError to provide a better error message",
    "arguments": [
      [
        "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " #... elif ( hasattr ( e, 'args' ) ) : #...... elif ( isinstance ( e, type ( RuntimeError ( 1, 1 ) ) ) and'message' not in vars ( e ) ) : #...... return ('{0} : {1} '. format ( e. args [ 0 ], e. args [ 1 ] ) ) elif ( isinstance ( e, type ( RuntimeError ( 1 ) ) ) ) : return ('{0} : {1} '. format ( e. args [ 0 ], 'No message' ) ) elif ( isinstance ( e, type ( RuntimeError ( ) ) ) ) : return ( str ( e ) ) elif ( isinstance ( e, ( type ( ValueError ),type ( TypeError ),type ( Exception ) ) ) ) : return ( str ( e ) ) else : # for other unknown exceptions return ( str ( e ). split ( ':' ) [ 0 ] )\n  - Input: e = <_ast. ImportFrom object at 0x7f...>\n  - Return: <_ast. ImportFrom object at 0x7f...>\n  - Return:\n    1"
      ]
    ],
    "filtered_resps": [
      " #... elif ( hasattr ( e, 'args' ) ) : #...... elif ( isinstance ( e, type ( RuntimeError ( 1, 1 ) ) ) and'message' not in vars ( e ) ) : #...... return ('{0} : {1} '. format ( e. args [ 0 ], e. args [ 1 ] ) ) elif ( isinstance ( e, type ( RuntimeError ( 1 ) ) ) ) : return ('{0} : {1} '. format ( e. args [ 0 ], 'No message' ) ) elif ( isinstance ( e, type ( RuntimeError ( ) ) ) ) : return ( str ( e ) ) elif ( isinstance ( e, ( type ( ValueError ),type ( TypeError ),type ( Exception ) ) ) ) : return ( str ( e ) ) else : # for other unknown exceptions return ( str ( e ). split ( ':' ) [ 0 ] )\n  - Input: e = <_ast. ImportFrom object at 0x7f...>\n  - Return: <_ast. ImportFrom object at 0x7f...>\n  - Return:\n    1"
    ],
    "smoothed_bleu_4": 0.4111020826565747
  },
  {
    "doc_id": 81,
    "doc": {
      "id": 81,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.get_records",
      "original_string": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
      "language": "python",
      "code": "def get_records(self, hql, parameters=None):\n        \"\"\"\n        Get a set of records from Presto\n        \"\"\"\n        try:\n            return super().get_records(\n                self._strip_sql(hql), parameters)\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))",
      "code_tokens": [
        "def",
        "get_records",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "try",
        ":",
        "return",
        "super",
        "(",
        ")",
        ".",
        "get_records",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")",
        "except",
        "DatabaseError",
        "as",
        "e",
        ":",
        "raise",
        "PrestoException",
        "(",
        "self",
        ".",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ")"
      ],
      "docstring": "Get a set of records from Presto",
      "docstring_tokens": [
        "Get",
        "a",
        "set",
        "of",
        "records",
        "from",
        "Presto"
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L80-L88"
    },
    "target": "Get a set of records from Presto",
    "arguments": [
      [
        "def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except Exception as e : raise PrestoException ( self. _get_pretty_exception_message ( e ) ) ``` Presto exceptions are mapped to a custom PrestoException class, while DatabaseError exceptions are re-raised with a pretty message, and other exceptions are also re-raised with a pretty message.\n  - ``` def _get_pretty_exception_message ( self, exception ) :... ``` This method returns a pretty error message for any exception. The message is constructed using the `__str__` method of the exception instance.\n\nHere are some key takeaways:\n\n1. Custom exception classes are used to wrap standard exceptions with additional functionality.\n2. The exception class can provide methods to extract relevant error information from the wrapped exception.\n3. This design is useful when you need to add custom error handling or logging logic to an existing codebase.\n\n### Step 3: Example Usage\n\nTo demonstrate how the custom `PrestoException` class can be used, let\u2019s assume you have a function that interacts with the Presto database using the `get_records` method:\n```python\ndef execute_query ( self, query ) : try : rows = self. get_records ( \"SELECT * FROM table WHERE condition\" ) except Presto"
      ]
    ],
    "filtered_resps": [
      " except Exception as e : raise PrestoException ( self. _get_pretty_exception_message ( e ) ) ``` Presto exceptions are mapped to a custom PrestoException class, while DatabaseError exceptions are re-raised with a pretty message, and other exceptions are also re-raised with a pretty message.\n  - ``` def _get_pretty_exception_message ( self, exception ) :... ``` This method returns a pretty error message for any exception. The message is constructed using the `__str__` method of the exception instance.\n\nHere are some key takeaways:\n\n1. Custom exception classes are used to wrap standard exceptions with additional functionality.\n2. The exception class can provide methods to extract relevant error information from the wrapped exception.\n3. This design is useful when you need to add custom error handling or logging logic to an existing codebase.\n\n### Step 3: Example Usage\n\nTo demonstrate how the custom `PrestoException` class can be used, let\u2019s assume you have a function that interacts with the Presto database using the `get_records` method:\n```python\ndef execute_query ( self, query ) : try : rows = self. get_records ( \"SELECT * FROM table WHERE condition\" ) except Presto"
    ],
    "smoothed_bleu_4": 0.6155724333085435
  },
  {
    "doc_id": 82,
    "doc": {
      "id": 82,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.get_pandas_df",
      "original_string": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df",
      "language": "python",
      "code": "def get_pandas_df(self, hql, parameters=None):\n        \"\"\"\n        Get a pandas dataframe from a sql query.\n        \"\"\"\n        import pandas\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(self._strip_sql(hql), parameters)\n            data = cursor.fetchall()\n        except DatabaseError as e:\n            raise PrestoException(self._get_pretty_exception_message(e))\n        column_descriptions = cursor.description\n        if data:\n            df = pandas.DataFrame(data)\n            df.columns = [c[0] for c in column_descriptions]\n        else:\n            df = pandas.DataFrame()\n        return df",
      "code_tokens": [
        "def",
        "get_pandas_df",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "import",
        "pandas",
        "cursor",
        "=",
        "self",
        ".",
        "get_cursor",
        "(",
        ")",
        "try",
        ":",
        "cursor",
        ".",
        "execute",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")",
        "data",
        "=",
        "cursor",
        ".",
        "fetchall",
        "(",
        ")",
        "except",
        "DatabaseError",
        "as",
        "e",
        ":",
        "raise",
        "PrestoException",
        "(",
        "self",
        ".",
        "_get_pretty_exception_message",
        "(",
        "e",
        ")",
        ")",
        "column_descriptions",
        "=",
        "cursor",
        ".",
        "description",
        "if",
        "data",
        ":",
        "df",
        "=",
        "pandas",
        ".",
        "DataFrame",
        "(",
        "data",
        ")",
        "df",
        ".",
        "columns",
        "=",
        "[",
        "c",
        "[",
        "0",
        "]",
        "for",
        "c",
        "in",
        "column_descriptions",
        "]",
        "else",
        ":",
        "df",
        "=",
        "pandas",
        ".",
        "DataFrame",
        "(",
        ")",
        "return",
        "df"
      ],
      "docstring": "Get a pandas dataframe from a sql query.",
      "docstring_tokens": [
        "Get",
        "a",
        "pandas",
        "dataframe",
        "from",
        "a",
        "sql",
        "query",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L101-L118"
    },
    "target": "Get a pandas dataframe from a sql query .",
    "arguments": [
      [
        "def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_cursor ( self ) : return self. _client. open_session ( ) def _handle_close ( self, close ) : if close : self. _client. _close_database ( ) def _is_close ( self ) : return self. _client. _is_database_closed ( ) def drop_table ( self, db_name, table_name, column_names = None, if_exists = 'fail' ) : \"\"\"Drop and database and a table from within the query engine... note:: To drop a table with existing constraints in Presto, you must use TRUNCATE instead of DROP. :Parameters: ``db_name`` : str - The name of the database schema name (database name). :type db_name: str :Parameters: ``table_name`` : str - The name of the table. :type table_name: str :Parameters: ``column_names`` : list/tuple of str - The names of the columns to drop. :type column_names: list/tuple of str :Parameters: ``if_exists`` : str - Action to take if the database and table does not exist: - \"fail\" - Raise an error (default) - \"drop\" - Drop the database and table (and all data"
      ]
    ],
    "filtered_resps": [
      " def get_cursor ( self ) : return self. _client. open_session ( ) def _handle_close ( self, close ) : if close : self. _client. _close_database ( ) def _is_close ( self ) : return self. _client. _is_database_closed ( ) def drop_table ( self, db_name, table_name, column_names = None, if_exists = 'fail' ) : \"\"\"Drop and database and a table from within the query engine... note:: To drop a table with existing constraints in Presto, you must use TRUNCATE instead of DROP. :Parameters: ``db_name`` : str - The name of the database schema name (database name). :type db_name: str :Parameters: ``table_name`` : str - The name of the table. :type table_name: str :Parameters: ``column_names`` : list/tuple of str - The names of the columns to drop. :type column_names: list/tuple of str :Parameters: ``if_exists`` : str - Action to take if the database and table does not exist: - \"fail\" - Raise an error (default) - \"drop\" - Drop the database and table (and all data"
    ],
    "smoothed_bleu_4": 0.5624767424099777
  },
  {
    "doc_id": 83,
    "doc": {
      "id": 83,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.run",
      "original_string": "def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)",
      "language": "python",
      "code": "def run(self, hql, parameters=None):\n        \"\"\"\n        Execute the statement against Presto. Can be used to create views.\n        \"\"\"\n        return super().run(self._strip_sql(hql), parameters)",
      "code_tokens": [
        "def",
        "run",
        "(",
        "self",
        ",",
        "hql",
        ",",
        "parameters",
        "=",
        "None",
        ")",
        ":",
        "return",
        "super",
        "(",
        ")",
        ".",
        "run",
        "(",
        "self",
        ".",
        "_strip_sql",
        "(",
        "hql",
        ")",
        ",",
        "parameters",
        ")"
      ],
      "docstring": "Execute the statement against Presto. Can be used to create views.",
      "docstring_tokens": [
        "Execute",
        "the",
        "statement",
        "against",
        "Presto",
        ".",
        "Can",
        "be",
        "used",
        "to",
        "create",
        "views",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L120-L124"
    },
    "target": "Execute the statement against Presto . Can be used to create views .",
    "arguments": [
      [
        "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_columns ( self ) : \"\"\"Return a list of column labels\"\"\" return [ name for name in self get_tables ( ) : \"\"\"Get all the tables used in the parsed query \"\"\" return [ get_table ( ). name for get_table in self\n\n## Step 3: Analyze the code to identify the missing functionality\nThe code provided seems to be an extension or modification of an existing class, likely the `BaseQuery` class from the DB-API. However, there are several aspects that are incomplete or missing, such as:\n- In the `run` method, the `self._strip_sql` method is called, but it's not defined in the provided code. This method appears to remove certain SQL tokens from the query string.\n- The `get_tables` method is only implemented for the `SELECT` query type, but there's no implementation for other query types like `FROM`, `JOIN`, `WHERE`, etc. This could lead to exceptions when trying to access tables for queries other than `SELECT`.\n- The `get_table` method is not properly implemented, as it would raise an exception for unimplemented query types.\n\n## Step 4: Identify the problem statement\nThe missing functionality is the implementation of the private"
      ]
    ],
    "filtered_resps": [
      " def get_columns ( self ) : \"\"\"Return a list of column labels\"\"\" return [ name for name in self get_tables ( ) : \"\"\"Get all the tables used in the parsed query \"\"\" return [ get_table ( ). name for get_table in self\n\n## Step 3: Analyze the code to identify the missing functionality\nThe code provided seems to be an extension or modification of an existing class, likely the `BaseQuery` class from the DB-API. However, there are several aspects that are incomplete or missing, such as:\n- In the `run` method, the `self._strip_sql` method is called, but it's not defined in the provided code. This method appears to remove certain SQL tokens from the query string.\n- The `get_tables` method is only implemented for the `SELECT` query type, but there's no implementation for other query types like `FROM`, `JOIN`, `WHERE`, etc. This could lead to exceptions when trying to access tables for queries other than `SELECT`.\n- The `get_table` method is not properly implemented, as it would raise an exception for unimplemented query types.\n\n## Step 4: Identify the problem statement\nThe missing functionality is the implementation of the private"
    ],
    "smoothed_bleu_4": 0.6109237398435766
  },
  {
    "doc_id": 84,
    "doc": {
      "id": 84,
      "repo": "apache/airflow",
      "path": "airflow/hooks/presto_hook.py",
      "func_name": "PrestoHook.insert_rows",
      "original_string": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)",
      "language": "python",
      "code": "def insert_rows(self, table, rows, target_fields=None):\n        \"\"\"\n        A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings\n        \"\"\"\n        super().insert_rows(table, rows, target_fields, 0)",
      "code_tokens": [
        "def",
        "insert_rows",
        "(",
        "self",
        ",",
        "table",
        ",",
        "rows",
        ",",
        "target_fields",
        "=",
        "None",
        ")",
        ":",
        "super",
        "(",
        ")",
        ".",
        "insert_rows",
        "(",
        "table",
        ",",
        "rows",
        ",",
        "target_fields",
        ",",
        "0",
        ")"
      ],
      "docstring": "A generic way to insert a set of tuples into a table.\n\n        :param table: Name of the target table\n        :type table: str\n        :param rows: The rows to insert into the table\n        :type rows: iterable of tuples\n        :param target_fields: The names of the columns to fill in the table\n        :type target_fields: iterable of strings",
      "docstring_tokens": [
        "A",
        "generic",
        "way",
        "to",
        "insert",
        "a",
        "set",
        "of",
        "tuples",
        "into",
        "a",
        "table",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L129-L140"
    },
    "target": "A generic way to insert a set of tuples into a table .",
    "arguments": [
      [
        "def insert_rows ( self , table , rows , target_fields = None ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # 0 is the default value\n```\n```\ndef insert_rows(self, table, rows, target_fields, offset=0):\n#... \n```\n```\ninsert_rows(table=\"users\", rows=[\n    {'name': 'John', 'age': 25},\n    {'name': 'Alice', 'age': 30}\n], target_fields=['name', 'email'])\n```\n```\ndef insert_rows ( self, table, rows, target_fields = None ) :\n     #... \n    # If rows is not an iterator, make it an iterator\n    if   not   hasattr ( rows,   '__iter__' ) :\n         rows   =   [ rows ]\n     #... \nself. insert_rows ( table,   rows [ 0 : offset ],   None,   0 )\nself. insert_rows ( table,   rows [ offset : ],   target_fields,   0 )\n``` 1. The last 3 code excerpts were combined into 1.\n\n```\ndef insert_rows ( self, table, rows, target_fields = None, offset = 0 ) :\n  \"\"\"Insert multiple rows into a table.\"\"\"\n  #... \n  # If rows is not an iterator,"
      ]
    ],
    "filtered_resps": [
      " # 0 is the default value\n```\n```\ndef insert_rows(self, table, rows, target_fields, offset=0):\n#... \n```\n```\ninsert_rows(table=\"users\", rows=[\n    {'name': 'John', 'age': 25},\n    {'name': 'Alice', 'age': 30}\n], target_fields=['name', 'email'])\n```\n```\ndef insert_rows ( self, table, rows, target_fields = None ) :\n     #... \n    # If rows is not an iterator, make it an iterator\n    if   not   hasattr ( rows,   '__iter__' ) :\n         rows   =   [ rows ]\n     #... \nself. insert_rows ( table,   rows [ 0 : offset ],   None,   0 )\nself. insert_rows ( table,   rows [ offset : ],   target_fields,   0 )\n``` 1. The last 3 code excerpts were combined into 1.\n\n```\ndef insert_rows ( self, table, rows, target_fields = None, offset = 0 ) :\n  \"\"\"Insert multiple rows into a table.\"\"\"\n  #... \n  # If rows is not an iterator,"
    ],
    "smoothed_bleu_4": 1.2292540870741455
  },
  {
    "doc_id": 85,
    "doc": {
      "id": 85,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_conn",
      "original_string": "def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client",
      "language": "python",
      "code": "def get_conn(self):\n        \"\"\"\n        Return a cosmos db client.\n        \"\"\"\n        if self.cosmos_client is not None:\n            return self.cosmos_client\n\n        # Initialize the Python Azure Cosmos DB client\n        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})\n\n        return self.cosmos_client",
      "code_tokens": [
        "def",
        "get_conn",
        "(",
        "self",
        ")",
        ":",
        "if",
        "self",
        ".",
        "cosmos_client",
        "is",
        "not",
        "None",
        ":",
        "return",
        "self",
        ".",
        "cosmos_client",
        "# Initialize the Python Azure Cosmos DB client",
        "self",
        ".",
        "cosmos_client",
        "=",
        "cosmos_client",
        ".",
        "CosmosClient",
        "(",
        "self",
        ".",
        "endpoint_uri",
        ",",
        "{",
        "'masterKey'",
        ":",
        "self",
        ".",
        "master_key",
        "}",
        ")",
        "return",
        "self",
        ".",
        "cosmos_client"
      ],
      "docstring": "Return a cosmos db client.",
      "docstring_tokens": [
        "Return",
        "a",
        "cosmos",
        "db",
        "client",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L50-L60"
    },
    "target": "Return a cosmos db client .",
    "arguments": [
      [
        "def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client # Initialize the Python Azure Cosmos DB client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` Once the connection to the Azure Cosmos DB account is established, you can use the `azure_cosmos_db` object to interact with the database. For example, you can create a container, add items to it, and retrieve items using the following code:\n  3. Add Items to Container: In this section, you will add items to the container. You'll create three items. Replace the values with your own for demonstration purposes. ``` 3.     def add_items_to_container ( self ) :         self. db_get_container (). offer_create ( 1, { \"name\" : \"Item1\", \"description\" : \"First item\" }, { \"partitionKeyvalue\" : \"Partition1\" })         self. db_get_container (). offer_create ( 2, { \"name\" : \"Item2\", \"description\" : \"Second item\" }, { \"partitionKeyvalue\" : \"Partition1\" })         self. db_get_container (). offer_create ( 3, { \"name\" : \"Item3\", \"description\" : \"Third item\" }, { \"partitionKeyvalue\" : \"Partition2\" }) ``` In the code above, `offer_create` is used"
      ]
    ],
    "filtered_resps": [
      " ``` Once the connection to the Azure Cosmos DB account is established, you can use the `azure_cosmos_db` object to interact with the database. For example, you can create a container, add items to it, and retrieve items using the following code:\n  3. Add Items to Container: In this section, you will add items to the container. You'll create three items. Replace the values with your own for demonstration purposes. ``` 3.     def add_items_to_container ( self ) :         self. db_get_container (). offer_create ( 1, { \"name\" : \"Item1\", \"description\" : \"First item\" }, { \"partitionKeyvalue\" : \"Partition1\" })         self. db_get_container (). offer_create ( 2, { \"name\" : \"Item2\", \"description\" : \"Second item\" }, { \"partitionKeyvalue\" : \"Partition1\" })         self. db_get_container (). offer_create ( 3, { \"name\" : \"Item3\", \"description\" : \"Third item\" }, { \"partitionKeyvalue\" : \"Partition2\" }) ``` In the code above, `offer_create` is used"
    ],
    "smoothed_bleu_4": 0.6537615454619797
  },
  {
    "doc_id": 86,
    "doc": {
      "id": 86,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.does_collection_exist",
      "original_string": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True",
      "language": "python",
      "code": "def does_collection_exist(self, collection_name, database_name=None):\n        \"\"\"\n        Checks if a collection exists in CosmosDB.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n        if len(existing_container) == 0:\n            return False\n\n        return True",
      "code_tokens": [
        "def",
        "does_collection_exist",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "existing_container",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryContainers",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "collection_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "if",
        "len",
        "(",
        "existing_container",
        ")",
        "==",
        "0",
        ":",
        "return",
        "False",
        "return",
        "True"
      ],
      "docstring": "Checks if a collection exists in CosmosDB.",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "collection",
        "exists",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L82-L99"
    },
    "target": "Checks if a collection exists in CosmosDB .",
    "arguments": [
      [
        "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n\ndef does_key_exist ( self, collection_name, collection_key ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) if collection_key is None : raise AirflowBadRequest ( \"Collection key cannot be None.\" ) existing_container = list ( self. get_conn ( ). QueryContainers ( get_database_link ( self. __get_database_name ( ) ), { \"query\" : \"SELECT * FROM r WHERE r.id=@id AND r.name like @_name\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : collection_name }, { \"name\" : \"@_name\", \"value\" : collection_key } ] } ) ) if len ( existing_container ) == 0 : return False return True\n\nThe first function, does_collection_exist, checks if a collection exists in a database. It takes the collection name and optionally the database name as parameters. The second function, does_key_exist, checks if a key exists in a collection.\n\nHere\u2019s a breakdown of the code:\n\n1. The functions take the collection name and optionally the database name as parameters.\n2. The code uses the ` AirflowBadRequest` exception to raise errors if"
      ]
    ],
    "filtered_resps": [
      "\n\ndef does_key_exist ( self, collection_name, collection_key ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) if collection_key is None : raise AirflowBadRequest ( \"Collection key cannot be None.\" ) existing_container = list ( self. get_conn ( ). QueryContainers ( get_database_link ( self. __get_database_name ( ) ), { \"query\" : \"SELECT * FROM r WHERE r.id=@id AND r.name like @_name\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : collection_name }, { \"name\" : \"@_name\", \"value\" : collection_key } ] } ) ) if len ( existing_container ) == 0 : return False return True\n\nThe first function, does_collection_exist, checks if a collection exists in a database. It takes the collection name and optionally the database name as parameters. The second function, does_key_exist, checks if a key exists in a collection.\n\nHere\u2019s a breakdown of the code:\n\n1. The functions take the collection name and optionally the database name as parameters.\n2. The code uses the ` AirflowBadRequest` exception to raise errors if"
    ],
    "smoothed_bleu_4": 1.9994758864060134
  },
  {
    "doc_id": 87,
    "doc": {
      "id": 87,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.create_collection",
      "original_string": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})",
      "language": "python",
      "code": "def create_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Creates a new collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        # We need to check to see if this container already exists so we don't try\n        # to create it twice\n        existing_container = list(self.get_conn().QueryContainers(\n            get_database_link(self.__get_database_name(database_name)), {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": collection_name}\n                ]\n            }))\n\n        # Only create if we did not find it already existing\n        if len(existing_container) == 0:\n            self.get_conn().CreateContainer(\n                get_database_link(self.__get_database_name(database_name)),\n                {\"id\": collection_name})",
      "code_tokens": [
        "def",
        "create_collection",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "# We need to check to see if this container already exists so we don't try",
        "# to create it twice",
        "existing_container",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryContainers",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "collection_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "# Only create if we did not find it already existing",
        "if",
        "len",
        "(",
        "existing_container",
        ")",
        "==",
        "0",
        ":",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateContainer",
        "(",
        "get_database_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ")",
        ",",
        "{",
        "\"id\"",
        ":",
        "collection_name",
        "}",
        ")"
      ],
      "docstring": "Creates a new collection in the CosmosDB database.",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L101-L122"
    },
    "target": "Creates a new collection in the CosmosDB database .",
    "arguments": [
      [
        "def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) # We need to check to see if this container already exists so we don't try # to create it twice existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def get_conn ( self ) : # TODO : Add support for Azure Cosmos Db and Postgres # for the moment we just support Azure SQL Database return self. connection ```... ``` def __get_database_name ( self, database_name ) : \"\"\"Returns the full database name. \"\"\" if database_name is None : return self. conn. get_database_name ( ) else : return self. conn. get_database_name ( database_name ) ```\nLine 12: `collection_name` is a string containing the name of the database collection.\n\nThe function `create_collection` is trying to determine if a container (collection) with the specified `collection_name` already exists in the database, and only create it if it does not exist. This is done by querying the `Containers` endpoint with a `SELECT * FROM r WHERE r.id=@id` query, where `@id` is a parameterized value that is set to the `collection_name` parameter.\n\n### Step 3: Use the `create_collection` function to create a collection named \"my_collection\"\n\n```python\n# Create an instance of the SQLDBOperator\nsql_db_operator = SQLDBOperator (\n     conn_id='sql_server_default',\n     database=get_database_name ()\n)\n\n"
      ]
    ],
    "filtered_resps": [
      " def get_conn ( self ) : # TODO : Add support for Azure Cosmos Db and Postgres # for the moment we just support Azure SQL Database return self. connection ```... ``` def __get_database_name ( self, database_name ) : \"\"\"Returns the full database name. \"\"\" if database_name is None : return self. conn. get_database_name ( ) else : return self. conn. get_database_name ( database_name ) ```\nLine 12: `collection_name` is a string containing the name of the database collection.\n\nThe function `create_collection` is trying to determine if a container (collection) with the specified `collection_name` already exists in the database, and only create it if it does not exist. This is done by querying the `Containers` endpoint with a `SELECT * FROM r WHERE r.id=@id` query, where `@id` is a parameterized value that is set to the `collection_name` parameter.\n\n### Step 3: Use the `create_collection` function to create a collection named \"my_collection\"\n\n```python\n# Create an instance of the SQLDBOperator\nsql_db_operator = SQLDBOperator (\n     conn_id='sql_server_default',\n     database=get_database_name ()\n)\n\n"
    ],
    "smoothed_bleu_4": 0.6617663339883744
  },
  {
    "doc_id": 88,
    "doc": {
      "id": 88,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.does_database_exist",
      "original_string": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True",
      "language": "python",
      "code": "def does_database_exist(self, database_name):\n        \"\"\"\n        Checks if a database exists in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n        if len(existing_database) == 0:\n            return False\n\n        return True",
      "code_tokens": [
        "def",
        "does_database_exist",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "existing_database",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryDatabases",
        "(",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "database_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "if",
        "len",
        "(",
        "existing_database",
        ")",
        "==",
        "0",
        ":",
        "return",
        "False",
        "return",
        "True"
      ],
      "docstring": "Checks if a database exists in CosmosDB.",
      "docstring_tokens": [
        "Checks",
        "if",
        "a",
        "database",
        "exists",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L124-L140"
    },
    "target": "Checks if a database exists in CosmosDB .",
    "arguments": [
      [
        "def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        "\n```\n\nThis code snippet is part of a class in an Object-Relational Mapping (ORM) system. The method `does_database_exist` checks whether a specified database exists in the system by performing a query on the `r` table where `id` equals the provided database name. If no results are found, it returns `False`, indicating that the database does not exist. Otherwise, it returns `True`.\n\nTo further improve this code, you could consider adding some error handling for potential exceptions that might occur during the query execution, such as connectivity issues or invalid database names. This could be achieved by wrapping the query execution in a `try`-`except` block and logging or raising specific errors as needed. Alternatively, you could leverage your ORM's built-in error handling features to simplify exception management. \n\nHere's an updated version incorporating some basic error handling:\n\n```python\ndef does_database_exist ( self, database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) try : existing_database = list ( self. get_conn ( ). QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\", \"parameters\" : [ {"
      ]
    ],
    "filtered_resps": [
      "\n```\n\nThis code snippet is part of a class in an Object-Relational Mapping (ORM) system. The method `does_database_exist` checks whether a specified database exists in the system by performing a query on the `r` table where `id` equals the provided database name. If no results are found, it returns `False`, indicating that the database does not exist. Otherwise, it returns `True`.\n\nTo further improve this code, you could consider adding some error handling for potential exceptions that might occur during the query execution, such as connectivity issues or invalid database names. This could be achieved by wrapping the query execution in a `try`-`except` block and logging or raising specific errors as needed. Alternatively, you could leverage your ORM's built-in error handling features to simplify exception management. \n\nHere's an updated version incorporating some basic error handling:\n\n```python\ndef does_database_exist ( self, database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) try : existing_database = list ( self. get_conn ( ). QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\", \"parameters\" : [ {"
    ],
    "smoothed_bleu_4": 0.9781912887723031
  },
  {
    "doc_id": 89,
    "doc": {
      "id": 89,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.create_database",
      "original_string": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})",
      "language": "python",
      "code": "def create_database(self, database_name):\n        \"\"\"\n        Creates a new database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        # We need to check to see if this database already exists so we don't try\n        # to create it twice\n        existing_database = list(self.get_conn().QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": database_name}\n            ]\n        }))\n\n        # Only create if we did not find it already existing\n        if len(existing_database) == 0:\n            self.get_conn().CreateDatabase({\"id\": database_name})",
      "code_tokens": [
        "def",
        "create_database",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "# We need to check to see if this database already exists so we don't try",
        "# to create it twice",
        "existing_database",
        "=",
        "list",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryDatabases",
        "(",
        "{",
        "\"query\"",
        ":",
        "\"SELECT * FROM r WHERE r.id=@id\"",
        ",",
        "\"parameters\"",
        ":",
        "[",
        "{",
        "\"name\"",
        ":",
        "\"@id\"",
        ",",
        "\"value\"",
        ":",
        "database_name",
        "}",
        "]",
        "}",
        ")",
        ")",
        "# Only create if we did not find it already existing",
        "if",
        "len",
        "(",
        "existing_database",
        ")",
        "==",
        "0",
        ":",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateDatabase",
        "(",
        "{",
        "\"id\"",
        ":",
        "database_name",
        "}",
        ")"
      ],
      "docstring": "Creates a new database in CosmosDB.",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "database",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L142-L160"
    },
    "target": "Creates a new database in CosmosDB .",
    "arguments": [
      [
        "def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) # We need to check to see if this database already exists so we don't try # to create it twice existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " # If we made it here, then the database already exists if it's an # existing database we don't need the existing_database variable # because it was created and initialized in the Query function above return database_name except AttributeError : raise AirflowBadRequest ( \"Can't create database\" )\n\n## Step 3: Write the test suite\n```python\nimport unittest\nfrom airflow import settings\nfrom airflow.configuration import conf\nfrom tests.utils import create_test_context\n\nclass TestDatabase(unittest.TestCase):\n    def test_create_database ( self ) :\n        db = DatabaseOperator ( conf = conf, api = settings. api ) database_name ='my_database' db. create_database ( database_name ) try : existing_database = list(db. get_conn ( ). QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : database_name } ] } ) ) self. fail ( \"Database should have been created\" ) except RuntimeError : # Database created successfully pass self. assertNull(existing_database)\n```\n## Step 4: Run the test suite\n```bash\npython -m unittest discover -s /path/to/air"
      ]
    ],
    "filtered_resps": [
      " # If we made it here, then the database already exists if it's an # existing database we don't need the existing_database variable # because it was created and initialized in the Query function above return database_name except AttributeError : raise AirflowBadRequest ( \"Can't create database\" )\n\n## Step 3: Write the test suite\n```python\nimport unittest\nfrom airflow import settings\nfrom airflow.configuration import conf\nfrom tests.utils import create_test_context\n\nclass TestDatabase(unittest.TestCase):\n    def test_create_database ( self ) :\n        db = DatabaseOperator ( conf = conf, api = settings. api ) database_name ='my_database' db. create_database ( database_name ) try : existing_database = list(db. get_conn ( ). QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\", \"parameters\" : [ { \"name\" : \"@id\", \"value\" : database_name } ] } ) ) self. fail ( \"Database should have been created\" ) except RuntimeError : # Database created successfully pass self. assertNull(existing_database)\n```\n## Step 4: Run the test suite\n```bash\npython -m unittest discover -s /path/to/air"
    ],
    "smoothed_bleu_4": 0.4943024981650175
  },
  {
    "doc_id": 90,
    "doc": {
      "id": 90,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_database",
      "original_string": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))",
      "language": "python",
      "code": "def delete_database(self, database_name):\n        \"\"\"\n        Deletes an existing database in CosmosDB.\n        \"\"\"\n        if database_name is None:\n            raise AirflowBadRequest(\"Database name cannot be None.\")\n\n        self.get_conn().DeleteDatabase(get_database_link(database_name))",
      "code_tokens": [
        "def",
        "delete_database",
        "(",
        "self",
        ",",
        "database_name",
        ")",
        ":",
        "if",
        "database_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Database name cannot be None.\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteDatabase",
        "(",
        "get_database_link",
        "(",
        "database_name",
        ")",
        ")"
      ],
      "docstring": "Deletes an existing database in CosmosDB.",
      "docstring_tokens": [
        "Deletes",
        "an",
        "existing",
        "database",
        "in",
        "CosmosDB",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L162-L169"
    },
    "target": "Deletes an existing database in CosmosDB .",
    "arguments": [
      [
        "def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " try : self. get_conn ( ). Commit ( ) except : self. get_conn ( ). Rollback ( ) self. _database_name = None\nDeleteDatabase : Deletes the specified database.\nThis function calls an instance method, DeleteDatabase, of the connection object to delete the database with the specified SQL Server link in the Airflow database.\nAfter executing the DeleteDatabase method, it commits the transaction if successful or rolls back the transaction if any error occurs.\n### Deleting a Schema\nTo remove a schema from the schema registry, you delete the corresponding table from the \u2018airgapped_schema_registry\u2019. Schema deletion is also an asynchronous operation because it may take some time to delete all the data from the schema table. However, you can cancel the operation if needed.\n- **delete_schema** : Delete a schema from the schema registry.\n\n```python\nasync def delete_schema ( self, schema_name ) : try : await self. delete_schema_async ( schema_name ) self. schema_cache. drop_tables ( schema_name )\nexcept Exception as e : Logging. error ( \" Failed to delete schema ( {} ) \". format ( schema_name ), exc_info = e )\n```\nThis asynchronous method calls another async method, delete_schema_async,"
      ]
    ],
    "filtered_resps": [
      " try : self. get_conn ( ). Commit ( ) except : self. get_conn ( ). Rollback ( ) self. _database_name = None\nDeleteDatabase : Deletes the specified database.\nThis function calls an instance method, DeleteDatabase, of the connection object to delete the database with the specified SQL Server link in the Airflow database.\nAfter executing the DeleteDatabase method, it commits the transaction if successful or rolls back the transaction if any error occurs.\n### Deleting a Schema\nTo remove a schema from the schema registry, you delete the corresponding table from the \u2018airgapped_schema_registry\u2019. Schema deletion is also an asynchronous operation because it may take some time to delete all the data from the schema table. However, you can cancel the operation if needed.\n- **delete_schema** : Delete a schema from the schema registry.\n\n```python\nasync def delete_schema ( self, schema_name ) : try : await self. delete_schema_async ( schema_name ) self. schema_cache. drop_tables ( schema_name )\nexcept Exception as e : Logging. error ( \" Failed to delete schema ( {} ) \". format ( schema_name ), exc_info = e )\n```\nThis asynchronous method calls another async method, delete_schema_async,"
    ],
    "smoothed_bleu_4": 0.5616360713083587
  },
  {
    "doc_id": 91,
    "doc": {
      "id": 91,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_collection",
      "original_string": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
      "language": "python",
      "code": "def delete_collection(self, collection_name, database_name=None):\n        \"\"\"\n        Deletes an existing collection in the CosmosDB database.\n        \"\"\"\n        if collection_name is None:\n            raise AirflowBadRequest(\"Collection name cannot be None.\")\n\n        self.get_conn().DeleteContainer(\n            get_collection_link(self.__get_database_name(database_name), collection_name))",
      "code_tokens": [
        "def",
        "delete_collection",
        "(",
        "self",
        ",",
        "collection_name",
        ",",
        "database_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "collection_name",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Collection name cannot be None.\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteContainer",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "collection_name",
        ")",
        ")"
      ],
      "docstring": "Deletes an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Deletes",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L171-L179"
    },
    "target": "Deletes an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) self . get_conn ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. __mark_collection_as_deleted ( collection_name, database_name )\n     def get_collection_id ( self, collection_name, database_name = None ) : \"Get collection id if exists in database \" collection_link = get_collection_link ( self. __get_database_name ( database_name ), collection_name ) container = self. get_conn ( ). GetContainer ( collection_link )\n     def create_collection ( self, collection_name, database_name = None ) : \"Create a new collection\" collection_link = get_collection_link ( self. __get_database_name ( database_name ), collection_name ) collection_doc = {} self. get_conn ( ). CreateContainer ( collection_link, collection_doc ) return self. get_collection_id ( collection_name, database_name )\n     @staticmethod def get_collection_count ( conn, link ) : return conn. GetContainerCount ( link )\n     def __mark_collection_as_deleted ( self, name, database_name ) : self. get_conn ( ). UpdateDocument ( collection_link = get_collection_link ( self. __get_database_name ( database_name ), name ), update = { \"$set\" : { \"couch_docs_total\" : 0, \"couch_db_name\" : None }, \"$"
      ]
    ],
    "filtered_resps": [
      " self. __mark_collection_as_deleted ( collection_name, database_name )\n     def get_collection_id ( self, collection_name, database_name = None ) : \"Get collection id if exists in database \" collection_link = get_collection_link ( self. __get_database_name ( database_name ), collection_name ) container = self. get_conn ( ). GetContainer ( collection_link )\n     def create_collection ( self, collection_name, database_name = None ) : \"Create a new collection\" collection_link = get_collection_link ( self. __get_database_name ( database_name ), collection_name ) collection_doc = {} self. get_conn ( ). CreateContainer ( collection_link, collection_doc ) return self. get_collection_id ( collection_name, database_name )\n     @staticmethod def get_collection_count ( conn, link ) : return conn. GetContainerCount ( link )\n     def __mark_collection_as_deleted ( self, name, database_name ) : self. get_conn ( ). UpdateDocument ( collection_link = get_collection_link ( self. __get_database_name ( database_name ), name ), update = { \"$set\" : { \"couch_docs_total\" : 0, \"couch_db_name\" : None }, \"$"
    ],
    "smoothed_bleu_4": 0.4741723495386492
  },
  {
    "doc_id": 92,
    "doc": {
      "id": 92,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.insert_documents",
      "original_string": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents",
      "language": "python",
      "code": "def insert_documents(self, documents, database_name=None, collection_name=None):\n        \"\"\"\n        Insert a list of new documents into an existing collection in the CosmosDB database.\n        \"\"\"\n        if documents is None:\n            raise AirflowBadRequest(\"You cannot insert empty documents\")\n\n        created_documents = []\n        for single_document in documents:\n            created_documents.append(\n                self.get_conn().CreateItem(\n                    get_collection_link(\n                        self.__get_database_name(database_name),\n                        self.__get_collection_name(collection_name)),\n                    single_document))\n\n        return created_documents",
      "code_tokens": [
        "def",
        "insert_documents",
        "(",
        "self",
        ",",
        "documents",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "documents",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"You cannot insert empty documents\"",
        ")",
        "created_documents",
        "=",
        "[",
        "]",
        "for",
        "single_document",
        "in",
        "documents",
        ":",
        "created_documents",
        ".",
        "append",
        "(",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "CreateItem",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ")",
        ",",
        "single_document",
        ")",
        ")",
        "return",
        "created_documents"
      ],
      "docstring": "Insert a list of new documents into an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Insert",
        "a",
        "list",
        "of",
        "new",
        "documents",
        "into",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L208-L224"
    },
    "target": "Insert a list of new documents into an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( \"You cannot insert empty documents\" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ```\nThe `insert_documents` method takes in a list of documents, an optional `database_name`, and an optional `collection_name`. It first checks if the input `documents` is `None` and raises a `AirflowBadRequest` exception if it is. Then, it uses a for loop to iterate through each document in the input list and calls the `get_conn()` method to return the database connection. It then uses the `CreateItem` method to create a new item in the specified collection. The created item is a dictionary containing information about the newly created document, including its `_id` attribute. This new item is then appended to the `created_documents` list, which is returned at the end of the method.\n## Step-by-Step Solution\nTo implement the `insert_or_update_documents` function using the `insert_documents` method described above, you would:\n\n1. Define the `insert_or_update_documents` method with the required parameters: `self`, `documents`, `database_name`, and `collection_name`.\n2. Initialize an empty dictionary `docs_to_update` to store documents that need to be updated.\n3. Iterate through the input `documents` and check if each document has a valid `_id` attribute."
      ]
    ],
    "filtered_resps": [
      " ```\nThe `insert_documents` method takes in a list of documents, an optional `database_name`, and an optional `collection_name`. It first checks if the input `documents` is `None` and raises a `AirflowBadRequest` exception if it is. Then, it uses a for loop to iterate through each document in the input list and calls the `get_conn()` method to return the database connection. It then uses the `CreateItem` method to create a new item in the specified collection. The created item is a dictionary containing information about the newly created document, including its `_id` attribute. This new item is then appended to the `created_documents` list, which is returned at the end of the method.\n## Step-by-Step Solution\nTo implement the `insert_or_update_documents` function using the `insert_documents` method described above, you would:\n\n1. Define the `insert_or_update_documents` method with the required parameters: `self`, `documents`, `database_name`, and `collection_name`.\n2. Initialize an empty dictionary `docs_to_update` to store documents that need to be updated.\n3. Iterate through the input `documents` and check if each document has a valid `_id` attribute."
    ],
    "smoothed_bleu_4": 1.1372146690513247
  },
  {
    "doc_id": 93,
    "doc": {
      "id": 93,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.delete_document",
      "original_string": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))",
      "language": "python",
      "code": "def delete_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Delete an existing document out of a collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot delete a document without an id\")\n\n        self.get_conn().DeleteItem(\n            get_document_link(\n                self.__get_database_name(database_name),\n                self.__get_collection_name(collection_name),\n                document_id))",
      "code_tokens": [
        "def",
        "delete_document",
        "(",
        "self",
        ",",
        "document_id",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "document_id",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Cannot delete a document without an id\"",
        ")",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "DeleteItem",
        "(",
        "get_document_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ",",
        "document_id",
        ")",
        ")"
      ],
      "docstring": "Delete an existing document out of a collection in the CosmosDB database.",
      "docstring_tokens": [
        "Delete",
        "an",
        "existing",
        "document",
        "out",
        "of",
        "a",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L226-L237"
    },
    "target": "Delete an existing document out of a collection in the CosmosDB database .",
    "arguments": [
      [
        "def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot delete a document without an id\" ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " self. close_conn ( )\n\n### close_connection\nClose connection with the NoSQL DB, if not yet closed, and clean the connection object for future reuse.\n\n#### Parameters\n\n#### Returns\n\n### get_collection\nGet the document collection object for given database_name and collection_name from the NOSQL instance.\n\n#### Parameters\n\n  - collection_name : The name of the collection\n  - database_name : The name of the database\n\n#### Returns\n\n#### Raises\n\n### get_document\nRetrieves a document from the database. Note that the document ID needs to be the _id field of the document. If the document is a list, the ID needs to be the first item in the list.\n\n#### Parameters\n\n  - document_id : The ID of the document to retrieve. Can be a single ID, or a list of documents if you need to pull back data for a list of IDs at once\n  - database_name : The name of the database\n  - collection_name : The name of the collection\n\n#### Returns\n\n#### Raises\n\n#### See Also\n\n### get_document_link\nReturns the full path of a document given the database, collection and document IDs\n\n#### Parameters\n\n  - database_name : the name of the database\n "
      ]
    ],
    "filtered_resps": [
      " self. close_conn ( )\n\n### close_connection\nClose connection with the NoSQL DB, if not yet closed, and clean the connection object for future reuse.\n\n#### Parameters\n\n#### Returns\n\n### get_collection\nGet the document collection object for given database_name and collection_name from the NOSQL instance.\n\n#### Parameters\n\n  - collection_name : The name of the collection\n  - database_name : The name of the database\n\n#### Returns\n\n#### Raises\n\n### get_document\nRetrieves a document from the database. Note that the document ID needs to be the _id field of the document. If the document is a list, the ID needs to be the first item in the list.\n\n#### Parameters\n\n  - document_id : The ID of the document to retrieve. Can be a single ID, or a list of documents if you need to pull back data for a list of IDs at once\n  - database_name : The name of the database\n  - collection_name : The name of the collection\n\n#### Returns\n\n#### Raises\n\n#### See Also\n\n### get_document_link\nReturns the full path of a document given the database, collection and document IDs\n\n#### Parameters\n\n  - database_name : the name of the database\n "
    ],
    "smoothed_bleu_4": 0.8866451457845068
  },
  {
    "doc_id": 94,
    "doc": {
      "id": 94,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_document",
      "original_string": "def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None",
      "language": "python",
      "code": "def get_document(self, document_id, database_name=None, collection_name=None):\n        \"\"\"\n        Get a document from an existing collection in the CosmosDB database.\n        \"\"\"\n        if document_id is None:\n            raise AirflowBadRequest(\"Cannot get a document without an id\")\n\n        try:\n            return self.get_conn().ReadItem(\n                get_document_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name),\n                    document_id))\n        except HTTPFailure:\n            return None",
      "code_tokens": [
        "def",
        "get_document",
        "(",
        "self",
        ",",
        "document_id",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ")",
        ":",
        "if",
        "document_id",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"Cannot get a document without an id\"",
        ")",
        "try",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "ReadItem",
        "(",
        "get_document_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ",",
        "document_id",
        ")",
        ")",
        "except",
        "HTTPFailure",
        ":",
        "return",
        "None"
      ],
      "docstring": "Get a document from an existing collection in the CosmosDB database.",
      "docstring_tokens": [
        "Get",
        "a",
        "document",
        "from",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L239-L253"
    },
    "target": "Get a document from an existing collection in the CosmosDB database .",
    "arguments": [
      [
        "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot get a document without an id\" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " except Exception : raise ```\n    - def get_id ( self ) : \"\"\"This is the MongoDB id of the document\"\"\" return self. doc [ \"_id\" ] ```\n    - def get_links ( self ) :\n    - ```return self.doc.get('links', None)```\n    - def get_link_url ( self ) :\n    - ```return self.doc.get('link_to', None)```\n    - def get_parent_id ( self ) :\n    - ```return self.doc.get('parent_id', None)```\n    - def get_parent_id_if_linked ( self ) :\n    - ```return self.doc.get('parent_id_if_linked', None)```\n    - def get_parent_if_linked ( self ) :\n    - ```return self.doc.get('parent_if_linked', None)```\n    - def get_version ( self ) :\n    - ```return self.doc.get('version', None)```\n    - def init ( self ) :\n    - `# self.db = database_manager.get_database(self.database_name)` `# self.collection = database_manager.get_collection(self.collection_name) self.document_link = self.__get_document_link() print(\"Got document link:\", self.document_link) self.doc = self.conn.ReadItem(self"
      ]
    ],
    "filtered_resps": [
      " except Exception : raise ```\n    - def get_id ( self ) : \"\"\"This is the MongoDB id of the document\"\"\" return self. doc [ \"_id\" ] ```\n    - def get_links ( self ) :\n    - ```return self.doc.get('links', None)```\n    - def get_link_url ( self ) :\n    - ```return self.doc.get('link_to', None)```\n    - def get_parent_id ( self ) :\n    - ```return self.doc.get('parent_id', None)```\n    - def get_parent_id_if_linked ( self ) :\n    - ```return self.doc.get('parent_id_if_linked', None)```\n    - def get_parent_if_linked ( self ) :\n    - ```return self.doc.get('parent_if_linked', None)```\n    - def get_version ( self ) :\n    - ```return self.doc.get('version', None)```\n    - def init ( self ) :\n    - `# self.db = database_manager.get_database(self.database_name)` `# self.collection = database_manager.get_collection(self.collection_name) self.document_link = self.__get_document_link() print(\"Got document link:\", self.document_link) self.doc = self.conn.ReadItem(self"
    ],
    "smoothed_bleu_4": 0.4654540794969885
  },
  {
    "doc_id": 95,
    "doc": {
      "id": 95,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/azure_cosmos_hook.py",
      "func_name": "AzureCosmosDBHook.get_documents",
      "original_string": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None",
      "language": "python",
      "code": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None",
      "code_tokens": [
        "def",
        "get_documents",
        "(",
        "self",
        ",",
        "sql_string",
        ",",
        "database_name",
        "=",
        "None",
        ",",
        "collection_name",
        "=",
        "None",
        ",",
        "partition_key",
        "=",
        "None",
        ")",
        ":",
        "if",
        "sql_string",
        "is",
        "None",
        ":",
        "raise",
        "AirflowBadRequest",
        "(",
        "\"SQL query string cannot be None\"",
        ")",
        "# Query them in SQL",
        "query",
        "=",
        "{",
        "'query'",
        ":",
        "sql_string",
        "}",
        "try",
        ":",
        "result_iterable",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "QueryItems",
        "(",
        "get_collection_link",
        "(",
        "self",
        ".",
        "__get_database_name",
        "(",
        "database_name",
        ")",
        ",",
        "self",
        ".",
        "__get_collection_name",
        "(",
        "collection_name",
        ")",
        ")",
        ",",
        "query",
        ",",
        "partition_key",
        ")",
        "return",
        "list",
        "(",
        "result_iterable",
        ")",
        "except",
        "HTTPFailure",
        ":",
        "return",
        "None"
      ],
      "docstring": "Get a list of documents from an existing collection in the CosmosDB database via SQL query.",
      "docstring_tokens": [
        "Get",
        "a",
        "list",
        "of",
        "documents",
        "from",
        "an",
        "existing",
        "collection",
        "in",
        "the",
        "CosmosDB",
        "database",
        "via",
        "SQL",
        "query",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275"
    },
    "target": "Get a list of documents from an existing collection in the CosmosDB database via SQL query .",
    "arguments": [
      [
        "def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) # Query them in SQL query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " def _run_get ( self, sql_string, method_name, path_prefix ) : if not self. _has_collection ( path_prefix ) : raise AirflowBadRequest ( \"Collection name is required for method\" + str ( method_name ) ) if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) query = { 'query' : sql_string } response = self. _do_request ( 'POST', urljoin ( path_prefix, method_name ), headers = self. _get_headers ( ), data = json. dumps ( query ) ) if 'error' in response ['response' ] : raise AirflowBadRequest ( response ['response' ] [ 'error_message' ] ) return response ['response' ] def _get_headers ( self ) : return { 'x-ms-version' : '2015-06-01' } ```\nThe function `get_documents` performs queries on Cosmos DB. The first thing it checks if the SQL query string, `sql_string`, is None, and if so, it raises an `AirflowBadRequest`. If not, it constructs a query dictionary, `query`, and tries to execute the query using the `QueryItems` method"
      ]
    ],
    "filtered_resps": [
      " def _run_get ( self, sql_string, method_name, path_prefix ) : if not self. _has_collection ( path_prefix ) : raise AirflowBadRequest ( \"Collection name is required for method\" + str ( method_name ) ) if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) query = { 'query' : sql_string } response = self. _do_request ( 'POST', urljoin ( path_prefix, method_name ), headers = self. _get_headers ( ), data = json. dumps ( query ) ) if 'error' in response ['response' ] : raise AirflowBadRequest ( response ['response' ] [ 'error_message' ] ) return response ['response' ] def _get_headers ( self ) : return { 'x-ms-version' : '2015-06-01' } ```\nThe function `get_documents` performs queries on Cosmos DB. The first thing it checks if the SQL query string, `sql_string`, is None, and if so, it raises an `AirflowBadRequest`. If not, it constructs a query dictionary, `query`, and tries to execute the query using the `QueryItems` method"
    ],
    "smoothed_bleu_4": 0.8063882203414353
  },
  {
    "doc_id": 96,
    "doc": {
      "id": 96,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.get_function",
      "original_string": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)",
      "language": "python",
      "code": "def get_function(self, name):\n        \"\"\"\n        Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict\n        \"\"\"\n        return self.get_conn().projects().locations().functions().get(\n            name=name).execute(num_retries=self.num_retries)",
      "code_tokens": [
        "def",
        "get_function",
        "(",
        "self",
        ",",
        "name",
        ")",
        ":",
        "return",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "get",
        "(",
        "name",
        "=",
        "name",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")"
      ],
      "docstring": "Returns the Cloud Function with the given name.\n\n        :param name: Name of the function.\n        :type name: str\n        :return: A Cloud Functions object representing the function.\n        :rtype: dict",
      "docstring_tokens": [
        "Returns",
        "the",
        "Cloud",
        "Function",
        "with",
        "the",
        "given",
        "name",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L76-L86"
    },
    "target": "Returns the Cloud Function with the given name .",
    "arguments": [
      [
        "def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . get ( name = name ) . execute ( num_retries = self . num_retries )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ; ``` If the function doesn\u2019t exist, it simply raises an exception without retries.\n\nThe code assumes you are familiar with the use of Python decorators, asynchronous code, and basic project structure.\n\n### Step 5: Create a testing file\n\nCreate a testing file, e.g., `test_project.py`, in the same directory as your `project.py` file. You can use the `unittest` framework to write test cases.\n\n```python\n# test_project.py\nimport unittest\nfrom project import Project\nfrom google.cloud import aip\n\nclass TestProject ( unittest(TestCase ) :\n    def setUp ( self ) :\n        self. project = Project ( )\n\n    def test_get_function ( self ) :\n        # Test a successful function retrieval\n        func = self. project. get_function ( \"my_function\" )\n        self. assertNotNull ( func )\n\n    def test_get_function_not_found ( self ) :\n        # Test when a function doesn\u2019t exist\n        self. expectRaises ( aip. NotFound ), lambda : self. project. get_function ( \"non_existent_function\" ) )\n\nif __name__ == '__main__' :\n    unittest. main ( )\n```\n\nThis test suite checks for a successful function retrieval and tests"
      ]
    ],
    "filtered_resps": [
      " ; ``` If the function doesn\u2019t exist, it simply raises an exception without retries.\n\nThe code assumes you are familiar with the use of Python decorators, asynchronous code, and basic project structure.\n\n### Step 5: Create a testing file\n\nCreate a testing file, e.g., `test_project.py`, in the same directory as your `project.py` file. You can use the `unittest` framework to write test cases.\n\n```python\n# test_project.py\nimport unittest\nfrom project import Project\nfrom google.cloud import aip\n\nclass TestProject ( unittest(TestCase ) :\n    def setUp ( self ) :\n        self. project = Project ( )\n\n    def test_get_function ( self ) :\n        # Test a successful function retrieval\n        func = self. project. get_function ( \"my_function\" )\n        self. assertNotNull ( func )\n\n    def test_get_function_not_found ( self ) :\n        # Test when a function doesn\u2019t exist\n        self. expectRaises ( aip. NotFound ), lambda : self. project. get_function ( \"non_existent_function\" ) )\n\nif __name__ == '__main__' :\n    unittest. main ( )\n```\n\nThis test suite checks for a successful function retrieval and tests"
    ],
    "smoothed_bleu_4": 0.7404195876962832
  },
  {
    "doc_id": 97,
    "doc": {
      "id": 97,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.create_new_function",
      "original_string": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "language": "python",
      "code": "def create_new_function(self, location, body, project_id=None):\n        \"\"\"\n        Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().create(\n            location=self._full_location(project_id, location),\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "code_tokens": [
        "def",
        "create_new_function",
        "(",
        "self",
        ",",
        "location",
        ",",
        "body",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "create",
        "(",
        "location",
        "=",
        "self",
        ".",
        "_full_location",
        "(",
        "project_id",
        ",",
        "location",
        ")",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Creates a new function in Cloud Function in the location specified in the body.\n\n        :param location: The location of the function.\n        :type location: str\n        :param body: The body required by the Cloud Functions insert API.\n        :type body: dict\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: None",
      "docstring_tokens": [
        "Creates",
        "a",
        "new",
        "function",
        "in",
        "Cloud",
        "Function",
        "in",
        "the",
        "location",
        "specified",
        "in",
        "the",
        "body",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L89-L107"
    },
    "target": "Creates a new function in Cloud Function in the location specified in the body .",
    "arguments": [
      [
        "def create_new_function ( self , location , body , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " return operation_name\n\n### def create_or_update_function ( self, location, body, project_id = None ) \u00b6\n\nCreates a new cloud functions function or updates an existing one.\n\n  - Parameters\n  -\n    - location ( ) \u2013 The cloud functions location to create the function in, e.g. \u2018us-central1\u2019. This can also be the name of a google cloud region. If both the location and project-id are specified, the name must be a google cloud region name. This parameter will be set internally if the project-id is also specified. The project-id will be used to make sure the function is within the correct project. If the project-id is not specified a new project will be created.\n    - body ( dict ) \u2013 Function to create or update. Example: ``` {    'entryPoint' :  'main', 'runtime' :   'nodejs8', 'timeout' :   -1, 'handler':      {       'source':         {          'url':             'gs://my-bucket/my-path'        }      } } ```\n    - project_id \u2013 The project id to create the function in. Default is None.\n\nExample: ``` from   googlecloudsdk.calliope"
      ]
    ],
    "filtered_resps": [
      " return operation_name\n\n### def create_or_update_function ( self, location, body, project_id = None ) \u00b6\n\nCreates a new cloud functions function or updates an existing one.\n\n  - Parameters\n  -\n    - location ( ) \u2013 The cloud functions location to create the function in, e.g. \u2018us-central1\u2019. This can also be the name of a google cloud region. If both the location and project-id are specified, the name must be a google cloud region name. This parameter will be set internally if the project-id is also specified. The project-id will be used to make sure the function is within the correct project. If the project-id is not specified a new project will be created.\n    - body ( dict ) \u2013 Function to create or update. Example: ``` {    'entryPoint' :  'main', 'runtime' :   'nodejs8', 'timeout' :   -1, 'handler':      {       'source':         {          'url':             'gs://my-bucket/my-path'        }      } } ```\n    - project_id \u2013 The project id to create the function in. Default is None.\n\nExample: ``` from   googlecloudsdk.calliope"
    ],
    "smoothed_bleu_4": 1.4386515592548075
  },
  {
    "doc_id": 98,
    "doc": {
      "id": 98,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.update_function",
      "original_string": "def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "language": "python",
      "code": "def update_function(self, name, body, update_mask):\n        \"\"\"\n        Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().patch(\n            updateMask=\",\".join(update_mask),\n            name=name,\n            body=body\n        ).execute(num_retries=self.num_retries)\n        operation_name = response[\"name\"]\n        self._wait_for_operation_to_complete(operation_name=operation_name)",
      "code_tokens": [
        "def",
        "update_function",
        "(",
        "self",
        ",",
        "name",
        ",",
        "body",
        ",",
        "update_mask",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "patch",
        "(",
        "updateMask",
        "=",
        "\",\"",
        ".",
        "join",
        "(",
        "update_mask",
        ")",
        ",",
        "name",
        "=",
        "name",
        ",",
        "body",
        "=",
        "body",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "operation_name",
        "=",
        "response",
        "[",
        "\"name\"",
        "]",
        "self",
        ".",
        "_wait_for_operation_to_complete",
        "(",
        "operation_name",
        "=",
        "operation_name",
        ")"
      ],
      "docstring": "Updates Cloud Functions according to the specified update mask.\n\n        :param name: The name of the function.\n        :type name: str\n        :param body: The body required by the cloud function patch API.\n        :type body: dict\n        :param update_mask: The update mask - array of fields that should be patched.\n        :type update_mask: [str]\n        :return: None",
      "docstring_tokens": [
        "Updates",
        "Cloud",
        "Functions",
        "according",
        "to",
        "the",
        "specified",
        "update",
        "mask",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L109-L127"
    },
    "target": "Updates Cloud Functions according to the specified update mask .",
    "arguments": [
      [
        "def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updateMask = \",\" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " _create_function ( self, name, body ) def get_conn ( self ) :- self. _initialise_connection ( ) def _initialise_connection ( self ) :- if not self. connected :- if not self. _service_account : raise ValueError ( \u2019No service account provided\u2019 ) self. client = discovery. build ( \"cloudfunctions\u201d, \u201dcloud-platform\u201d, credentials = Credentials ( \u201c_service_account\u201d ) self. client. projects ( ). locations ( ). functions ( ). get ( ) self. connected = True return self. client def _wait_for_operation_to_complete ( self, operation_name ) :- request = self. client. projects ( ). locations ( ). operations ( ). get ( operationName = operation_name ). execute ( num_retries = self. num_retries ) response = request. result while response [ \"done\" ] == False :- time. sleep ( response [ \"metadata\" ] [ \"estimatedTimeToCompletion\" ] ) request = self. client. projects ( ). locations ( ). operations ( ). get ( operationName = operation_name ). execute ( num_retries = self. num_retries ) response = request. result return response [ \"done"
      ]
    ],
    "filtered_resps": [
      " _create_function ( self, name, body ) def get_conn ( self ) :- self. _initialise_connection ( ) def _initialise_connection ( self ) :- if not self. connected :- if not self. _service_account : raise ValueError ( \u2019No service account provided\u2019 ) self. client = discovery. build ( \"cloudfunctions\u201d, \u201dcloud-platform\u201d, credentials = Credentials ( \u201c_service_account\u201d ) self. client. projects ( ). locations ( ). functions ( ). get ( ) self. connected = True return self. client def _wait_for_operation_to_complete ( self, operation_name ) :- request = self. client. projects ( ). locations ( ). operations ( ). get ( operationName = operation_name ). execute ( num_retries = self. num_retries ) response = request. result while response [ \"done\" ] == False :- time. sleep ( response [ \"metadata\" ] [ \"estimatedTimeToCompletion\" ] ) request = self. client. projects ( ). locations ( ). operations ( ). get ( operationName = operation_name ). execute ( num_retries = self. num_retries ) response = request. result return response [ \"done"
    ],
    "smoothed_bleu_4": 0.5413284947723218
  },
  {
    "doc_id": 99,
    "doc": {
      "id": 99,
      "repo": "apache/airflow",
      "path": "airflow/contrib/hooks/gcp_function_hook.py",
      "func_name": "GcfHook.upload_function_zip",
      "original_string": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url",
      "language": "python",
      "code": "def upload_function_zip(self, location, zip_path, project_id=None):\n        \"\"\"\n        Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.\n        \"\"\"\n        response = self.get_conn().projects().locations().functions().generateUploadUrl(\n            parent=self._full_location(project_id, location)\n        ).execute(num_retries=self.num_retries)\n        upload_url = response.get('uploadUrl')\n        with open(zip_path, 'rb') as fp:\n            requests.put(\n                url=upload_url,\n                data=fp,\n                # Those two headers needs to be specified according to:\n                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl\n                # nopep8\n                headers={\n                    'Content-type': 'application/zip',\n                    'x-goog-content-length-range': '0,104857600',\n                }\n            )\n        return upload_url",
      "code_tokens": [
        "def",
        "upload_function_zip",
        "(",
        "self",
        ",",
        "location",
        ",",
        "zip_path",
        ",",
        "project_id",
        "=",
        "None",
        ")",
        ":",
        "response",
        "=",
        "self",
        ".",
        "get_conn",
        "(",
        ")",
        ".",
        "projects",
        "(",
        ")",
        ".",
        "locations",
        "(",
        ")",
        ".",
        "functions",
        "(",
        ")",
        ".",
        "generateUploadUrl",
        "(",
        "parent",
        "=",
        "self",
        ".",
        "_full_location",
        "(",
        "project_id",
        ",",
        "location",
        ")",
        ")",
        ".",
        "execute",
        "(",
        "num_retries",
        "=",
        "self",
        ".",
        "num_retries",
        ")",
        "upload_url",
        "=",
        "response",
        ".",
        "get",
        "(",
        "'uploadUrl'",
        ")",
        "with",
        "open",
        "(",
        "zip_path",
        ",",
        "'rb'",
        ")",
        "as",
        "fp",
        ":",
        "requests",
        ".",
        "put",
        "(",
        "url",
        "=",
        "upload_url",
        ",",
        "data",
        "=",
        "fp",
        ",",
        "# Those two headers needs to be specified according to:",
        "# https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl",
        "# nopep8",
        "headers",
        "=",
        "{",
        "'Content-type'",
        ":",
        "'application/zip'",
        ",",
        "'x-goog-content-length-range'",
        ":",
        "'0,104857600'",
        ",",
        "}",
        ")",
        "return",
        "upload_url"
      ],
      "docstring": "Uploads zip file with sources.\n\n        :param location: The location where the function is created.\n        :type location: str\n        :param zip_path: The path of the valid .zip file to upload.\n        :type zip_path: str\n        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n            If set to None or missing, the default project_id from the GCP connection is used.\n        :type project_id: str\n        :return: The upload URL that was returned by generateUploadUrl method.",
      "docstring_tokens": [
        "Uploads",
        "zip",
        "file",
        "with",
        "sources",
        "."
      ],
      "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597",
      "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L130-L159"
    },
    "target": "Uploads zip file with sources .",
    "arguments": [
      [
        "def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , # Those two headers needs to be specified according to: # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl # nopep8 headers = { 'Content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url",
        {
          "num_beams": 1,
          "max_length": 128,
          "until": [
            "</s>"
          ],
          "do_sample": true,
          "temperature": 1.0,
          "top_k": 25,
          "top_p": 1.0,
          "max_new_tokens": 250
        }
      ]
    ],
    "resps": [
      [
        " ``` The above code is a snippet of a function to generate and upload a zip file to Google Cloud Function\u2019s bucket, in order to deploy a new version of the function. It uses the `google-cloud-sdk` library to get the upload URL using the `generateUploadUrl` method. The code also includes checks to ensure a response is received and the upload URL is correctly formatted. The `num_retries` parameter is set to 5, allowing the function to attempt to generate the upload URL up to 5 times before failing.\n\nIn this example, the `generateUploadUrl` method is used to get the upload URL, which is then used to upload the zip file using the `requests.put` method. The `x-goog-content-length-range` header is specified to ensure the zip file is uploaded within the expected size range. Note that the `Content-type` header is set to `application/zip` to indicate the type of file being uploaded.\n\nThis code is quite complex and uses various features of the `google-cloud-sdk` library. It also includes some error handling and retry logic to ensure the function can recover from failures. To write this code from scratch, you would need a good understanding of the library and its APIs,"
      ]
    ],
    "filtered_resps": [
      " ``` The above code is a snippet of a function to generate and upload a zip file to Google Cloud Function\u2019s bucket, in order to deploy a new version of the function. It uses the `google-cloud-sdk` library to get the upload URL using the `generateUploadUrl` method. The code also includes checks to ensure a response is received and the upload URL is correctly formatted. The `num_retries` parameter is set to 5, allowing the function to attempt to generate the upload URL up to 5 times before failing.\n\nIn this example, the `generateUploadUrl` method is used to get the upload URL, which is then used to upload the zip file using the `requests.put` method. The `x-goog-content-length-range` header is specified to ensure the zip file is uploaded within the expected size range. Note that the `Content-type` header is set to `application/zip` to indicate the type of file being uploaded.\n\nThis code is quite complex and uses various features of the `google-cloud-sdk` library. It also includes some error handling and retry logic to ensure the function can recover from failures. To write this code from scratch, you would need a good understanding of the library and its APIs,"
    ],
    "smoothed_bleu_4": 0.613160762584693
  }
]